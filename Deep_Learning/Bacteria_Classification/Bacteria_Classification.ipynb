{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-04T06:42:45.761996Z",
     "iopub.status.busy": "2024-06-04T06:42:45.761392Z",
     "iopub.status.idle": "2024-06-04T06:42:45.781863Z",
     "shell.execute_reply": "2024-06-04T06:42:45.780860Z",
     "shell.execute_reply.started": "2024-06-04T06:42:45.761966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tabular-playground-series-feb-2022/sample_submission.csv\n",
      "/kaggle/input/tabular-playground-series-feb-2022/train.csv\n",
      "/kaggle/input/tabular-playground-series-feb-2022/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:44:21.565092Z",
     "iopub.status.busy": "2024-06-04T06:44:21.564478Z",
     "iopub.status.idle": "2024-06-04T06:44:24.901731Z",
     "shell.execute_reply": "2024-06-04T06:44:24.900763Z",
     "shell.execute_reply.started": "2024-06-04T06:44:21.565058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU is available.')\n",
    "else:\n",
    "    print('GPU is not available.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:45:22.566122Z",
     "iopub.status.busy": "2024-06-04T06:45:22.565735Z",
     "iopub.status.idle": "2024-06-04T06:45:22.571038Z",
     "shell.execute_reply": "2024-06-04T06:45:22.570030Z",
     "shell.execute_reply.started": "2024-06-04T06:45:22.566090Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:45:25.886897Z",
     "iopub.status.busy": "2024-06-04T06:45:25.886514Z",
     "iopub.status.idle": "2024-06-04T06:45:52.492012Z",
     "shell.execute_reply": "2024-06-04T06:45:52.491146Z",
     "shell.execute_reply.started": "2024-06-04T06:45:25.886855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>A0T0G0C10</th>\n",
       "      <th>A0T0G1C9</th>\n",
       "      <th>A0T0G2C8</th>\n",
       "      <th>A0T0G3C7</th>\n",
       "      <th>A0T0G4C6</th>\n",
       "      <th>A0T0G5C5</th>\n",
       "      <th>A0T0G6C4</th>\n",
       "      <th>A0T0G7C3</th>\n",
       "      <th>A0T0G8C2</th>\n",
       "      <th>...</th>\n",
       "      <th>A8T0G1C1</th>\n",
       "      <th>A8T0G2C0</th>\n",
       "      <th>A8T1G0C1</th>\n",
       "      <th>A8T1G1C0</th>\n",
       "      <th>A8T2G0C0</th>\n",
       "      <th>A9T0G0C1</th>\n",
       "      <th>A9T0G1C0</th>\n",
       "      <th>A9T1G0C0</th>\n",
       "      <th>A10T0G0C0</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.536743e-07</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-9.536743e-07</td>\n",
       "      <td>Streptococcus_pyogenes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-9.536743e-07</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-9.536743e-07</td>\n",
       "      <td>Salmonella_enterica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-9.536743e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1.046326e-06</td>\n",
       "      <td>Salmonella_enterica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.632568e-08</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-9.536743e-07</td>\n",
       "      <td>Salmonella_enterica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-9.536743e-07</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-9.536743e-07</td>\n",
       "      <td>Enterococcus_hirae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id     A0T0G0C10  A0T0G1C9  A0T0G2C8  A0T0G3C7  A0T0G4C6  A0T0G5C5  \\\n",
       "0       0 -9.536743e-07 -0.000010 -0.000043 -0.000114 -0.000200 -0.000240   \n",
       "1       1 -9.536743e-07 -0.000010 -0.000043  0.000886 -0.000200  0.000760   \n",
       "2       2 -9.536743e-07 -0.000002  0.000007  0.000129  0.000268  0.000270   \n",
       "3       3  4.632568e-08 -0.000006  0.000012  0.000245  0.000492  0.000522   \n",
       "4       4 -9.536743e-07 -0.000010 -0.000043 -0.000114 -0.000200 -0.000240   \n",
       "\n",
       "   A0T0G6C4  A0T0G7C3  A0T0G8C2  ...  A8T0G1C1  A8T0G2C0  A8T1G0C1  A8T1G1C0  \\\n",
       "0 -0.000200 -0.000114 -0.000043  ... -0.000086 -0.000043 -0.000086 -0.000086   \n",
       "1 -0.000200 -0.000114 -0.000043  ... -0.000086 -0.000043  0.000914  0.000914   \n",
       "2  0.000243  0.000125  0.000001  ...  0.000084  0.000048  0.000081  0.000106   \n",
       "3  0.000396  0.000197 -0.000003  ...  0.000151  0.000100  0.000180  0.000202   \n",
       "4 -0.000200 -0.000114 -0.000043  ... -0.000086 -0.000043 -0.000086 -0.000086   \n",
       "\n",
       "   A8T2G0C0  A9T0G0C1  A9T0G1C0  A9T1G0C0     A10T0G0C0  \\\n",
       "0 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07   \n",
       "1 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07   \n",
       "2  0.000072  0.000010  0.000008  0.000019  1.046326e-06   \n",
       "3  0.000153  0.000021  0.000015  0.000046 -9.536743e-07   \n",
       "4 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07   \n",
       "\n",
       "                   target  \n",
       "0  Streptococcus_pyogenes  \n",
       "1     Salmonella_enterica  \n",
       "2     Salmonella_enterica  \n",
       "3     Salmonella_enterica  \n",
       "4      Enterococcus_hirae  \n",
       "\n",
       "[5 rows x 288 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/train.csv')\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:46:34.490857Z",
     "iopub.status.busy": "2024-06-04T06:46:34.490493Z",
     "iopub.status.idle": "2024-06-04T06:46:34.496706Z",
     "shell.execute_reply": "2024-06-04T06:46:34.495699Z",
     "shell.execute_reply.started": "2024-06-04T06:46:34.490827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['row_id', 'A0T0G0C10', 'A0T0G1C9', 'A0T0G2C8', 'A0T0G3C7', 'A0T0G4C6',\n",
      "       'A0T0G5C5', 'A0T0G6C4', 'A0T0G7C3', 'A0T0G8C2',\n",
      "       ...\n",
      "       'A8T0G1C1', 'A8T0G2C0', 'A8T1G0C1', 'A8T1G1C0', 'A8T2G0C0', 'A9T0G0C1',\n",
      "       'A9T0G1C0', 'A9T1G0C0', 'A10T0G0C0', 'target'],\n",
      "      dtype='object', length=288)\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T14:57:00.008282Z",
     "iopub.status.busy": "2024-03-06T14:57:00.007331Z",
     "iopub.status.idle": "2024-03-06T14:57:01.841531Z",
     "shell.execute_reply": "2024-03-06T14:57:01.840532Z",
     "shell.execute_reply.started": "2024-03-06T14:57:00.008244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (200000, 288)\n",
      "Missing Data: 0\n",
      "Duplicates: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train Shape: {}\\nMissing Data: {}\\nDuplicates: {}\\n'\\\n",
    "      .format(train.shape, train.isna().sum().sum(), train.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:46:48.109512Z",
     "iopub.status.busy": "2024-06-04T06:46:48.108754Z",
     "iopub.status.idle": "2024-06-04T06:46:48.114044Z",
     "shell.execute_reply": "2024-06-04T06:46:48.112885Z",
     "shell.execute_reply.started": "2024-06-04T06:46:48.109481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the target column (assuming the column is named 'target' in this example)\n",
    "target_column = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:46:50.820867Z",
     "iopub.status.busy": "2024-06-04T06:46:50.820016Z",
     "iopub.status.idle": "2024-06-04T06:46:50.963941Z",
     "shell.execute_reply": "2024-06-04T06:46:50.962727Z",
     "shell.execute_reply.started": "2024-06-04T06:46:50.820831Z"
    }
   },
   "outputs": [],
   "source": [
    "train=train.drop(columns=['target']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:47:22.470036Z",
     "iopub.status.busy": "2024-06-04T06:47:22.469669Z",
     "iopub.status.idle": "2024-06-04T06:47:23.036542Z",
     "shell.execute_reply": "2024-06-04T06:47:23.035365Z",
     "shell.execute_reply.started": "2024-06-04T06:47:22.470008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled Data (Min-Max scaled between 0 and 1):\n",
      " [[0.000000e+00 0.000000e+00 0.000000e+00 ... 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00]\n",
      " [5.000025e-06 0.000000e+00 0.000000e+00 ... 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00]\n",
      " [1.000005e-05 0.000000e+00 8.000000e-04 ... 1.800000e-03 1.450000e-03\n",
      "  2.000000e-03]\n",
      " ...\n",
      " [9.999900e-01 1.000000e-04 1.100000e-03 ... 4.500000e-03 1.550000e-03\n",
      "  1.000000e-03]\n",
      " [9.999950e-01 0.000000e+00 0.000000e+00 ... 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00]\n",
      " [1.000000e+00 2.000000e-04 8.000000e-04 ... 2.300000e-03 2.150000e-03\n",
      "  0.000000e+00]]\n",
      "\n",
      "Min Values for each feature:\n",
      " [ 0.00000000e+00 -9.53674316e-07 -9.53674316e-06 -4.29153442e-05\n",
      " -1.14440918e-04 -2.00271606e-04 -2.40325928e-04 -2.00271606e-04\n",
      " -1.14440918e-04 -4.29153442e-05 -9.53674316e-06 -9.53674316e-07\n",
      " -9.53674316e-06 -8.58306885e-05 -3.43322754e-04 -8.01086426e-04\n",
      " -1.20162964e-03 -1.20162964e-03 -8.01086426e-04 -3.43322754e-04\n",
      " -8.58306885e-05 -9.53674316e-06 -4.29153442e-05 -3.43322754e-04\n",
      " -1.20162964e-03 -2.40325928e-03 -3.00407410e-03 -2.40325928e-03\n",
      " -1.20162964e-03 -3.43322754e-04 -4.29153442e-05 -1.14440918e-04\n",
      " -8.01086426e-04 -2.40325928e-03 -4.00543213e-03 -4.00543213e-03\n",
      " -2.40325928e-03 -8.01086426e-04 -1.14440918e-04 -2.00271606e-04\n",
      " -1.20162964e-03 -3.00407410e-03 -4.00543213e-03 -3.00407410e-03\n",
      " -1.20162964e-03 -2.00271606e-04 -2.40325928e-04 -1.20162964e-03\n",
      " -2.40325928e-03 -2.40325928e-03 -1.20162964e-03 -2.40325928e-04\n",
      " -2.00271606e-04 -8.01086426e-04 -1.20162964e-03 -8.01086426e-04\n",
      " -2.00271606e-04 -1.14440918e-04 -3.43322754e-04 -3.43322754e-04\n",
      " -1.14440918e-04 -4.29153442e-05 -8.58306885e-05 -4.29153442e-05\n",
      " -9.53674316e-06 -9.53674316e-06 -9.53674316e-07 -9.53674316e-06\n",
      " -8.58306885e-05 -3.43322754e-04 -8.01086426e-04 -1.20162964e-03\n",
      " -1.20162964e-03 -8.01086426e-04 -3.43322754e-04 -8.58306885e-05\n",
      " -9.53674316e-06 -8.58306885e-05 -6.86645508e-04 -2.40325928e-03\n",
      " -4.80651855e-03 -6.00814819e-03 -4.80651855e-03 -2.40325928e-03\n",
      " -6.86645508e-04 -8.58306885e-05 -3.43322754e-04 -2.40325928e-03\n",
      " -7.20977783e-03 -1.20162964e-02 -1.20162964e-02 -7.20977783e-03\n",
      " -2.40325928e-03 -3.43322754e-04 -8.01086426e-04 -4.80651855e-03\n",
      " -1.20162964e-02 -1.60217285e-02 -1.20162964e-02 -4.80651855e-03\n",
      " -8.01086426e-04 -1.20162964e-03 -6.00814819e-03 -1.20162964e-02\n",
      " -1.20162964e-02 -6.00814819e-03 -1.20162964e-03 -1.20162964e-03\n",
      " -4.80651855e-03 -7.20977783e-03 -4.80651855e-03 -1.20162964e-03\n",
      " -8.01086426e-04 -2.40325928e-03 -2.40325928e-03 -8.01086426e-04\n",
      " -3.43322754e-04 -6.86645508e-04 -3.43322754e-04 -8.58306885e-05\n",
      " -8.58306885e-05 -9.53674316e-06 -4.29153442e-05 -3.43322754e-04\n",
      " -1.20162964e-03 -2.40325928e-03 -3.00407410e-03 -2.40325928e-03\n",
      " -1.20162964e-03 -3.43322754e-04 -4.29153442e-05 -3.43322754e-04\n",
      " -2.40325928e-03 -7.20977783e-03 -1.20162964e-02 -1.20162964e-02\n",
      " -7.20977783e-03 -2.40325928e-03 -3.43322754e-04 -1.20162964e-03\n",
      " -7.20977783e-03 -1.80244446e-02 -2.40325928e-02 -1.80244446e-02\n",
      " -7.20977783e-03 -1.20162964e-03 -2.40325928e-03 -1.20162964e-02\n",
      " -2.40325928e-02 -2.40325928e-02 -1.20162964e-02 -2.40325928e-03\n",
      " -3.00407410e-03 -1.20162964e-02 -1.80244446e-02 -1.20162964e-02\n",
      " -3.00407410e-03 -2.40325928e-03 -7.20977783e-03 -7.20977783e-03\n",
      " -2.40325928e-03 -1.20162964e-03 -2.40325928e-03 -1.20162964e-03\n",
      " -3.43322754e-04 -3.43322754e-04 -4.29153442e-05 -1.14440918e-04\n",
      " -8.01086426e-04 -2.40325928e-03 -4.00543213e-03 -4.00543213e-03\n",
      " -2.40325928e-03 -8.01086426e-04 -1.14440918e-04 -8.01086426e-04\n",
      " -4.80651855e-03 -1.20162964e-02 -1.60217285e-02 -1.20162964e-02\n",
      " -4.80651855e-03 -8.01086426e-04 -2.40325928e-03 -1.20162964e-02\n",
      " -2.40325928e-02 -2.40325928e-02 -1.20162964e-02 -2.40325928e-03\n",
      " -4.00543213e-03 -1.60217285e-02 -2.40325928e-02 -1.60217285e-02\n",
      " -4.00543213e-03 -4.00543213e-03 -1.20162964e-02 -1.20162964e-02\n",
      " -4.00543213e-03 -2.40325928e-03 -4.80651855e-03 -2.40325928e-03\n",
      " -8.01086426e-04 -8.01086426e-04 -1.14440918e-04 -2.00271606e-04\n",
      " -1.20162964e-03 -3.00407410e-03 -4.00543213e-03 -3.00407410e-03\n",
      " -1.20162964e-03 -2.00271606e-04 -1.20162964e-03 -6.00814819e-03\n",
      " -1.20162964e-02 -1.20162964e-02 -6.00814819e-03 -1.20162964e-03\n",
      " -3.00407410e-03 -1.20162964e-02 -1.80244446e-02 -1.20162964e-02\n",
      " -3.00407410e-03 -4.00543213e-03 -1.20162964e-02 -1.20162964e-02\n",
      " -4.00543213e-03 -3.00407410e-03 -6.00814819e-03 -3.00407410e-03\n",
      " -1.20162964e-03 -1.20162964e-03 -2.00271606e-04 -2.40325928e-04\n",
      " -1.20162964e-03 -2.40325928e-03 -2.40325928e-03 -1.20162964e-03\n",
      " -2.40325928e-04 -1.20162964e-03 -4.80651855e-03 -7.20977783e-03\n",
      " -4.80651855e-03 -1.20162964e-03 -2.40325928e-03 -7.20977783e-03\n",
      " -7.20977783e-03 -2.40325928e-03 -2.40325928e-03 -4.80651855e-03\n",
      " -2.40325928e-03 -1.20162964e-03 -1.20162964e-03 -2.40325928e-04\n",
      " -2.00271606e-04 -8.01086426e-04 -1.20162964e-03 -8.01086426e-04\n",
      " -2.00271606e-04 -8.01086426e-04 -2.40325928e-03 -2.40325928e-03\n",
      " -8.01086426e-04 -1.20162964e-03 -2.40325928e-03 -1.20162964e-03\n",
      " -8.01086426e-04 -8.01086426e-04 -2.00271606e-04 -1.14440918e-04\n",
      " -3.43322754e-04 -3.43322754e-04 -1.14440918e-04 -3.43322754e-04\n",
      " -6.86645508e-04 -3.43322754e-04 -3.43322754e-04 -3.43322754e-04\n",
      " -1.14440918e-04 -4.29153442e-05 -8.58306885e-05 -4.29153442e-05\n",
      " -8.58306885e-05 -8.58306885e-05 -4.29153442e-05 -9.53674316e-06\n",
      " -9.53674316e-06 -9.53674316e-06 -9.53674316e-07]\n",
      "\n",
      "Max Values for each feature:\n",
      " [1.99999000e+05 9.99904633e-03 9.99046326e-03 9.95708466e-03\n",
      " 9.88555908e-03 1.97997284e-02 1.97596741e-02 1.97997284e-02\n",
      " 9.88555908e-03 9.95708466e-03 9.99046326e-03 9.99046326e-04\n",
      " 9.90463257e-04 9.91416931e-03 1.96566772e-02 2.91989136e-02\n",
      " 3.87983704e-02 2.87983704e-02 2.91989136e-02 2.96566772e-02\n",
      " 9.91416931e-03 9.90463257e-04 9.95708466e-03 9.65667725e-03\n",
      " 2.87983704e-02 2.75967407e-02 3.69959259e-02 2.75967407e-02\n",
      " 2.87983704e-02 1.96566772e-02 9.95708466e-03 9.88555908e-03\n",
      " 1.91989136e-02 3.75967407e-02 3.59945679e-02 2.59945679e-02\n",
      " 3.75967407e-02 1.91989136e-02 9.88555908e-03 9.79972839e-03\n",
      " 1.87983704e-02 2.69959259e-02 3.59945679e-02 2.69959259e-02\n",
      " 1.87983704e-02 9.79972839e-03 1.97596741e-02 2.87983704e-02\n",
      " 3.75967407e-02 2.75967407e-02 2.87983704e-02 9.75967407e-03\n",
      " 1.97997284e-02 1.91989136e-02 2.87983704e-02 2.91989136e-02\n",
      " 1.97997284e-02 1.98855591e-02 1.96566772e-02 1.96566772e-02\n",
      " 1.98855591e-02 1.99570847e-02 1.99141693e-02 1.99570847e-02\n",
      " 9.99046326e-03 9.99046326e-03 9.99904633e-03 9.99046326e-03\n",
      " 9.91416931e-03 1.96566772e-02 2.91989136e-02 4.87983704e-02\n",
      " 2.87983704e-02 1.91989136e-02 9.65667725e-03 9.91416931e-03\n",
      " 9.90463257e-04 9.91416931e-03 9.31335449e-03 4.75967407e-02\n",
      " 4.51934814e-02 6.39918518e-02 5.51934814e-02 2.75967407e-02\n",
      " 1.93133545e-02 9.91416931e-03 1.96566772e-02 1.75967407e-02\n",
      " 5.27902222e-02 5.79837036e-02 5.79837036e-02 5.27902222e-02\n",
      " 2.75967407e-02 1.96566772e-02 1.91989136e-02 2.51934814e-02\n",
      " 3.79837036e-02 5.39782715e-02 4.79837036e-02 3.51934814e-02\n",
      " 1.91989136e-02 1.87983704e-02 3.39918518e-02 4.79837036e-02\n",
      " 3.79837036e-02 3.39918518e-02 2.87983704e-02 2.87983704e-02\n",
      " 3.51934814e-02 5.27902222e-02 3.51934814e-02 4.87983704e-02\n",
      " 4.91989136e-02 4.75967407e-02 3.75967407e-02 2.91989136e-02\n",
      " 2.96566772e-02 3.93133545e-02 3.96566772e-02 1.99141693e-02\n",
      " 1.99141693e-02 1.99904633e-02 9.95708466e-03 1.96566772e-02\n",
      " 2.87983704e-02 3.75967407e-02 3.69959259e-02 3.75967407e-02\n",
      " 1.87983704e-02 9.65667725e-03 9.95708466e-03 1.96566772e-02\n",
      " 2.75967407e-02 5.27902222e-02 6.79837036e-02 4.79837036e-02\n",
      " 4.27902222e-02 1.75967407e-02 9.65667725e-03 1.87983704e-02\n",
      " 3.27902222e-02 6.19755554e-02 5.59674072e-02 6.19755554e-02\n",
      " 3.27902222e-02 1.87983704e-02 2.75967407e-02 3.79837036e-02\n",
      " 6.59674072e-02 3.59674072e-02 3.79837036e-02 2.75967407e-02\n",
      " 3.69959259e-02 5.79837036e-02 5.19755554e-02 5.79837036e-02\n",
      " 2.69959259e-02 3.75967407e-02 5.27902222e-02 6.27902222e-02\n",
      " 3.75967407e-02 4.87983704e-02 5.75967407e-02 3.87983704e-02\n",
      " 2.96566772e-02 3.96566772e-02 1.99570847e-02 9.88555908e-03\n",
      " 1.91989136e-02 2.75967407e-02 4.59945679e-02 4.59945679e-02\n",
      " 2.75967407e-02 1.91989136e-02 9.88555908e-03 1.91989136e-02\n",
      " 3.51934814e-02 4.79837036e-02 7.39782715e-02 5.79837036e-02\n",
      " 3.51934814e-02 1.91989136e-02 2.75967407e-02 4.79837036e-02\n",
      " 6.59674072e-02 6.59674072e-02 3.79837036e-02 3.75967407e-02\n",
      " 3.59945679e-02 8.39782715e-02 5.59674072e-02 5.39782715e-02\n",
      " 2.59945679e-02 5.59945679e-02 7.79837036e-02 8.79837036e-02\n",
      " 5.59945679e-02 5.75967407e-02 6.51934814e-02 4.75967407e-02\n",
      " 3.91989136e-02 4.91989136e-02 2.98855591e-02 9.79972839e-03\n",
      " 2.87983704e-02 3.69959259e-02 3.59945679e-02 2.69959259e-02\n",
      " 1.87983704e-02 9.79972839e-03 1.87983704e-02 3.39918518e-02\n",
      " 4.79837036e-02 5.79837036e-02 4.39918518e-02 1.87983704e-02\n",
      " 3.69959259e-02 5.79837036e-02 6.19755554e-02 5.79837036e-02\n",
      " 2.69959259e-02 4.59945679e-02 5.79837036e-02 9.79837036e-02\n",
      " 4.59945679e-02 5.69959259e-02 7.39918518e-02 4.69959259e-02\n",
      " 4.87983704e-02 3.87983704e-02 2.97997284e-02 1.97596741e-02\n",
      " 2.87983704e-02 4.75967407e-02 3.75967407e-02 1.87983704e-02\n",
      " 9.75967407e-03 2.87983704e-02 3.51934814e-02 4.27902222e-02\n",
      " 5.51934814e-02 2.87983704e-02 4.75967407e-02 5.27902222e-02\n",
      " 6.27902222e-02 5.75967407e-02 5.75967407e-02 8.51934814e-02\n",
      " 4.75967407e-02 6.87983704e-02 4.87983704e-02 5.97596741e-02\n",
      " 9.79972839e-03 1.91989136e-02 2.87983704e-02 2.91989136e-02\n",
      " 1.97997284e-02 3.91989136e-02 4.75967407e-02 3.75967407e-02\n",
      " 2.91989136e-02 3.87983704e-02 5.75967407e-02 3.87983704e-02\n",
      " 4.91989136e-02 4.91989136e-02 3.97997284e-02 1.98855591e-02\n",
      " 1.96566772e-02 2.96566772e-02 1.98855591e-02 3.96566772e-02\n",
      " 3.93133545e-02 2.96566772e-02 3.96566772e-02 3.96566772e-02\n",
      " 2.98855591e-02 1.99570847e-02 1.99141693e-02 1.99570847e-02\n",
      " 1.99141693e-02 1.99141693e-02 1.99570847e-02 9.99046326e-03\n",
      " 9.99046326e-03 1.99904633e-02 9.99046326e-04]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example data (replace this with your actual data)\n",
    "train = np.array(train)\n",
    "\n",
    "# Function to perform Min-Max scaling\n",
    "def min_max_scaling(train):\n",
    "    # Create MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Fit the scaler to the data and transform the data\n",
    "    scaled_data = scaler.fit_transform(train)\n",
    "    \n",
    "    # Get the minimum and maximum values for each feature\n",
    "    min_vals = scaler.data_min_\n",
    "    max_vals = scaler.data_max_\n",
    "    \n",
    "    return scaled_data, min_vals, max_vals\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_data, min_vals, max_vals = min_max_scaling(train)\n",
    "\n",
    "# Print the scaled data, minimum, and maximum values for each feature\n",
    "print(\"\\nScaled Data (Min-Max scaled between 0 and 1):\\n\", scaled_data)\n",
    "print(\"\\nMin Values for each feature:\\n\", min_vals)\n",
    "print(\"\\nMax Values for each feature:\\n\", max_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:47:37.954261Z",
     "iopub.status.busy": "2024-06-04T06:47:37.953906Z",
     "iopub.status.idle": "2024-06-04T06:47:42.550812Z",
     "shell.execute_reply": "2024-06-04T06:47:42.549512Z",
     "shell.execute_reply.started": "2024-06-04T06:47:37.954232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "0      -0.195312  1.450241 -0.493021 -0.572658 -0.297634  0.197065  0.409359   \n",
      "1      -0.455772  0.023449 -0.499954 -0.147321  0.226932  0.058239 -0.082557   \n",
      "2      -0.455364 -0.176181 -0.501369  0.009695 -0.088106 -0.045754 -0.005821   \n",
      "3      -0.514005 -0.202394 -0.502223  0.088732 -0.101302  0.066162 -0.007537   \n",
      "4       0.498484  1.020506 -0.501142  0.310238 -0.013760 -0.050889 -0.443492   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "199995 -0.481202 -0.183816  0.498148  0.051616 -0.089788 -0.002522 -0.010078   \n",
      "199996 -0.196881 -0.019007  0.500695 -0.205748  0.337806 -0.063222 -0.172514   \n",
      "199997 -0.212165 -0.158487  0.500078 -0.081012 -0.065431 -0.176711 -0.001364   \n",
      "199998 -0.364806  0.009943  0.501534 -0.485663  0.894588  0.268238  0.036826   \n",
      "199999 -0.871026 -0.253236  0.495066  0.280512 -0.152345  0.362412 -0.034244   \n",
      "\n",
      "             PC8       PC9      PC10  ...      PC24      PC25      PC26  \\\n",
      "0      -0.055441  0.132258 -0.026770  ...  0.045912  0.067584  0.058161   \n",
      "1      -0.255540 -0.023778 -0.059134  ... -0.013358 -0.006301 -0.081865   \n",
      "2       0.039761 -0.008595 -0.047018  ...  0.005180  0.001069  0.003818   \n",
      "3       0.020200  0.002239  0.002178  ...  0.003745 -0.002305  0.009451   \n",
      "4       0.146230  0.392338  0.096294  ...  0.084912 -0.090635  0.077074   \n",
      "...          ...       ...       ...  ...       ...       ...       ...   \n",
      "199995  0.040621 -0.002768 -0.022499  ... -0.003722  0.007198  0.006078   \n",
      "199996 -0.316561 -0.015176 -0.072131  ... -0.025409 -0.042501 -0.060227   \n",
      "199997  0.050212 -0.012698 -0.063586  ...  0.003545 -0.000374 -0.003997   \n",
      "199998  0.354068  0.001798  0.033603  ... -0.002946  0.001763  0.012658   \n",
      "199999  0.010154  0.009853  0.037610  ... -0.031859 -0.020852  0.033486   \n",
      "\n",
      "            PC27      PC28      PC29      PC30      PC31      PC32      PC33  \n",
      "0       0.080422 -0.055667  0.031298 -0.056714 -0.022512 -0.073218 -0.012292  \n",
      "1      -0.061775  0.011944  0.026773  0.060464  0.036971 -0.000869  0.035747  \n",
      "2      -0.003698 -0.005330  0.003066 -0.001054 -0.003000  0.001669  0.009610  \n",
      "3       0.004183 -0.002584  0.000024 -0.003779  0.002862 -0.006979  0.007767  \n",
      "4      -0.001332 -0.065228  0.054994  0.165180 -0.077017  0.151799  0.103229  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "199995 -0.005379 -0.008743  0.003717 -0.001842  0.000351  0.002165 -0.001142  \n",
      "199996  0.036894  0.017688  0.008969  0.062921  0.011243  0.030771 -0.036063  \n",
      "199997 -0.007766 -0.007388  0.002517  0.002112 -0.006581  0.004305  0.005221  \n",
      "199998 -0.017252 -0.001766 -0.014007 -0.028318 -0.001110 -0.002408  0.001950  \n",
      "199999  0.014347  0.004890 -0.009912  0.014541  0.003062 -0.039314  0.006941  \n",
      "\n",
      "[200000 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# This line imports the PCA class from the scikit-learn library, which is used for dimensionality reduction \n",
    "# using Principal Component Analysis.\n",
    "\n",
    "# Assuming X_train_scaled is your scaled training data\n",
    "# You can use X_train_formatted if you've converted it to human-readable format X_train_formatted\n",
    "\n",
    "# Initialize the PCA model with the desired number of components\n",
    "n_components = 33  # Adjust this value as needed, desired number of component.\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "# This initializes a PCA model with the specified number of components.\n",
    "\n",
    "# Fit the PCA model to your scaled data\n",
    "pca.fit(scaled_data)   \n",
    "# This fits the PCA model to the scaled data\n",
    "\n",
    "# Transform the data to its principal components\n",
    "principal_compo = pca.transform(scaled_data)\n",
    "# This line transforms the scaled data into its principal components using the trained PCA model.\n",
    "\n",
    "# Create a DataFrame for the PCA results using common columns\n",
    "pca_df = pd.DataFrame(data=principal_compo, columns=[f'PC{i}' for i in range(1, 34)])\n",
    "# This creates a DataFrame (pca_df) to store the principal components. Each column is labeled with a prefix 'PC' followed \n",
    "# by the component number.\n",
    "# Printing the PCA results\n",
    "print(pca_df)\n",
    "\n",
    "# Now, X_train_pca contains your training data reduced to the specified number of principal components\n",
    "# You can use X_train_pca for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:47:49.748652Z",
     "iopub.status.busy": "2024-06-04T06:47:49.748249Z",
     "iopub.status.idle": "2024-06-04T06:47:49.756202Z",
     "shell.execute_reply": "2024-06-04T06:47:49.755267Z",
     "shell.execute_reply.started": "2024-06-04T06:47:49.748620Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_df['target'] = target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:47:52.099371Z",
     "iopub.status.busy": "2024-06-04T06:47:52.098538Z",
     "iopub.status.idle": "2024-06-04T06:47:52.124415Z",
     "shell.execute_reply": "2024-06-04T06:47:52.123119Z",
     "shell.execute_reply.started": "2024-06-04T06:47:52.099327Z"
    }
   },
   "outputs": [],
   "source": [
    "y = pca_df['target']\n",
    "X = pca_df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:47:55.615368Z",
     "iopub.status.busy": "2024-06-04T06:47:55.614648Z",
     "iopub.status.idle": "2024-06-04T06:47:55.619588Z",
     "shell.execute_reply": "2024-06-04T06:47:55.618531Z",
     "shell.execute_reply.started": "2024-06-04T06:47:55.615337Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:47:57.944143Z",
     "iopub.status.busy": "2024-06-04T06:47:57.943411Z",
     "iopub.status.idle": "2024-06-04T06:47:57.948805Z",
     "shell.execute_reply": "2024-06-04T06:47:57.947639Z",
     "shell.execute_reply.started": "2024-06-04T06:47:57.944115Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert y to a pandas Series\n",
    "y = pd.Series(y)\n",
    "# Convert X to a pandas DataFrame\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T17:13:05.240407Z",
     "iopub.status.busy": "2024-06-03T17:13:05.239970Z",
     "iopub.status.idle": "2024-06-03T17:13:05.247998Z",
     "shell.execute_reply": "2024-06-03T17:13:05.246375Z",
     "shell.execute_reply.started": "2024-06-03T17:13:05.240362Z"
    }
   },
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans as cuKMeans  # Import cuML KMeans for GPU acceleration\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T17:13:25.685453Z",
     "iopub.status.busy": "2024-06-03T17:13:25.684715Z",
     "iopub.status.idle": "2024-06-03T17:13:25.716453Z",
     "shell.execute_reply": "2024-06-03T17:13:25.715690Z",
     "shell.execute_reply.started": "2024-06-03T17:13:25.685422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the target variable is categorical\n",
    "y = pd.Categorical(y)\n",
    "y_encoded = y.codes  # Convert to integer codes\n",
    "num_classes = len(y.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T17:13:42.356957Z",
     "iopub.status.busy": "2024-06-03T17:13:42.355859Z",
     "iopub.status.idle": "2024-06-03T17:13:42.361771Z",
     "shell.execute_reply": "2024-06-03T17:13:42.360700Z",
     "shell.execute_reply.started": "2024-06-03T17:13:42.356917Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform StratifiedShuffleSplit cross-validation\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=42)\n",
    "train_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:09:14.507660Z",
     "iopub.status.busy": "2024-06-03T18:09:14.506863Z",
     "iopub.status.idle": "2024-06-03T19:01:05.215116Z",
     "shell.execute_reply": "2024-06-03T19:01:05.214002Z",
     "shell.execute_reply.started": "2024-06-03T18:09:14.507626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9889\n",
      "Fold test accuracy: 0.8534\n",
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9856\n",
      "Fold test accuracy: 0.8404\n",
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9883\n",
      "Fold test accuracy: 0.8410\n",
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9875\n",
      "Fold test accuracy: 0.8508\n",
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9809\n",
      "Fold test accuracy: 0.8343\n",
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9809\n",
      "Fold test accuracy: 0.8537\n",
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9811\n",
      "Fold test accuracy: 0.8440\n",
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9820\n",
      "Fold test accuracy: 0.8525\n",
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9810\n",
      "Fold test accuracy: 0.8600\n",
      "Before KMeans\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Fold training accuracy: 0.9855\n",
      "Fold test accuracy: 0.8579\n",
      "Average training accuracy: 0.9831\n",
      "Average test accuracy: 0.8450\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cuml.cluster import KMeans as cuKMeans  # Import cuML KMeans for GPU acceleration\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "\n",
    "# Function to get the predominant class and its average record for each cluster\n",
    "def get_representative_record(cluster_df):\n",
    "    # Get the predominant class\n",
    "    predominant_class = cluster_df['target'].mode()[0]\n",
    "    \n",
    "    # Filter records of the predominant class\n",
    "    predominant_class_records = cluster_df[cluster_df['target'] == predominant_class].drop(columns=['cluster', 'target'])\n",
    "    \n",
    "    # Compute the average record for the predominant class\n",
    "    representative_record = predominant_class_records.mean()\n",
    "    representative_record['target'] = predominant_class\n",
    "    \n",
    "    return representative_record\n",
    "\n",
    "# Define the multi-layered dense neural network model\n",
    "def create_dense_nn(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer=he_normal(), input_shape=input_shape, kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=he_normal(), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer=he_normal(), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer=he_normal(), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=he_normal(), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer=he_normal(), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer=he_normal(), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=he_normal(), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer=he_normal(), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=Adamax(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "for train_index, test_index in sss.split(X, y_encoded):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "    \n",
    "    print(\"Before KMeans\")\n",
    "    \n",
    "    # Apply KMeans clustering on the training set using cuML\n",
    "    kmeans = cuKMeans(n_clusters=10000)\n",
    "    kmeans.fit(X_train.values)  # Convert to NumPy array\n",
    "    \n",
    "    X_train_clustered = X_train.copy()  # Avoid SettingWithCopyWarning by working on a copy\n",
    "    X_train_clustered['cluster'] = kmeans.labels_\n",
    "    X_train_clustered['target'] = y_train\n",
    "    \n",
    "    # Generate the reduced training dataset\n",
    "    reduced_train_data = X_train_clustered.groupby('cluster').apply(get_representative_record).reset_index(drop=True)\n",
    "    \n",
    "    # Prepare data for model training\n",
    "    X_train_reduced = reduced_train_data.drop(columns=['target']).values  # Convert to NumPy array\n",
    "    y_train_reduced = reduced_train_data['target'].values  # Convert to NumPy array\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    y_train_reduced_categorical = to_categorical(y_train_reduced, num_classes=num_classes)\n",
    "    \n",
    "    # Create the dense neural network model\n",
    "    cnn_model = create_dense_nn(input_shape=(X_train_reduced.shape[1],), num_classes=num_classes)\n",
    "    \n",
    "    # Train the model on the reduced training set\n",
    "    cnn_model.fit(X_train_reduced, y_train_reduced_categorical, epochs=100, batch_size=32, verbose=0)  # Set verbose=0\n",
    "    \n",
    "    # Predict on the training set\n",
    "    y_train_pred = cnn_model.predict(X_train_reduced)\n",
    "    y_train_pred_classes = np.argmax(y_train_pred, axis=1)\n",
    "    train_accuracy = accuracy_score(y_train_reduced, y_train_pred_classes)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # One-hot encode the test labels\n",
    "    y_test_categorical = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    # Predict on the original test set\n",
    "    y_test_pred = cnn_model.predict(X_test.values)\n",
    "    y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred_classes)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(f'Fold training accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Fold test accuracy: {test_accuracy:.4f}')\n",
    "#     break\n",
    "\n",
    "# Calculate the average accuracies\n",
    "average_train_accuracy = np.mean(train_accuracies)\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "print(f'Average training accuracy: {average_train_accuracy:.4f}')\n",
    "print(f'Average test accuracy: {average_test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T02:29:48.332846Z",
     "iopub.status.busy": "2024-06-04T02:29:48.332458Z",
     "iopub.status.idle": "2024-06-04T02:29:53.350961Z",
     "shell.execute_reply": "2024-06-04T02:29:53.349926Z",
     "shell.execute_reply.started": "2024-06-04T02:29:48.332816Z"
    }
   },
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans as cuKMeans  # Import cuML's KMeans for GPU acceleration\n",
    "from sklearn.ensemble import AdaBoostClassifier  # Import AdaBoost for classification\n",
    "from sklearn.model_selection import StratifiedShuffleSplit  # Import StratifiedShuffleSplit for cross-validation\n",
    "from sklearn.metrics import accuracy_score  # Import accuracy_score for evaluation\n",
    "import numpy as np  # Import NumPy for numerical operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T02:29:57.333429Z",
     "iopub.status.busy": "2024-06-04T02:29:57.332713Z",
     "iopub.status.idle": "2024-06-04T02:29:57.338237Z",
     "shell.execute_reply": "2024-06-04T02:29:57.337292Z",
     "shell.execute_reply.started": "2024-06-04T02:29:57.333396Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(max_depth=5)  # You can adjust max_depth\n",
    "# Step 4: Initialize the AdaBoost Classifier\n",
    "model_4 = AdaBoostClassifier(estimator=base_estimator,n_estimators=50,learning_rate=1)\n",
    "# Initialize the AdaBoost model\n",
    "# model_4 = AdaBoostClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T02:30:03.495822Z",
     "iopub.status.busy": "2024-06-04T02:30:03.495451Z",
     "iopub.status.idle": "2024-06-04T02:30:03.500629Z",
     "shell.execute_reply": "2024-06-04T02:30:03.499564Z",
     "shell.execute_reply.started": "2024-06-04T02:30:03.495791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform StratifiedShuffleSplit cross-validation\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=42)\n",
    "train_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T03:06:25.501160Z",
     "iopub.status.busy": "2024-06-04T03:06:25.500736Z",
     "iopub.status.idle": "2024-06-04T03:35:13.725648Z",
     "shell.execute_reply": "2024-06-04T03:35:13.724668Z",
     "shell.execute_reply.started": "2024-06-04T03:06:25.501129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost  Results:-\n",
      "Fold training accuracy for adaboost: 0.5762\n",
      "Fold test accuracy for adaboost: 0.5152\n",
      "-------------------------------------\n",
      "Fold training accuracy for adaboost: 0.5932\n",
      "Fold test accuracy for adaboost: 0.5361\n",
      "-------------------------------------\n",
      "Fold training accuracy for adaboost: 0.5887\n",
      "Fold test accuracy for adaboost: 0.5172\n",
      "-------------------------------------\n",
      "Fold training accuracy for adaboost: 0.5791\n",
      "Fold test accuracy for adaboost: 0.5452\n",
      "-------------------------------------\n",
      "Fold training accuracy for adaboost: 0.6015\n",
      "Fold test accuracy for adaboost: 0.5494\n",
      "-------------------------------------\n",
      "Fold training accuracy for adaboost: 0.5963\n",
      "Fold test accuracy for adaboost: 0.5608\n",
      "-------------------------------------\n",
      "Fold training accuracy for adaboost: 0.5985\n",
      "Fold test accuracy for adaboost: 0.5143\n",
      "-------------------------------------\n",
      "Fold training accuracy for adaboost: 0.5701\n",
      "Fold test accuracy for adaboost: 0.5450\n",
      "-------------------------------------\n",
      "Fold training accuracy for adaboost: 0.5800\n",
      "Fold test accuracy for adaboost: 0.5648\n",
      "-------------------------------------\n",
      "Fold training accuracy for adaboost: 0.5841\n",
      "Fold test accuracy for adaboost: 0.4926\n",
      "-------------------------------------\n",
      "Average training accuracy for adaboost: 0.5861\n",
      "Average test accuracy for adaboost: 0.5269\n"
     ]
    }
   ],
   "source": [
    "print(\"AdaBoost  Results:-\")\n",
    "\n",
    "# Function to get the predominant class and its average record for each cluster\n",
    "def get_representative_record(cluster_df):\n",
    "    # Get the predominant class\n",
    "    predominant_class = cluster_df['target'].mode()[0]\n",
    "    \n",
    "    # Filter records of the predominant class\n",
    "    predominant_class_records = cluster_df[cluster_df['target'] == predominant_class].drop(columns=['cluster', 'target'])\n",
    "    \n",
    "    # Compute the average record for the predominant class\n",
    "    representative_record = predominant_class_records.mean()\n",
    "    representative_record['target'] = predominant_class\n",
    "    \n",
    "    return representative_record\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "#     print(f'Before kmean:{X_train.shape}')\n",
    "    # Apply KMeans clustering on the training set using cuML\n",
    "    kmeans = cuKMeans(n_clusters=10000)\n",
    "    kmeans.fit(X_train.values)  # Convert to NumPy array\n",
    "    \n",
    "    X_train_clustered = X_train.copy()  # Avoid SettingWithCopyWarning by working on a copy\n",
    "    X_train_clustered['cluster'] = kmeans.labels_\n",
    "    X_train_clustered['target'] = y_train.values\n",
    "    \n",
    "    # Generate the reduced training dataset\n",
    "    reduced_train_data = X_train_clustered.groupby('cluster').apply(get_representative_record).reset_index(drop=True)\n",
    "    \n",
    "#     print(f'After kmean:{X_train_reduced.shape}')\n",
    "\n",
    "    # Prepare data for model training\n",
    "    X_train_reduced = reduced_train_data.drop(columns=['target']).values  # Convert to NumPy array\n",
    "    y_train_reduced = reduced_train_data['target'].values  # Convert to NumPy array\n",
    "    \n",
    "#     print(\"After KMeans\")\n",
    "#     print(X_train_reduced.shape)\n",
    "    \n",
    "    # Train the model on the reduced training set\n",
    "    model_4.fit(X_train_reduced, y_train_reduced)\n",
    "    \n",
    "    # Predict on the training set\n",
    "    y_train_pred = model_4.predict(X_train_reduced)\n",
    "    train_accuracy = accuracy_score(y_train_reduced, y_train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Predict on the original test set\n",
    "    y_test_pred = model_4.predict(X_test.values)  # Convert to NumPy array\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(f'Fold training accuracy for adaboost: {train_accuracy:.4f}')\n",
    "    print(f'Fold test accuracy for adaboost: {test_accuracy:.4f}')\n",
    "    print(\"-------------------------------------\")\n",
    "#     break\n",
    "    \n",
    "# Calculate the average accuracies\n",
    "average_train_accuracy = np.mean(train_accuracies)\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "print(f'Average training accuracy for adaboost: {average_train_accuracy:.4f}')\n",
    "print(f'Average test accuracy for adaboost: {average_test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T03:56:07.758018Z",
     "iopub.status.busy": "2024-06-04T03:56:07.757382Z",
     "iopub.status.idle": "2024-06-04T03:56:07.771713Z",
     "shell.execute_reply": "2024-06-04T03:56:07.770880Z",
     "shell.execute_reply.started": "2024-06-04T03:56:07.757964Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from cuml.cluster import KMeans as cuKMeans  # Import cuML KMeans for GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T03:56:13.234298Z",
     "iopub.status.busy": "2024-06-04T03:56:13.233462Z",
     "iopub.status.idle": "2024-06-04T03:56:13.238284Z",
     "shell.execute_reply": "2024-06-04T03:56:13.237253Z",
     "shell.execute_reply.started": "2024-06-04T03:56:13.234266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the Naive Bayes classifier\n",
    "model_3 = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T03:56:15.467122Z",
     "iopub.status.busy": "2024-06-04T03:56:15.466751Z",
     "iopub.status.idle": "2024-06-04T03:56:15.471849Z",
     "shell.execute_reply": "2024-06-04T03:56:15.470984Z",
     "shell.execute_reply.started": "2024-06-04T03:56:15.467092Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Define the StratifiedShuffleSplit cross-validator\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "train_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T04:23:21.723115Z",
     "iopub.status.busy": "2024-06-04T04:23:21.722739Z",
     "iopub.status.idle": "2024-06-04T04:49:56.825767Z",
     "shell.execute_reply": "2024-06-04T04:49:56.824777Z",
     "shell.execute_reply.started": "2024-06-04T04:23:21.723086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes Result\n",
      "Fold training accuracy for Naive : 0.52\n",
      "Fold test accuracy for Naive: 0.46\n",
      "---------------------------\n",
      "Fold training accuracy for Naive : 0.51\n",
      "Fold test accuracy for Naive: 0.44\n",
      "---------------------------\n",
      "Fold training accuracy for Naive : 0.51\n",
      "Fold test accuracy for Naive: 0.43\n",
      "---------------------------\n",
      "Fold training accuracy for Naive : 0.51\n",
      "Fold test accuracy for Naive: 0.47\n",
      "---------------------------\n",
      "Fold training accuracy for Naive : 0.51\n",
      "Fold test accuracy for Naive: 0.44\n",
      "---------------------------\n",
      "Fold training accuracy for Naive : 0.51\n",
      "Fold test accuracy for Naive: 0.45\n",
      "---------------------------\n",
      "Fold training accuracy for Naive : 0.52\n",
      "Fold test accuracy for Naive: 0.46\n",
      "---------------------------\n",
      "Fold training accuracy for Naive : 0.51\n",
      "Fold test accuracy for Naive: 0.45\n",
      "---------------------------\n",
      "Fold training accuracy for Naive : 0.52\n",
      "Fold test accuracy for Naive: 0.46\n",
      "---------------------------\n",
      "Fold training accuracy for Naive : 0.51\n",
      "Fold test accuracy for Naive: 0.45\n",
      "---------------------------\n",
      "Average training accuracy Naive: 0.5138\n",
      "Average test accuracy for Naive: 0.4512\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive bayes Result\")\n",
    "# Function to get the predominant class and its average record for each cluster\n",
    "def get_representative_record(cluster_df):\n",
    "    # Get the predominant class\n",
    "    predominant_class = cluster_df['target'].mode()[0]\n",
    "    \n",
    "    # Filter records of the predominant class\n",
    "    predominant_class_records = cluster_df[cluster_df['target'] == predominant_class].drop(columns=['cluster', 'target'])\n",
    "    \n",
    "    # Compute the average record for the predominant class\n",
    "    representative_record = predominant_class_records.mean()\n",
    "    representative_record['target'] = predominant_class\n",
    "    \n",
    "    return representative_record\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "#     print(\"Before KMeans\")\n",
    "#     print(X_train.shape)\n",
    "    \n",
    "    # Apply KMeans clustering on the training set using cuML\n",
    "    kmeans = cuKMeans(n_clusters=10000)\n",
    "    kmeans.fit(X_train.values)  # Convert to NumPy array\n",
    "    \n",
    "    # Create a DataFrame with cluster labels\n",
    "    X_train_clustered = X_train.copy()  # Avoid SettingWithCopyWarning by working on a copy\n",
    "    X_train_clustered['cluster'] = kmeans.labels_\n",
    "    X_train_clustered['target'] = y_train.values\n",
    "    \n",
    "    # Generate the reduced training dataset\n",
    "    reduced_train_data = X_train_clustered.groupby('cluster').apply(get_representative_record).reset_index(drop=True)\n",
    "    \n",
    "    # Prepare data for model training\n",
    "    X_train_reduced = reduced_train_data.drop(columns=['target']).values  # Convert to NumPy array\n",
    "    y_train_reduced = reduced_train_data['target'].values  # Convert to NumPy array\n",
    "    \n",
    "#     print(\"After KMeans\")\n",
    "#     print(X_train_reduced.shape)\n",
    "    \n",
    "    # Train the model on the reduced training set\n",
    "    model_3.fit(X_train_reduced, y_train_reduced)\n",
    "    \n",
    "    # Predict on the training set\n",
    "    y_train_pred = model_3.predict(X_train_reduced)\n",
    "    train_accuracy = accuracy_score(y_train_reduced, y_train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Predict on the original test set\n",
    "    y_test_pred = model_3.predict(X_test.values)  # Convert to NumPy array\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(f'Fold training accuracy for Naive : {train_accuracy:.2f}')\n",
    "    print(f'Fold test accuracy for Naive: {test_accuracy:.2f}')\n",
    "    print(\"---------------------------\")\n",
    "#     break\n",
    "\n",
    "# Calculate the average accuracies\n",
    "average_train_accuracy = np.mean(train_accuracies)\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "print(f'Average training accuracy Naive: {average_train_accuracy:.4f}')\n",
    "print(f'Average test accuracy for Naive: {average_test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:48:33.680265Z",
     "iopub.status.busy": "2024-06-04T06:48:33.679924Z",
     "iopub.status.idle": "2024-06-04T06:48:33.807468Z",
     "shell.execute_reply": "2024-06-04T06:48:33.806681Z",
     "shell.execute_reply.started": "2024-06-04T06:48:33.680237Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import cudf\n",
    "import cuml\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:48:48.815318Z",
     "iopub.status.busy": "2024-06-04T06:48:48.814696Z",
     "iopub.status.idle": "2024-06-04T06:48:48.819624Z",
     "shell.execute_reply": "2024-06-04T06:48:48.818627Z",
     "shell.execute_reply.started": "2024-06-04T06:48:48.815285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_2 = SVC(kernel='linear', C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:48:52.292842Z",
     "iopub.status.busy": "2024-06-04T06:48:52.292046Z",
     "iopub.status.idle": "2024-06-04T06:48:52.297571Z",
     "shell.execute_reply": "2024-06-04T06:48:52.296666Z",
     "shell.execute_reply.started": "2024-06-04T06:48:52.292805Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Define the StratifiedShuffleSplit cross-validator\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "train_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T06:48:56.554263Z",
     "iopub.status.busy": "2024-06-04T06:48:56.553381Z",
     "iopub.status.idle": "2024-06-04T07:17:31.470918Z",
     "shell.execute_reply": "2024-06-04T07:17:31.469981Z",
     "shell.execute_reply.started": "2024-06-04T06:48:56.554222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Results\n",
      "Fold model_2 training  accuracy for SVM: 0.6371\n",
      "Fold model_2 test accuracy for SVM: 0.5920\n",
      "---------------------------\n",
      "Fold model_2 training  accuracy for SVM: 0.6405\n",
      "Fold model_2 test accuracy for SVM: 0.5859\n",
      "---------------------------\n",
      "Fold model_2 training  accuracy for SVM: 0.6339\n",
      "Fold model_2 test accuracy for SVM: 0.5977\n",
      "---------------------------\n",
      "Fold model_2 training  accuracy for SVM: 0.6395\n",
      "Fold model_2 test accuracy for SVM: 0.5999\n",
      "---------------------------\n",
      "Fold model_2 training  accuracy for SVM: 0.6372\n",
      "Fold model_2 test accuracy for SVM: 0.5955\n",
      "---------------------------\n",
      "Fold model_2 training  accuracy for SVM: 0.6367\n",
      "Fold model_2 test accuracy for SVM: 0.6055\n",
      "---------------------------\n",
      "Fold model_2 training  accuracy for SVM: 0.6428\n",
      "Fold model_2 test accuracy for SVM: 0.6085\n",
      "---------------------------\n",
      "Fold model_2 training  accuracy for SVM: 0.6343\n",
      "Fold model_2 test accuracy for SVM: 0.5857\n",
      "---------------------------\n",
      "Fold model_2 training  accuracy for SVM: 0.6364\n",
      "Fold model_2 test accuracy for SVM: 0.5944\n",
      "---------------------------\n",
      "Fold model_2 training  accuracy for SVM: 0.6344\n",
      "Fold model_2 test accuracy for SVM: 0.5857\n",
      "---------------------------\n",
      "Average training accuracy for SVM: 0.6373\n",
      "Average test accuracy for SVM: 0.5951\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Results\")\n",
    "from cuml.cluster import KMeans as cuKMeans  # Import cuML KMeans for GPU acceleration\n",
    "# Function to get the predominant class and its average record for each cluster\n",
    "def get_representative_record(cluster_df):\n",
    "    # Get the predominant class\n",
    "    predominant_class = cluster_df['target'].mode()[0]\n",
    "    \n",
    "    # Filter records of the predominant class\n",
    "    predominant_class_records = cluster_df[cluster_df['target'] == predominant_class].drop(columns=['cluster', 'target'])\n",
    "    \n",
    "    # Compute the average record for the predominant class\n",
    "    representative_record = predominant_class_records.mean()\n",
    "    representative_record['target'] = predominant_class\n",
    "    \n",
    "    return representative_record\n",
    "  \n",
    "# Perform StratifiedShuffleSplit cross-validation\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "#     print(\"Before Kmean\")\n",
    "#     print(X_train.shape)\n",
    "    \n",
    "    # Apply KMeans clustering on the training set using cuML\n",
    "     # Define the number of clusters\n",
    "    kmeans = cuKMeans(n_clusters=10000)\n",
    "    kmeans.fit(X_train.values)  # Convert to NumPy array  <-- Change: Convert DataFrame to NumPy array\n",
    "\n",
    "    \n",
    "    X_train_clustered = X_train.copy()  # Avoid SettingWithCopyWarning by working on a copy\n",
    "    X_train_clustered['cluster'] = kmeans.labels_\n",
    "    X_train_clustered['target'] = y_train.values\n",
    "    \n",
    "    # Generate the reduced training dataset\n",
    "    reduced_train_data = X_train_clustered.groupby('cluster').apply(get_representative_record).reset_index(drop=True)\n",
    "    \n",
    "    # Prepare data for model training\n",
    "    X_train_reduced = reduced_train_data.drop(columns=['target']).values # Convert to NumPy array  <-- Change: Convert Series to NumPy array\n",
    "    y_train_reduced = reduced_train_data['target'].values # Convert to NumPy array  <-- Change: Convert Series to NumPy array\n",
    "    \n",
    "#     print(\"After kmean\")\n",
    "#     print(X_train_reduced.shape)\n",
    "    # Train the model on the reduced training set\n",
    "    model_2.fit(X_train_reduced, y_train_reduced)\n",
    "    \n",
    "    # Predict on the training set\n",
    "    y_train_pred = model_2.predict(X_train_reduced)\n",
    "    train_accuracy = accuracy_score(y_train_reduced, y_train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Predict on the original test set\n",
    "    y_test_pred = model_2.predict(X_test.values) # Convert to NumPy array  <-- Change: Convert DataFrame to NumPy array\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(f'Fold model_2 training  accuracy for SVM: {train_accuracy:.4f}')\n",
    "    print(f'Fold model_2 test accuracy for SVM: {test_accuracy:.4f}')\n",
    "    print(\"---------------------------\")\n",
    "#     break\n",
    "    \n",
    "\n",
    "# Calculate the average accuracies\n",
    "average_train_accuracy = np.mean(train_accuracies)\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "print(f'Average training accuracy for SVM: {average_train_accuracy:.4f}')\n",
    "print(f'Average test accuracy for SVM: {average_test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T07:19:35.587516Z",
     "iopub.status.busy": "2024-06-04T07:19:35.586759Z",
     "iopub.status.idle": "2024-06-04T07:19:35.592521Z",
     "shell.execute_reply": "2024-06-04T07:19:35.591533Z",
     "shell.execute_reply.started": "2024-06-04T07:19:35.587479Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import cudf\n",
    "import cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T07:19:38.753899Z",
     "iopub.status.busy": "2024-06-04T07:19:38.753503Z",
     "iopub.status.idle": "2024-06-04T07:19:38.759367Z",
     "shell.execute_reply": "2024-06-04T07:19:38.758306Z",
     "shell.execute_reply.started": "2024-06-04T07:19:38.753867Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Perform StratifiedShuffleSplit cross-validation\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=42)\n",
    "train_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T07:19:44.591934Z",
     "iopub.status.busy": "2024-06-04T07:19:44.591554Z",
     "iopub.status.idle": "2024-06-04T07:19:44.597123Z",
     "shell.execute_reply": "2024-06-04T07:19:44.596076Z",
     "shell.execute_reply.started": "2024-06-04T07:19:44.591904Z"
    }
   },
   "outputs": [],
   "source": [
    "model_1 = RandomForestClassifier(class_weight=\"balanced\",n_estimators=1000, max_depth = 9, min_samples_split=10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:06:40.631215Z",
     "iopub.status.busy": "2024-06-04T08:06:40.630466Z",
     "iopub.status.idle": "2024-06-04T08:40:35.535655Z",
     "shell.execute_reply": "2024-06-04T08:40:35.534628Z",
     "shell.execute_reply.started": "2024-06-04T08:06:40.631178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold training accuracy for Random Forest : 0.9049\n",
      "Fold test accuracy for Random Forest: 0.7617\n",
      "------------------------------\n",
      "Fold training accuracy for Random Forest : 0.9106\n",
      "Fold test accuracy for Random Forest: 0.7678\n",
      "------------------------------\n",
      "Fold training accuracy for Random Forest : 0.9047\n",
      "Fold test accuracy for Random Forest: 0.7633\n",
      "------------------------------\n",
      "Fold training accuracy for Random Forest : 0.9088\n",
      "Fold test accuracy for Random Forest: 0.7584\n",
      "------------------------------\n",
      "Fold training accuracy for Random Forest : 0.9045\n",
      "Fold test accuracy for Random Forest: 0.7728\n",
      "------------------------------\n",
      "Fold training accuracy for Random Forest : 0.9056\n",
      "Fold test accuracy for Random Forest: 0.7511\n",
      "------------------------------\n",
      "Fold training accuracy for Random Forest : 0.9093\n",
      "Fold test accuracy for Random Forest: 0.7722\n",
      "------------------------------\n",
      "Fold training accuracy for Random Forest : 0.9069\n",
      "Fold test accuracy for Random Forest: 0.7612\n",
      "------------------------------\n",
      "Fold training accuracy for Random Forest : 0.9083\n",
      "Fold test accuracy for Random Forest: 0.7580\n",
      "------------------------------\n",
      "Fold training accuracy for Random Forest : 0.9148\n",
      "Fold test accuracy for Random Forest: 0.7690\n",
      "------------------------------\n",
      "Average training accuracy for RF: 0.9078\n",
      "Average test accuracy for RF: 0.7635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from cuml.ensemble import RandomForestClassifier as cuRFClassifier  # Import cuML RandomForest for GPU acceleration\n",
    "from cuml.cluster import KMeans as cuKMeans  # Import cuML KMeans for GPU acceleration\n",
    "\n",
    "# Function to get the predominant class and its average record for each cluster\n",
    "def get_representative_record(cluster_df):\n",
    "    # Get the predominant class\n",
    "    predominant_class = cluster_df['target'].mode()[0]\n",
    "    \n",
    "    # Filter records of the predominant class\n",
    "    predominant_class_records = cluster_df[cluster_df['target'] == predominant_class].drop(columns=['cluster', 'target'])\n",
    "    \n",
    "    # Compute the average record for the predominant class\n",
    "    representative_record = predominant_class_records.mean()\n",
    "    representative_record['target'] = predominant_class\n",
    "    \n",
    "    return representative_record\n",
    "  \n",
    "# Perform StratifiedShuffleSplit cross-validation\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Apply KMeans clustering on the training set using cuML\n",
    "     # Define the number of clusters\n",
    "    kmeans = cuKMeans(n_clusters=10000)\n",
    "    kmeans.fit(X_train.values)  # Convert to NumPy array  <-- Change: Convert DataFrame to NumPy array\n",
    "\n",
    "    X_train_clustered = X_train.copy()  # Avoid SettingWithCopyWarning by working on a copy\n",
    "    X_train_clustered['cluster'] = kmeans.labels_\n",
    "    X_train_clustered['target'] = y_train.values\n",
    "    \n",
    "    # Generate the reduced training dataset\n",
    "    reduced_train_data = X_train_clustered.groupby('cluster').apply(get_representative_record).reset_index(drop=True)\n",
    "    \n",
    "    # Prepare data for model training\n",
    "    X_train_reduced = reduced_train_data.drop(columns=['target']).values # Convert to NumPy array  <-- Change: Convert Series to NumPy array\n",
    "    y_train_reduced = reduced_train_data['target'].values # Convert to NumPy array  <-- Change: Convert Series to NumPy array\n",
    "    \n",
    "    # Train the model on the reduced training set\n",
    "    model_1.fit(X_train_reduced, y_train_reduced)\n",
    "    \n",
    "    # Predict on the training set\n",
    "    y_train_pred = model_1.predict(X_train_reduced)\n",
    "    train_accuracy = accuracy_score(y_train_reduced, y_train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Predict on the original test set\n",
    "    y_test_pred = model_1.predict(X_test.values) # Convert to NumPy array  <-- Change: Convert DataFrame to NumPy array\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(f'Fold training accuracy for Random Forest : {train_accuracy:.4f}')\n",
    "    print(f'Fold test accuracy for Random Forest: {test_accuracy:.4f}')\n",
    "    print(\"------------------------------\")\n",
    "#     break\n",
    "    \n",
    "# Calculate the average accuracies\n",
    "average_train_accuracy = np.mean(train_accuracies)\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "print(f'Average training accuracy for RF: {average_train_accuracy:.4f}')\n",
    "print(f'Average test accuracy for RF: {average_test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------END-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T15:06:40.653299Z",
     "iopub.status.busy": "2024-03-06T15:06:40.652193Z",
     "iopub.status.idle": "2024-03-06T15:07:03.863571Z",
     "shell.execute_reply": "2024-03-06T15:07:03.862745Z",
     "shell.execute_reply.started": "2024-03-06T15:06:40.653263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN FEATURES:\n",
      "\n",
      "\n",
      "TEST FEATURES:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>A0T0G0C10</th>\n",
       "      <th>A0T0G1C9</th>\n",
       "      <th>A0T0G2C8</th>\n",
       "      <th>A0T0G3C7</th>\n",
       "      <th>A0T0G4C6</th>\n",
       "      <th>A0T0G5C5</th>\n",
       "      <th>A0T0G6C4</th>\n",
       "      <th>A0T0G7C3</th>\n",
       "      <th>A0T0G8C2</th>\n",
       "      <th>...</th>\n",
       "      <th>A8T0G0C2</th>\n",
       "      <th>A8T0G1C1</th>\n",
       "      <th>A8T0G2C0</th>\n",
       "      <th>A8T1G0C1</th>\n",
       "      <th>A8T1G1C0</th>\n",
       "      <th>A8T2G0C0</th>\n",
       "      <th>A9T0G0C1</th>\n",
       "      <th>A9T0G1C0</th>\n",
       "      <th>A9T1G0C0</th>\n",
       "      <th>A10T0G0C0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119737</th>\n",
       "      <td>0.598686</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00950</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00650</td>\n",
       "      <td>0.01450</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.01700</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72272</th>\n",
       "      <td>0.361359</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00600</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.01750</td>\n",
       "      <td>0.02050</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158154</th>\n",
       "      <td>0.790773</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.00440</td>\n",
       "      <td>0.00470</td>\n",
       "      <td>0.00435</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01045</td>\n",
       "      <td>0.02935</td>\n",
       "      <td>0.01775</td>\n",
       "      <td>0.02955</td>\n",
       "      <td>0.03215</td>\n",
       "      <td>0.01825</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65426</th>\n",
       "      <td>0.327128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.01150</td>\n",
       "      <td>0.01165</td>\n",
       "      <td>0.01010</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00700</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00710</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.01055</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30074</th>\n",
       "      <td>0.150367</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97771</th>\n",
       "      <td>0.488855</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.01165</td>\n",
       "      <td>0.01265</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00330</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>0.00635</td>\n",
       "      <td>0.00695</td>\n",
       "      <td>0.00355</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59813</th>\n",
       "      <td>0.299063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.02350</td>\n",
       "      <td>0.01500</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>0.00950</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103735</th>\n",
       "      <td>0.518675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180226</th>\n",
       "      <td>0.901134</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119389</th>\n",
       "      <td>0.596946</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id  A0T0G0C10  A0T0G1C9  A0T0G2C8  A0T0G3C7  A0T0G4C6  A0T0G5C5  \\\n",
       "119737  0.598686     0.0000    0.0010    0.0010    0.0130   0.01350   0.01050   \n",
       "72272   0.361359     0.0000    0.0020    0.0050    0.0110   0.00850   0.01300   \n",
       "158154  0.790773     0.0000    0.0003    0.0020    0.0045   0.00440   0.00470   \n",
       "65426   0.327128     0.0001    0.0001    0.0025    0.0157   0.01150   0.01165   \n",
       "30074   0.150367     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "...          ...        ...       ...       ...       ...       ...       ...   \n",
       "97771   0.488855     0.0001    0.0011    0.0045    0.0116   0.01165   0.01265   \n",
       "59813   0.299063     0.0000    0.0020    0.0050    0.0110   0.00900   0.01300   \n",
       "103735  0.518675     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "180226  0.901134     0.0000    0.0020    0.0050    0.0170   0.01200   0.01650   \n",
       "119389  0.596946     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "\n",
       "        A0T0G6C4  A0T0G7C3  A0T0G8C2  ...  A8T0G0C2  A8T0G1C1  A8T0G2C0  \\\n",
       "119737   0.00950    0.0080    0.0040  ...   0.00650   0.01450   0.00800   \n",
       "72272    0.00750    0.0130    0.0000  ...   0.00600   0.01300   0.00900   \n",
       "158154   0.00435    0.0048    0.0022  ...   0.01045   0.02935   0.01775   \n",
       "65426    0.01010    0.0161    0.0024  ...   0.00700   0.01300   0.00710   \n",
       "30074    0.50000    1.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "97771    0.01100    0.0132    0.0047  ...   0.00330   0.00625   0.00345   \n",
       "59813    0.00750    0.0130    0.0000  ...   0.01000   0.02350   0.01500   \n",
       "103735   0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "180226   0.01200    0.0220    0.0010  ...   0.00300   0.01050   0.00750   \n",
       "119389   0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "\n",
       "        A8T1G0C1  A8T1G1C0  A8T2G0C0  A9T0G0C1  A9T0G1C0  A9T1G0C0  A10T0G0C0  \n",
       "119737   0.01700   0.01200   0.01100    0.0030    0.0040    0.0025      0.000  \n",
       "72272    0.01750   0.02050   0.01550    0.0020    0.0030    0.0030      0.000  \n",
       "158154   0.02955   0.03215   0.01825    0.0074    0.0109    0.0048      0.002  \n",
       "65426    0.01425   0.01575   0.01055    0.0033    0.0025    0.0028      0.000  \n",
       "30074    0.00000   0.00000   0.50000    0.0000    0.0000    0.0000      0.000  \n",
       "...          ...       ...       ...       ...       ...       ...        ...  \n",
       "97771    0.00635   0.00695   0.00355    0.0013    0.0013    0.0010      0.001  \n",
       "59813    0.01900   0.02500   0.00950    0.0030    0.0070    0.0030      0.000  \n",
       "103735   0.00000   0.00000   0.00000    0.0000    0.0000    0.0000      0.000  \n",
       "180226   0.01000   0.01250   0.00250    0.0020    0.0040    0.0005      0.000  \n",
       "119389   0.00000   0.00000   0.00000    0.0000    0.0000    0.0000      0.000  \n",
       "\n",
       "[60000 rows x 287 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# This function accepts a dataframe.\n",
    "# It selects the dataframe's numerical columns and normalizes those columns.\n",
    "# The dataframe with normalized values are returned.\n",
    "def normalize_numerical_features(train_features, test_features):\n",
    "    \n",
    "    train_numerical_features = train_features._get_numeric_data() # Selecting all the numerical features of Train\n",
    "    test_numerical_features = test_features._get_numeric_data() # Selecting all the numerical features of Test\n",
    "    numerical_columns = list(set(train_numerical_features.columns).intersection(set(test_numerical_features.columns)))\n",
    "    train_numerical_features = train_numerical_features[numerical_columns]\n",
    "    test_numerical_features = test_numerical_features[numerical_columns]\n",
    "    \n",
    "    # Determining the value_range and min_value to be used in normalization using Train\n",
    "    value_range = train_numerical_features.max() - train_numerical_features.min()\n",
    "    min_value = train_numerical_features.min()\n",
    "    \n",
    "    # Getting the normalized feature values\n",
    "    train_normalized_numerical_features = (train_numerical_features - min_value) / value_range\n",
    "    test_normalized_numerical_features = (test_numerical_features - min_value) / value_range\n",
    "    \n",
    "    # Replacing the normalized feature values in the original dataset.\n",
    "    train_features[numerical_columns] = train_normalized_numerical_features\n",
    "    test_features[numerical_columns] = test_normalized_numerical_features\n",
    "    \n",
    " \n",
    "    # All the numerical features in train and test are now normalized.\n",
    "    return (train_features, test_features)\n",
    "train_features, test_features = normalize_numerical_features(train_features, test_features)\n",
    "print('TRAIN FEATURES:')\n",
    "train_features\n",
    "print('\\n\\nTEST FEATURES:')\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T15:07:28.084355Z",
     "iopub.status.busy": "2024-03-06T15:07:28.083973Z",
     "iopub.status.idle": "2024-03-06T15:07:28.147370Z",
     "shell.execute_reply": "2024-03-06T15:07:28.146435Z",
     "shell.execute_reply.started": "2024-03-06T15:07:28.084304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN FEATURES:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>A0T0G0C10</th>\n",
       "      <th>A0T0G1C9</th>\n",
       "      <th>A0T0G2C8</th>\n",
       "      <th>A0T0G3C7</th>\n",
       "      <th>A0T0G4C6</th>\n",
       "      <th>A0T0G5C5</th>\n",
       "      <th>A0T0G6C4</th>\n",
       "      <th>A0T0G7C3</th>\n",
       "      <th>A0T0G8C2</th>\n",
       "      <th>...</th>\n",
       "      <th>A8T0G0C2</th>\n",
       "      <th>A8T0G1C1</th>\n",
       "      <th>A8T0G2C0</th>\n",
       "      <th>A8T1G0C1</th>\n",
       "      <th>A8T1G1C0</th>\n",
       "      <th>A8T2G0C0</th>\n",
       "      <th>A9T0G0C1</th>\n",
       "      <th>A9T0G1C0</th>\n",
       "      <th>A9T1G0C0</th>\n",
       "      <th>A10T0G0C0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21269</th>\n",
       "      <td>0.106341</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.01600</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187660</th>\n",
       "      <td>0.938304</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.05110</td>\n",
       "      <td>0.06795</td>\n",
       "      <td>0.05280</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00325</td>\n",
       "      <td>0.00605</td>\n",
       "      <td>0.00380</td>\n",
       "      <td>0.00680</td>\n",
       "      <td>0.00765</td>\n",
       "      <td>0.00425</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.01035</td>\n",
       "      <td>0.01180</td>\n",
       "      <td>0.00975</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00305</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.00385</td>\n",
       "      <td>0.00760</td>\n",
       "      <td>0.00775</td>\n",
       "      <td>0.00420</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.00095</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184577</th>\n",
       "      <td>0.922889</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37127</th>\n",
       "      <td>0.185632</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>0.599396</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103694</th>\n",
       "      <td>0.518470</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00600</td>\n",
       "      <td>0.00950</td>\n",
       "      <td>0.00400</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>0.659662</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.00665</td>\n",
       "      <td>0.00680</td>\n",
       "      <td>0.00575</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01630</td>\n",
       "      <td>0.03670</td>\n",
       "      <td>0.02165</td>\n",
       "      <td>0.04940</td>\n",
       "      <td>0.06465</td>\n",
       "      <td>0.04640</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.01185</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146867</th>\n",
       "      <td>0.734337</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>0.609791</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.00430</td>\n",
       "      <td>0.00470</td>\n",
       "      <td>0.00435</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01205</td>\n",
       "      <td>0.02870</td>\n",
       "      <td>0.01620</td>\n",
       "      <td>0.02915</td>\n",
       "      <td>0.03370</td>\n",
       "      <td>0.01880</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140000 rows Ã— 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id  A0T0G0C10  A0T0G1C9  A0T0G2C8  A0T0G3C7  A0T0G4C6  A0T0G5C5  \\\n",
       "21269   0.106341     0.0000    0.0020    0.0050    0.0110   0.00850   0.01300   \n",
       "187660  0.938304     0.0001    0.0012    0.0091    0.0462   0.05110   0.06795   \n",
       "774     0.003865     0.0001    0.0013    0.0043    0.0099   0.01035   0.01180   \n",
       "184577  0.922889     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "37127   0.185632     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "...          ...        ...       ...       ...       ...       ...       ...   \n",
       "119879  0.599396     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "103694  0.518470     0.0000    0.0010    0.0040    0.0150   0.01550   0.01350   \n",
       "131932  0.659662     0.0003    0.0009    0.0029    0.0058   0.00665   0.00680   \n",
       "146867  0.734337     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "121958  0.609791     0.0000    0.0003    0.0020    0.0043   0.00430   0.00470   \n",
       "\n",
       "        A0T0G6C4  A0T0G7C3  A0T0G8C2  ...  A8T0G0C2  A8T0G1C1  A8T0G2C0  \\\n",
       "21269    0.00750    0.0130    0.0000  ...   0.00800   0.01350   0.00900   \n",
       "187660   0.05280    0.0457    0.0075  ...   0.00325   0.00605   0.00380   \n",
       "774      0.00975    0.0117    0.0046  ...   0.00305   0.00750   0.00385   \n",
       "184577   0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.05000   \n",
       "37127    0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "119879   0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "103694   0.01050    0.0150    0.0020  ...   0.00600   0.00950   0.00400   \n",
       "131932   0.00575    0.0066    0.0028  ...   0.01630   0.03670   0.02165   \n",
       "146867   0.00000    0.0000    0.0000  ...   0.05000   0.05000   0.00000   \n",
       "121958   0.00435    0.0048    0.0022  ...   0.01205   0.02870   0.01620   \n",
       "\n",
       "        A8T1G0C1  A8T1G1C0  A8T2G0C0  A9T0G0C1  A9T0G1C0  A9T1G0C0  A10T0G0C0  \n",
       "21269    0.01600   0.01550   0.00900    0.0030    0.0020   0.00200      0.000  \n",
       "187660   0.00680   0.00765   0.00425    0.0022    0.0018   0.00140      0.001  \n",
       "774      0.00760   0.00775   0.00420    0.0022    0.0026   0.00095      0.001  \n",
       "184577   0.00000   0.00000   0.00000    0.0000    0.0000   0.00000      0.000  \n",
       "37127    0.00000   0.00000   0.00000    0.0000    0.0000   0.00000      0.000  \n",
       "...          ...       ...       ...       ...       ...       ...        ...  \n",
       "119879   0.00000   0.00000   0.00000    0.0000    0.0000   0.00000      0.000  \n",
       "103694   0.01100   0.01000   0.00250    0.0010    0.0030   0.00100      0.000  \n",
       "131932   0.04940   0.06465   0.04640    0.0143    0.0213   0.01185      0.002  \n",
       "146867   0.05000   0.10000   0.00000    0.1000    0.1000   0.00000      0.000  \n",
       "121958   0.02915   0.03370   0.01880    0.0082    0.0106   0.00480      0.001  \n",
       "\n",
       "[140000 rows x 287 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('TRAIN FEATURES:')\n",
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T15:07:32.911019Z",
     "iopub.status.busy": "2024-03-06T15:07:32.910636Z",
     "iopub.status.idle": "2024-03-06T15:07:32.958400Z",
     "shell.execute_reply": "2024-03-06T15:07:32.957573Z",
     "shell.execute_reply.started": "2024-03-06T15:07:32.910990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TEST FEATURES:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>A0T0G0C10</th>\n",
       "      <th>A0T0G1C9</th>\n",
       "      <th>A0T0G2C8</th>\n",
       "      <th>A0T0G3C7</th>\n",
       "      <th>A0T0G4C6</th>\n",
       "      <th>A0T0G5C5</th>\n",
       "      <th>A0T0G6C4</th>\n",
       "      <th>A0T0G7C3</th>\n",
       "      <th>A0T0G8C2</th>\n",
       "      <th>...</th>\n",
       "      <th>A8T0G0C2</th>\n",
       "      <th>A8T0G1C1</th>\n",
       "      <th>A8T0G2C0</th>\n",
       "      <th>A8T1G0C1</th>\n",
       "      <th>A8T1G1C0</th>\n",
       "      <th>A8T2G0C0</th>\n",
       "      <th>A9T0G0C1</th>\n",
       "      <th>A9T0G1C0</th>\n",
       "      <th>A9T1G0C0</th>\n",
       "      <th>A10T0G0C0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119737</th>\n",
       "      <td>0.598686</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00950</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00650</td>\n",
       "      <td>0.01450</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.01700</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72272</th>\n",
       "      <td>0.361359</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00600</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.01750</td>\n",
       "      <td>0.02050</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158154</th>\n",
       "      <td>0.790773</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.00440</td>\n",
       "      <td>0.00470</td>\n",
       "      <td>0.00435</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01045</td>\n",
       "      <td>0.02935</td>\n",
       "      <td>0.01775</td>\n",
       "      <td>0.02955</td>\n",
       "      <td>0.03215</td>\n",
       "      <td>0.01825</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65426</th>\n",
       "      <td>0.327128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.01150</td>\n",
       "      <td>0.01165</td>\n",
       "      <td>0.01010</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00700</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00710</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.01055</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30074</th>\n",
       "      <td>0.150367</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97771</th>\n",
       "      <td>0.488855</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.01165</td>\n",
       "      <td>0.01265</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00330</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>0.00635</td>\n",
       "      <td>0.00695</td>\n",
       "      <td>0.00355</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59813</th>\n",
       "      <td>0.299063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.02350</td>\n",
       "      <td>0.01500</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>0.00950</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103735</th>\n",
       "      <td>0.518675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180226</th>\n",
       "      <td>0.901134</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119389</th>\n",
       "      <td>0.596946</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id  A0T0G0C10  A0T0G1C9  A0T0G2C8  A0T0G3C7  A0T0G4C6  A0T0G5C5  \\\n",
       "119737  0.598686     0.0000    0.0010    0.0010    0.0130   0.01350   0.01050   \n",
       "72272   0.361359     0.0000    0.0020    0.0050    0.0110   0.00850   0.01300   \n",
       "158154  0.790773     0.0000    0.0003    0.0020    0.0045   0.00440   0.00470   \n",
       "65426   0.327128     0.0001    0.0001    0.0025    0.0157   0.01150   0.01165   \n",
       "30074   0.150367     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "...          ...        ...       ...       ...       ...       ...       ...   \n",
       "97771   0.488855     0.0001    0.0011    0.0045    0.0116   0.01165   0.01265   \n",
       "59813   0.299063     0.0000    0.0020    0.0050    0.0110   0.00900   0.01300   \n",
       "103735  0.518675     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "180226  0.901134     0.0000    0.0020    0.0050    0.0170   0.01200   0.01650   \n",
       "119389  0.596946     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "\n",
       "        A0T0G6C4  A0T0G7C3  A0T0G8C2  ...  A8T0G0C2  A8T0G1C1  A8T0G2C0  \\\n",
       "119737   0.00950    0.0080    0.0040  ...   0.00650   0.01450   0.00800   \n",
       "72272    0.00750    0.0130    0.0000  ...   0.00600   0.01300   0.00900   \n",
       "158154   0.00435    0.0048    0.0022  ...   0.01045   0.02935   0.01775   \n",
       "65426    0.01010    0.0161    0.0024  ...   0.00700   0.01300   0.00710   \n",
       "30074    0.50000    1.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "97771    0.01100    0.0132    0.0047  ...   0.00330   0.00625   0.00345   \n",
       "59813    0.00750    0.0130    0.0000  ...   0.01000   0.02350   0.01500   \n",
       "103735   0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "180226   0.01200    0.0220    0.0010  ...   0.00300   0.01050   0.00750   \n",
       "119389   0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "\n",
       "        A8T1G0C1  A8T1G1C0  A8T2G0C0  A9T0G0C1  A9T0G1C0  A9T1G0C0  A10T0G0C0  \n",
       "119737   0.01700   0.01200   0.01100    0.0030    0.0040    0.0025      0.000  \n",
       "72272    0.01750   0.02050   0.01550    0.0020    0.0030    0.0030      0.000  \n",
       "158154   0.02955   0.03215   0.01825    0.0074    0.0109    0.0048      0.002  \n",
       "65426    0.01425   0.01575   0.01055    0.0033    0.0025    0.0028      0.000  \n",
       "30074    0.00000   0.00000   0.50000    0.0000    0.0000    0.0000      0.000  \n",
       "...          ...       ...       ...       ...       ...       ...        ...  \n",
       "97771    0.00635   0.00695   0.00355    0.0013    0.0013    0.0010      0.001  \n",
       "59813    0.01900   0.02500   0.00950    0.0030    0.0070    0.0030      0.000  \n",
       "103735   0.00000   0.00000   0.00000    0.0000    0.0000    0.0000      0.000  \n",
       "180226   0.01000   0.01250   0.00250    0.0020    0.0040    0.0005      0.000  \n",
       "119389   0.00000   0.00000   0.00000    0.0000    0.0000    0.0000      0.000  \n",
       "\n",
       "[60000 rows x 287 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\n\\nTEST FEATURES:')\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-06T15:34:03.876847Z",
     "iopub.status.idle": "2024-03-06T15:34:03.877316Z",
     "shell.execute_reply": "2024-03-06T15:34:03.877093Z",
     "shell.execute_reply.started": "2024-03-06T15:34:03.877072Z"
    }
   },
   "outputs": [],
   "source": [
    "train_class=y_trainnew\n",
    "test_class=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-06T15:34:03.880540Z",
     "iopub.status.idle": "2024-03-06T15:34:03.880943Z",
     "shell.execute_reply": "2024-03-06T15:34:03.880777Z",
     "shell.execute_reply.started": "2024-03-06T15:34:03.880758Z"
    }
   },
   "outputs": [],
   "source": [
    "##Converting class labels to numeric values\n",
    "\n",
    "\n",
    "# This function accepts the class labels of train and test\n",
    "# It generates a mapper that maps from the class labels to the output neuron index number\n",
    "# For example if we consider IRIS dataset then the mapper would be like:\n",
    "# iris setosa -> 0\n",
    "# iris virginica -> 1\n",
    "# iris versicolor -> 2\n",
    "def class_label_to_output_neuron_index_mapper(train_class, test_class):\n",
    "    \n",
    "    class_label_to_output_neuron_index_mapper = {j: i for i, j in enumerate(train_class.astype('category').cat.categories)} \n",
    " \n",
    "    train_class = train_class.map(class_label_to_output_neuron_index_mapper)\n",
    "    test_class = test_class.map(class_label_to_output_neuron_index_mapper)\n",
    "    if test_class.dtype != train_class.dtype:\n",
    "        train_class = train_class.astype(test_class.dtype)\n",
    " \n",
    "    return (class_label_to_output_neuron_index_mapper, train_class, test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T18:27:34.158163Z",
     "iopub.status.busy": "2024-03-03T18:27:34.157309Z",
     "iopub.status.idle": "2024-03-03T18:27:34.205852Z",
     "shell.execute_reply": "2024-03-03T18:27:34.204913Z",
     "shell.execute_reply.started": "2024-03-03T18:27:34.158131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:\n",
      "\n",
      "\n",
      "TRAIN CLASS:\n",
      "\n",
      "\n",
      "TEST CLASS\n",
      "\n",
      "\n",
      "AFTER:\n",
      "\n",
      "THE MAP:\n",
      "\n",
      "\n",
      "TRAIN CLASS:\n",
      "\n",
      "\n",
      "TEST CLASS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119737    4\n",
       "72272     7\n",
       "158154    9\n",
       "65426     4\n",
       "30074     5\n",
       "         ..\n",
       "97771     4\n",
       "59813     2\n",
       "103735    4\n",
       "180226    3\n",
       "119389    9\n",
       "Name: target, Length: 60000, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('BEFORE:')\n",
    "print('\\n\\nTRAIN CLASS:')\n",
    "train_class\n",
    "print('\\n\\nTEST CLASS')\n",
    "test_class\n",
    "\n",
    "the_map, train_class, test_class = class_label_to_output_neuron_index_mapper(train_class, test_class)\n",
    "\n",
    "print('\\n\\nAFTER:')\n",
    "print('\\nTHE MAP:')\n",
    "the_map\n",
    "print('\\n\\nTRAIN CLASS:')\n",
    "train_class\n",
    "print('\\n\\nTEST CLASS')\n",
    "test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T18:27:40.032814Z",
     "iopub.status.busy": "2024-03-03T18:27:40.031956Z",
     "iopub.status.idle": "2024-03-03T18:27:40.189309Z",
     "shell.execute_reply": "2024-03-03T18:27:40.188412Z",
     "shell.execute_reply.started": "2024-03-03T18:27:40.032780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ENCODED TEST:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>A0T0G0C10</th>\n",
       "      <th>A0T0G1C9</th>\n",
       "      <th>A0T0G2C8</th>\n",
       "      <th>A0T0G3C7</th>\n",
       "      <th>A0T0G4C6</th>\n",
       "      <th>A0T0G5C5</th>\n",
       "      <th>A0T0G6C4</th>\n",
       "      <th>A0T0G7C3</th>\n",
       "      <th>A0T0G8C2</th>\n",
       "      <th>...</th>\n",
       "      <th>A8T0G1C1</th>\n",
       "      <th>A8T0G2C0</th>\n",
       "      <th>A8T1G0C1</th>\n",
       "      <th>A8T1G1C0</th>\n",
       "      <th>A8T2G0C0</th>\n",
       "      <th>A9T0G0C1</th>\n",
       "      <th>A9T0G1C0</th>\n",
       "      <th>A9T1G0C0</th>\n",
       "      <th>A10T0G0C0</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119737</th>\n",
       "      <td>0.598686</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00950</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01450</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.01700</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72272</th>\n",
       "      <td>0.361359</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.01750</td>\n",
       "      <td>0.02050</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158154</th>\n",
       "      <td>0.790773</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.00440</td>\n",
       "      <td>0.00470</td>\n",
       "      <td>0.00435</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02935</td>\n",
       "      <td>0.01775</td>\n",
       "      <td>0.02955</td>\n",
       "      <td>0.03215</td>\n",
       "      <td>0.01825</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.002</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65426</th>\n",
       "      <td>0.327128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.01150</td>\n",
       "      <td>0.01165</td>\n",
       "      <td>0.01010</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00710</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.01055</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30074</th>\n",
       "      <td>0.150367</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97771</th>\n",
       "      <td>0.488855</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.01165</td>\n",
       "      <td>0.01265</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>0.00635</td>\n",
       "      <td>0.00695</td>\n",
       "      <td>0.00355</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59813</th>\n",
       "      <td>0.299063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02350</td>\n",
       "      <td>0.01500</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>0.00950</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103735</th>\n",
       "      <td>0.518675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180226</th>\n",
       "      <td>0.901134</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119389</th>\n",
       "      <td>0.596946</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id  A0T0G0C10  A0T0G1C9  A0T0G2C8  A0T0G3C7  A0T0G4C6  A0T0G5C5  \\\n",
       "119737  0.598686     0.0000    0.0010    0.0010    0.0130   0.01350   0.01050   \n",
       "72272   0.361359     0.0000    0.0020    0.0050    0.0110   0.00850   0.01300   \n",
       "158154  0.790773     0.0000    0.0003    0.0020    0.0045   0.00440   0.00470   \n",
       "65426   0.327128     0.0001    0.0001    0.0025    0.0157   0.01150   0.01165   \n",
       "30074   0.150367     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "...          ...        ...       ...       ...       ...       ...       ...   \n",
       "97771   0.488855     0.0001    0.0011    0.0045    0.0116   0.01165   0.01265   \n",
       "59813   0.299063     0.0000    0.0020    0.0050    0.0110   0.00900   0.01300   \n",
       "103735  0.518675     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "180226  0.901134     0.0000    0.0020    0.0050    0.0170   0.01200   0.01650   \n",
       "119389  0.596946     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "\n",
       "        A0T0G6C4  A0T0G7C3  A0T0G8C2  ...  A8T0G1C1  A8T0G2C0  A8T1G0C1  \\\n",
       "119737   0.00950    0.0080    0.0040  ...   0.01450   0.00800   0.01700   \n",
       "72272    0.00750    0.0130    0.0000  ...   0.01300   0.00900   0.01750   \n",
       "158154   0.00435    0.0048    0.0022  ...   0.02935   0.01775   0.02955   \n",
       "65426    0.01010    0.0161    0.0024  ...   0.01300   0.00710   0.01425   \n",
       "30074    0.50000    1.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "97771    0.01100    0.0132    0.0047  ...   0.00625   0.00345   0.00635   \n",
       "59813    0.00750    0.0130    0.0000  ...   0.02350   0.01500   0.01900   \n",
       "103735   0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "180226   0.01200    0.0220    0.0010  ...   0.01050   0.00750   0.01000   \n",
       "119389   0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "\n",
       "        A8T1G1C0  A8T2G0C0  A9T0G0C1  A9T0G1C0  A9T1G0C0  A10T0G0C0  target  \n",
       "119737   0.01200   0.01100    0.0030    0.0040    0.0025      0.000       4  \n",
       "72272    0.02050   0.01550    0.0020    0.0030    0.0030      0.000       7  \n",
       "158154   0.03215   0.01825    0.0074    0.0109    0.0048      0.002       9  \n",
       "65426    0.01575   0.01055    0.0033    0.0025    0.0028      0.000       4  \n",
       "30074    0.00000   0.50000    0.0000    0.0000    0.0000      0.000       5  \n",
       "...          ...       ...       ...       ...       ...        ...     ...  \n",
       "97771    0.00695   0.00355    0.0013    0.0013    0.0010      0.001       4  \n",
       "59813    0.02500   0.00950    0.0030    0.0070    0.0030      0.000       2  \n",
       "103735   0.00000   0.00000    0.0000    0.0000    0.0000      0.000       4  \n",
       "180226   0.01250   0.00250    0.0020    0.0040    0.0005      0.000       3  \n",
       "119389   0.00000   0.00000    0.0000    0.0000    0.0000      0.000       9  \n",
       "\n",
       "[60000 rows x 288 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally we concatinate the feature columns and the class label column\n",
    "\n",
    "encoded_train = pd.concat([train_features, train_class], axis=1)\n",
    "encoded_test = pd.concat([test_features, test_class], axis=1)\n",
    "\n",
    "\n",
    "print('\\n\\nENCODED TEST:')\n",
    "encoded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T18:27:45.071631Z",
     "iopub.status.busy": "2024-03-03T18:27:45.071190Z",
     "iopub.status.idle": "2024-03-03T18:27:45.152613Z",
     "shell.execute_reply": "2024-03-03T18:27:45.151532Z",
     "shell.execute_reply.started": "2024-03-03T18:27:45.071595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED TRAIN:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>A0T0G0C10</th>\n",
       "      <th>A0T0G1C9</th>\n",
       "      <th>A0T0G2C8</th>\n",
       "      <th>A0T0G3C7</th>\n",
       "      <th>A0T0G4C6</th>\n",
       "      <th>A0T0G5C5</th>\n",
       "      <th>A0T0G6C4</th>\n",
       "      <th>A0T0G7C3</th>\n",
       "      <th>A0T0G8C2</th>\n",
       "      <th>...</th>\n",
       "      <th>A8T0G1C1</th>\n",
       "      <th>A8T0G2C0</th>\n",
       "      <th>A8T1G0C1</th>\n",
       "      <th>A8T1G1C0</th>\n",
       "      <th>A8T2G0C0</th>\n",
       "      <th>A9T0G0C1</th>\n",
       "      <th>A9T0G1C0</th>\n",
       "      <th>A9T1G0C0</th>\n",
       "      <th>A10T0G0C0</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21269</th>\n",
       "      <td>0.106341</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.01600</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187660</th>\n",
       "      <td>0.938304</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.05110</td>\n",
       "      <td>0.06795</td>\n",
       "      <td>0.05280</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00605</td>\n",
       "      <td>0.00380</td>\n",
       "      <td>0.00680</td>\n",
       "      <td>0.00765</td>\n",
       "      <td>0.00425</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.01035</td>\n",
       "      <td>0.01180</td>\n",
       "      <td>0.00975</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.00385</td>\n",
       "      <td>0.00760</td>\n",
       "      <td>0.00775</td>\n",
       "      <td>0.00420</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.00095</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184577</th>\n",
       "      <td>0.922889</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37127</th>\n",
       "      <td>0.185632</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>0.599396</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103694</th>\n",
       "      <td>0.518470</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00950</td>\n",
       "      <td>0.00400</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>0.659662</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.00665</td>\n",
       "      <td>0.00680</td>\n",
       "      <td>0.00575</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03670</td>\n",
       "      <td>0.02165</td>\n",
       "      <td>0.04940</td>\n",
       "      <td>0.06465</td>\n",
       "      <td>0.04640</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.01185</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146867</th>\n",
       "      <td>0.734337</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>0.609791</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.00430</td>\n",
       "      <td>0.00470</td>\n",
       "      <td>0.00435</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02870</td>\n",
       "      <td>0.01620</td>\n",
       "      <td>0.02915</td>\n",
       "      <td>0.03370</td>\n",
       "      <td>0.01880</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140000 rows Ã— 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id  A0T0G0C10  A0T0G1C9  A0T0G2C8  A0T0G3C7  A0T0G4C6  A0T0G5C5  \\\n",
       "21269   0.106341     0.0000    0.0020    0.0050    0.0110   0.00850   0.01300   \n",
       "187660  0.938304     0.0001    0.0012    0.0091    0.0462   0.05110   0.06795   \n",
       "774     0.003865     0.0001    0.0013    0.0043    0.0099   0.01035   0.01180   \n",
       "184577  0.922889     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "37127   0.185632     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "...          ...        ...       ...       ...       ...       ...       ...   \n",
       "119879  0.599396     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "103694  0.518470     0.0000    0.0010    0.0040    0.0150   0.01550   0.01350   \n",
       "131932  0.659662     0.0003    0.0009    0.0029    0.0058   0.00665   0.00680   \n",
       "146867  0.734337     0.0000    0.0000    0.0000    0.0000   0.00000   0.00000   \n",
       "121958  0.609791     0.0000    0.0003    0.0020    0.0043   0.00430   0.00470   \n",
       "\n",
       "        A0T0G6C4  A0T0G7C3  A0T0G8C2  ...  A8T0G1C1  A8T0G2C0  A8T1G0C1  \\\n",
       "21269    0.00750    0.0130    0.0000  ...   0.01350   0.00900   0.01600   \n",
       "187660   0.05280    0.0457    0.0075  ...   0.00605   0.00380   0.00680   \n",
       "774      0.00975    0.0117    0.0046  ...   0.00750   0.00385   0.00760   \n",
       "184577   0.00000    0.0000    0.0000  ...   0.00000   0.05000   0.00000   \n",
       "37127    0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "119879   0.00000    0.0000    0.0000  ...   0.00000   0.00000   0.00000   \n",
       "103694   0.01050    0.0150    0.0020  ...   0.00950   0.00400   0.01100   \n",
       "131932   0.00575    0.0066    0.0028  ...   0.03670   0.02165   0.04940   \n",
       "146867   0.00000    0.0000    0.0000  ...   0.05000   0.00000   0.05000   \n",
       "121958   0.00435    0.0048    0.0022  ...   0.02870   0.01620   0.02915   \n",
       "\n",
       "        A8T1G1C0  A8T2G0C0  A9T0G0C1  A9T0G1C0  A9T1G0C0  A10T0G0C0  target  \n",
       "21269    0.01550   0.00900    0.0030    0.0020   0.00200      0.000       9  \n",
       "187660   0.00765   0.00425    0.0022    0.0018   0.00140      0.001       5  \n",
       "774      0.00775   0.00420    0.0022    0.0026   0.00095      0.001       9  \n",
       "184577   0.00000   0.00000    0.0000    0.0000   0.00000      0.000       4  \n",
       "37127    0.00000   0.00000    0.0000    0.0000   0.00000      0.000       8  \n",
       "...          ...       ...       ...       ...       ...        ...     ...  \n",
       "119879   0.00000   0.00000    0.0000    0.0000   0.00000      0.000       8  \n",
       "103694   0.01000   0.00250    0.0010    0.0030   0.00100      0.000       3  \n",
       "131932   0.06465   0.04640    0.0143    0.0213   0.01185      0.002       1  \n",
       "146867   0.10000   0.00000    0.1000    0.1000   0.00000      0.000       2  \n",
       "121958   0.03370   0.01880    0.0082    0.0106   0.00480      0.001       9  \n",
       "\n",
       "[140000 rows x 288 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ENCODED TRAIN:')\n",
    "encoded_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T18:27:49.217311Z",
     "iopub.status.busy": "2024-03-03T18:27:49.216677Z",
     "iopub.status.idle": "2024-03-03T18:28:00.401443Z",
     "shell.execute_reply": "2024-03-03T18:28:00.400303Z",
     "shell.execute_reply.started": "2024-03-03T18:27:49.217278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T18:28:25.772216Z",
     "iopub.status.busy": "2024-03-03T18:28:25.771402Z",
     "iopub.status.idle": "2024-03-03T18:28:32.322996Z",
     "shell.execute_reply": "2024-03-03T18:28:32.322165Z",
     "shell.execute_reply.started": "2024-03-03T18:28:25.772172Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T18:28:35.415439Z",
     "iopub.status.busy": "2024-03-03T18:28:35.414515Z",
     "iopub.status.idle": "2024-03-03T18:28:35.434370Z",
     "shell.execute_reply": "2024-03-03T18:28:35.433327Z",
     "shell.execute_reply.started": "2024-03-03T18:28:35.415394Z"
    }
   },
   "outputs": [],
   "source": [
    "# THE NETWORK\n",
    "\n",
    "# Network class contains the layers needed for our network\n",
    "# It inherits torch.nn.Module class to create the layers using torch.nn.Module class's methods.\n",
    "class Network(torch.nn.Module):   \n",
    "    \n",
    "    # __init__() is called when an object of Network class is created.\n",
    "    def __init__(self, number_of_features, number_of_classes): \n",
    "        \n",
    "        super().__init__() \n",
    " \n",
    "        self.number_of_features = number_of_features # Store the number of features in the input dataset.\n",
    "        self.number_of_classes = number_of_classes # Store the number of classes in the input dataset.\n",
    "\n",
    "        kernel_size = 5\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        # We have 2 convolutional layers in our network.\n",
    "        # Below 2 statements defines the number of filtes (or out channels) to be used in our model.\n",
    "        self.conv1_out_channels = 32\n",
    "        # In the 2nd convolutional layer, the number of filters (or out channels) is doubled from the previous\n",
    "        # convolutional layer.\n",
    "        self.conv2_out_channels = self.conv1_out_channels * 2\n",
    "        \n",
    "        # Defining the convolutional layers of the model.\n",
    "        # We have 2 convolutional layer in our model (self.conv1 and self.conv2) which are created as objects \n",
    "        # of the torch.nn.Conv1d class which are callable classes.\n",
    "        # These objects are called in the forward method of this Network class by sending the input as argument.\n",
    "        # The kernel size used in the convolution is 3\n",
    "        # The padding size is kept 1 to keep the resulting output size same as that of the input size (same padding) \n",
    "        # For the 1st convolutional layer, the number of in channels is 1 as the number of channels in the input is 1.\n",
    "        # The number of out channels is equal to the value of self.conv1_out_channels variable which is defined above.\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=1, out_channels=self.conv1_out_channels, kernel_size=kernel_size, padding=padding)\n",
    "        \n",
    "        # For the 2nd convolutional layer, the number of in channels is equal to the out channels of the previous layer.\n",
    "        # The number of out channels is equal to the value of self.conv2_out_cjhannels variable which is defined above.\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=self.conv1_out_channels, out_channels=self.conv2_out_channels, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        self.pool = torch.nn.MaxPool1d(kernel_size=2, ceil_mode=True)\n",
    "                     \n",
    "        # We have 2 fully connected layers in pur model after the convolutinal layers\n",
    "        # For the 1st fully connected layer, the number of input neurons will be equal to the number of elements\n",
    "        # in the flattened output of the 2nd convolutional layer i.e. the number of out channels of the 2nd convolutional\n",
    "        # layer multiplied by the number of features in the dataset (i.e. the size of the input)\n",
    "        fc1_input_neurons = self.conv2_out_channels * (math.ceil(math.ceil(self.number_of_features / 2) / 2))\n",
    "        \n",
    "        # The number of output neurons is half of the input neurons.\n",
    "        fc1_output_neurons = fc1_input_neurons // 2\n",
    "      \n",
    "        # Defining the fully connected layers.\n",
    "        # The 2 fully connected layers of our model (self.fc1 and self.fc2) is created as objects of the \n",
    "        # torch.nn.Linear class which are callable classes and are called in the forward method of this network.\n",
    "        # The 1st fully connected layer accepts the flattened output of the 2nd convolutional layer as input.\n",
    "        self.fc1 = torch.nn.Linear(in_features=fc1_input_neurons, out_features=fc1_output_neurons) \n",
    "        \n",
    "        # The 2nd fully connected layer accepts the output of the 1st fully connected layer as input.\n",
    "        # The 2nd fully connected layer is the final layer of our model.\n",
    "        # Its number of output neurons is equal to the number of classes in the dataset.\n",
    "        self.fc2 = torch.nn.Linear(in_features=fc1_output_neurons, out_features=self.number_of_classes)\n",
    "        \n",
    "        # Defining ReLU activation function\n",
    "        self.relu = torch.nn.ReLU() \n",
    "         \n",
    "    # This method is called during the forward propagation\n",
    "    # It accepts the input tensor and passes it through all the layers\n",
    "    # and finally returns the resulting output tensor\n",
    "    def forward(self, t):       \n",
    "        \n",
    "        conv1_output = self.conv1(t) # Feeding the input to the 1st convolutional layer.\n",
    "        conv1_output = self.relu(conv1_output) \n",
    "        conv1_output = self.pool(conv1_output)\n",
    "\n",
    "        conv2_output = self.conv2(conv1_output)  # Feeding the output of previous layer to the 2nd convolutional layer.\n",
    "        conv2_output = self.relu(conv2_output) \n",
    "        conv2_output = self.pool(conv2_output)\n",
    "        \n",
    "        # Flattening the output of the 2nd convolutional layer in order to feed it to the 1st fully connected layer.\n",
    "        flattened_output = conv2_output.reshape(-1, self.conv2_out_channels * (math.ceil(math.ceil(self.number_of_features / 2) / 2))) \n",
    " \n",
    "        # Feed the input to the fully connected layers\n",
    "        fc1_output = self.fc1(flattened_output) # Feeding the flattened output to the 1st fully connected layer whose weights are now changed\n",
    "        fc1_output = self.relu(fc1_output) # Applying relu activation on the output.\n",
    "        fc2_output = self.fc2(fc1_output) # Feeding the output of previous layer to the 2nd fully connected layer.\n",
    "\n",
    "        return fc2_output # return the output of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T18:28:42.698232Z",
     "iopub.status.busy": "2024-03-03T18:28:42.697295Z",
     "iopub.status.idle": "2024-03-03T18:28:42.704948Z",
     "shell.execute_reply": "2024-03-03T18:28:42.703878Z",
     "shell.execute_reply.started": "2024-03-03T18:28:42.698188Z"
    }
   },
   "outputs": [],
   "source": [
    "# We create a NumericalDataset class which inherits torch.utils.data.Dataset class \n",
    "# Its __init__() method accepts the features and classes of the dataset as arguments.\n",
    "# It eases the process of feeding the dataset into our deep learning model.\n",
    "class NumericalDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x # x: the attributes\n",
    "        self.y = y # y: the class labels\n",
    "        \n",
    "    def __len__(self): \n",
    "        return (len(self.x))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        x = self.x[i, :] # Returns an 1D array.\n",
    "        x = x.reshape(1, x.shape[0]) # Adding an extra dimension for channel [shape: (1, no_of_attributes)]\n",
    "        x = x.double() \n",
    "        y = self.y[i].item() # Assuming y is a single number denoting the class\n",
    "        y = int(y) # Assuming the class is denoted by integer\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T18:28:50.664131Z",
     "iopub.status.busy": "2024-03-03T18:28:50.663246Z",
     "iopub.status.idle": "2024-03-03T19:24:25.361641Z",
     "shell.execute_reply": "2024-03-03T19:24:25.360266Z",
     "shell.execute_reply.started": "2024-03-03T18:28:50.664098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     48\u001b[0m     train_x, train_y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 49\u001b[0m     train_x, train_y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, train_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m     shuffle_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(\u001b[38;5;28mlen\u001b[39m(train_x))\n\u001b[1;32m     51\u001b[0m     train_x \u001b[38;5;241m=\u001b[39m train_x[shuffle_idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import math \n",
    "\n",
    "# Assuming encode_dataframe, NumericalDataset, and Network classes are defined\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Define the device\n",
    "\n",
    "\n",
    "def get_data_loader(df, batch_size):\n",
    "    np_data = df.values\n",
    "    t_data = torch.tensor(np_data)\n",
    "    x = t_data[:, :-1]\n",
    "    y = t_data[:, -1]\n",
    "    nd = NumericalDataset(x, y)\n",
    "    data_loader = torch.utils.data.DataLoader(nd, batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "total_epochs = 100\n",
    "percentage = 10\n",
    "batch_size = int(len(train_features) * (percentage / 100))\n",
    "total_train_accuracy = 0\n",
    "total_test_accuracy = 0\n",
    "\n",
    "# print('Dataset:', dataset_name, dataset_number, '| Epoch:', total_epochs)\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Encoding train and test\n",
    "# encoded_train, encoded_test = encode_dataframe(train_features, test_features)\n",
    "\n",
    "# Storing the number of classes and features in the encoded dataframe\n",
    "number_of_classes = int(max(encoded_train.iloc[:, -1]) + 1)\n",
    "number_of_attributes = encoded_train.shape[1] - 1\n",
    "\n",
    "train_loader = get_data_loader(encoded_train, batch_size=batch_size)\n",
    "test_loader = get_data_loader(encoded_test, batch_size=batch_size)\n",
    "\n",
    "# Creating a Network object for this fold\n",
    "net = Network(number_of_attributes, number_of_classes).double().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(total_epochs):\n",
    "    for batch in train_loader:\n",
    "        train_x, train_y = batch\n",
    "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "        shuffle_idx = torch.randperm(len(train_x))\n",
    "        train_x = train_x[shuffle_idx]\n",
    "        train_y = train_y[shuffle_idx]\n",
    "        preds = net(train_x)\n",
    "        loss = criterion(preds, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Testing the model\n",
    "correct_predictions_train = 0\n",
    "for batch in train_loader:\n",
    "    train_x, train_y = batch\n",
    "    train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "    preds = net(train_x)\n",
    "    correct_predictions_train += preds.argmax(dim=1).eq(train_y).sum().item()\n",
    "train_accuracy = round(correct_predictions_train / len(encoded_train) * 100, 2)\n",
    "\n",
    "correct_predictions_test = 0\n",
    "for batch in test_loader:\n",
    "    test_x, test_y = batch\n",
    "    test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "    preds = net(test_x)\n",
    "    correct_predictions_test += preds.argmax(dim=1).eq(test_y).sum().item()\n",
    "test_accuracy = round(correct_predictions_test / len(encoded_test) * 100, 2)\n",
    "\n",
    "print('Train: %.2f Test: %.2f' % (train_accuracy, test_accuracy))\n",
    "\n",
    "print('\\nExecution Time:', round(time.time() - start_time, 2), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method accepts a DataFrame as argument and converts it into tensor.\n",
    "def get_data_loader(df, batch_size):\n",
    "        np_data = df.values # Fetching data as numpy array from the dataframe\n",
    "        t_data = torch.tensor(np_data) # Converting the numpy array into tensor.\n",
    "        x = t_data[:, :-1] # Attributes\n",
    "        y = t_data[:, -1] # Classes\n",
    "        nd = NumericalDataset(x, y) # creating the NumericalDataset object\n",
    "        # data_loader (object of torch.utils.data.DataLoader class) will be used to load each row of the dataset\n",
    "        # and feed it to our deep learning network.\n",
    "        data_loader = torch.utils.data.DataLoader(nd, batch_size=batch_size) \n",
    "        return data_loader\n",
    "\n",
    "\n",
    "total_epochs = 100 # The number of epochs with which we'll train the model.\n",
    "percentage = 10 # Percentage of the train data to be taken in each batch\n",
    "batch_size = int(len(train_list[0]) * (percentage / 100))\n",
    "total_train_accuracy = 0 \n",
    "total_test_accuracy = 0 # Add the accuracy of each fold. Divide it by 10 to get the final accuracy.\n",
    "fold = 1\n",
    "\n",
    "print('Dataset:', dataset_name, dataset_number, '| Epoch:', total_epochs)\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    " \n",
    "for i in range(10):\n",
    "    \n",
    "    # Selecting train and test\n",
    "    train_data, test_data = train_list[i], test_list[i]\n",
    "    # Encoding train and test\n",
    "    encoded_train, encoded_test = encode_dataframe(train_data, test_data)\n",
    "    \n",
    "    # Storing the number of classes and features in the encoded dataframe\n",
    "    number_of_classes = int(max(encoded_train.iloc[:, -1]) + 1) \n",
    "    number_of_attributes = encoded_train.shape[1] - 1 \n",
    "\n",
    "    train_loader = get_data_loader(encoded_train, batch_size=batch_size)\n",
    "    test_loader = get_data_loader(encoded_test, batch_size=batch_size)\n",
    "    \n",
    "    # Creating a Network object for this fold (A new object is created for each fold)\n",
    "    net = Network(number_of_attributes, number_of_classes).double().to(device) \n",
    "    \n",
    "    # Defining the optimizer for our Network object. We apply learing rate of 0.001.\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    # Defining the loss calculating method for our Network object. We use cross entropy loss.\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    " \n",
    "    # Training our model.\n",
    "    for epoch in range(total_epochs): \n",
    "        for batch in train_loader:\n",
    "            \n",
    "            # Get features and class labels in the v=batch and store them in train_x and train_y respectively\n",
    "            train_x, train_y = batch\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "\n",
    "            # Shufle the batch\n",
    "            shuffle_idx = torch.randperm(len(train_x)) # Get random indices\n",
    "            train_x = train_x[shuffle_idx] # Arrange train_x and train_y with the random indices\n",
    "            train_y = train_y[shuffle_idx]\n",
    "\n",
    "            preds = net(train_x) # Forward propagation. Feed train_x as input into the model\n",
    "            loss = criterion(preds, train_y) # Calculate loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # Backpropagation. Calculate the Gradients\n",
    "            optimizer.step() # Update the Weights  \n",
    "\n",
    "\n",
    "    # Training of this fold is done \n",
    "    # Testing for this fold\n",
    "    # Train accuracy\n",
    "    correct_predictions_train = 0 \n",
    "    for batch in train_loader:\n",
    "        train_x, train_y = batch\n",
    "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "        preds = net(train_x)\n",
    "        correct_predictions_train += preds.argmax(dim=1).eq(train_y).sum().item()\n",
    "    train_accuracy = round(correct_predictions_train / len(encoded_train) * 100, 2)\n",
    "    # Test accuracy            \n",
    "    correct_predictions_test = 0 \n",
    "    for batch in test_loader:\n",
    "        test_x, test_y = batch\n",
    "        test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "        preds = net(test_x) \n",
    "        correct_predictions_test += preds.argmax(dim=1).eq(test_y).sum().item()\n",
    "    test_accuracy = round(correct_predictions_test / len(encoded_test) * 100, 2)\n",
    "\n",
    "    print('Train: %.2f Test: %.2f (Fold %d)' %(train_accuracy, test_accuracy, fold))\n",
    "    total_train_accuracy += train_accuracy\n",
    "    total_test_accuracy += test_accuracy\n",
    "    fold += 1\n",
    "    \n",
    "print()\n",
    "print('10CV result:')\n",
    "final_train_accuracy = round(total_train_accuracy / 10, 2)\n",
    "print('Train Accuracy:', final_train_accuracy)\n",
    "final_test_accuracy = round(total_test_accuracy / 10, 2)\n",
    "print('Test Accuracy:', final_test_accuracy)\n",
    "\n",
    "print('\\nExecution Time:', round(time.time() - start_time, 2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T09:51:53.746153Z",
     "iopub.status.busy": "2024-02-25T09:51:53.745445Z",
     "iopub.status.idle": "2024-02-25T09:51:53.752209Z",
     "shell.execute_reply": "2024-02-25T09:51:53.751233Z",
     "shell.execute_reply.started": "2024-02-25T09:51:53.746122Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:24:39.585569Z",
     "iopub.status.busy": "2024-02-25T13:24:39.585174Z",
     "iopub.status.idle": "2024-02-25T13:24:39.646422Z",
     "shell.execute_reply": "2024-02-25T13:24:39.645369Z",
     "shell.execute_reply.started": "2024-02-25T13:24:39.585537Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:24:44.086146Z",
     "iopub.status.busy": "2024-02-25T13:24:44.085438Z",
     "iopub.status.idle": "2024-02-25T13:24:49.126822Z",
     "shell.execute_reply": "2024-02-25T13:24:49.125799Z",
     "shell.execute_reply.started": "2024-02-25T13:24:44.086112Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert a list of lists to a NumPy array\n",
    "X_train = np.array(train)\n",
    "# Assuming X_train is your training data\n",
    "# X_train should be a 2D NumPy array\n",
    "\n",
    "# Initialize an empty array to store the scaled data\n",
    "X_train_scaled = np.zeros_like(X_train)\n",
    "\n",
    "# Iterate through each row of X_train\n",
    "for i in range(X_train.shape[0]):\n",
    "    row = X_train[i, :]  # Get the current row\n",
    "    min_val = np.min(row)  # Calculate the minimum value in the row\n",
    "    max_val = np.max(row)  # Calculate the maximum value in the row\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if max_val != min_val:\n",
    "        scaled_row = (row - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        scaled_row = np.zeros_like(row)  # If max and min are the same, set all values to 0\n",
    "\n",
    "    X_train_scaled[i, :] = scaled_row  # Store the scaled row in the result array\n",
    "\n",
    "# Now, X_train_scaled contains your normalized training data with each row scaled individually between 0 and 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:24:59.528044Z",
     "iopub.status.busy": "2024-02-25T13:24:59.527352Z",
     "iopub.status.idle": "2024-02-25T13:24:59.534897Z",
     "shell.execute_reply": "2024-02-25T13:24:59.533680Z",
     "shell.execute_reply.started": "2024-02-25T13:24:59.528009Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:25:03.139359Z",
     "iopub.status.busy": "2024-02-25T13:25:03.138482Z",
     "iopub.status.idle": "2024-02-25T13:26:58.973349Z",
     "shell.execute_reply": "2024-02-25T13:26:58.972333Z",
     "shell.execute_reply.started": "2024-02-25T13:25:03.139313Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# Assuming X_train_scaled is your scaled dataset\n",
    "# Create an empty array for the formatted data\n",
    "X_train_formatted = np.empty_like(X_train_scaled, dtype=np.float64)\n",
    "\n",
    "# Iterate through each row and column of X_train_scaled\n",
    "for i in range(X_train_scaled.shape[0]):\n",
    "    for j in range(X_train_scaled.shape[1]):\n",
    "        scaled_value = X_train_scaled[i, j]\n",
    "        formatted_value = \"{:.5f}\".format(scaled_value)\n",
    "        X_train_formatted[i, j] = float(formatted_value)\n",
    "\n",
    "# Now, X_train_formatted contains your scaled dataset with all values formatted as decimals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:27:16.398839Z",
     "iopub.status.busy": "2024-02-25T13:27:16.398488Z",
     "iopub.status.idle": "2024-02-25T13:27:16.405732Z",
     "shell.execute_reply": "2024-02-25T13:27:16.404383Z",
     "shell.execute_reply.started": "2024-02-25T13:27:16.398811Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:27:19.086896Z",
     "iopub.status.busy": "2024-02-25T13:27:19.086545Z",
     "iopub.status.idle": "2024-02-25T13:27:19.093303Z",
     "shell.execute_reply": "2024-02-25T13:27:19.092309Z",
     "shell.execute_reply.started": "2024-02-25T13:27:19.086868Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_formatted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T06:34:52.165901Z",
     "iopub.status.busy": "2023-12-19T06:34:52.165070Z",
     "iopub.status.idle": "2023-12-19T06:34:52.171793Z",
     "shell.execute_reply": "2023-12-19T06:34:52.170812Z",
     "shell.execute_reply.started": "2023-12-19T06:34:52.165866Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Example data (replace this with your actual data)\n",
    "# train = np.array(train)\n",
    "\n",
    "# # Function to perform Min-Max scaling\n",
    "# def min_max_scaling(train):\n",
    "#     # Calculate the minimum and maximum values for each feature\n",
    "#     min_vals = np.min(train, axis=0)\n",
    "#     max_vals = np.max(train, axis=0)\n",
    "    \n",
    "#     # Apply the Min-Max scaling formula\n",
    "#     scaled_data = (train - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "#     return scaled_data, min_vals, max_vals\n",
    "\n",
    "# # Apply Min-Max scaling\n",
    "# scaled_data, min_vals, max_vals = min_max_scaling(train)\n",
    "\n",
    "# # print(\"Original Data:\\n\", data)\n",
    "# print(\"\\nScaled Data (Min-Max scaled between 0 and 1):\\n\", scaled_data)\n",
    "# print(\"\\nMin Values for each feature:\\n\", min_vals)\n",
    "# print(\"\\nMax Values for each feature:\\n\", max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T14:56:48.432364Z",
     "iopub.status.busy": "2023-12-14T14:56:48.431446Z",
     "iopub.status.idle": "2023-12-14T14:56:48.436101Z",
     "shell.execute_reply": "2023-12-14T14:56:48.435198Z",
     "shell.execute_reply.started": "2023-12-14T14:56:48.432326Z"
    }
   },
   "outputs": [],
   "source": [
    "                                                  #  PCA of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_trainnew, X_val, y_trainnew, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T05:48:32.116216Z",
     "iopub.status.busy": "2023-12-19T05:48:32.115487Z",
     "iopub.status.idle": "2023-12-19T05:48:32.120482Z",
     "shell.execute_reply": "2023-12-19T05:48:32.119407Z",
     "shell.execute_reply.started": "2023-12-19T05:48:32.116180Z"
    }
   },
   "outputs": [],
   "source": [
    "# target_data=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T06:36:34.080449Z",
     "iopub.status.busy": "2023-12-19T06:36:34.079539Z",
     "iopub.status.idle": "2023-12-19T06:36:34.084009Z",
     "shell.execute_reply": "2023-12-19T06:36:34.083087Z",
     "shell.execute_reply.started": "2023-12-19T06:36:34.080416Z"
    }
   },
   "outputs": [],
   "source": [
    "# target_data\n",
    "# train=train.drop(columns=['target']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:27:25.581188Z",
     "iopub.status.busy": "2024-02-25T13:27:25.580798Z",
     "iopub.status.idle": "2024-02-25T13:27:29.807762Z",
     "shell.execute_reply": "2024-02-25T13:27:29.806316Z",
     "shell.execute_reply.started": "2024-02-25T13:27:25.581158Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming X_train_scaled is your scaled training data\n",
    "# You can use X_train_formatted if you've converted it to human-readable format X_train_formatted\n",
    "\n",
    "# Initialize the PCA model with the desired number of components\n",
    "n_components = 33  # Adjust this value as needed\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit the PCA model to your scaled data\n",
    "pca.fit(X_train_formatted)\n",
    "\n",
    "# Transform the data to its principal components\n",
    "X_train_pca = pca.transform(X_train_formatted)\n",
    "# Create a DataFrame for the PCA results using common columns\n",
    "pca_df = pd.DataFrame(data=X_train_pca, columns=[f'PC{i}' for i in range(1, 34)])\n",
    "# Printing the PCA results\n",
    "print(pca_df)\n",
    "\n",
    "# Now, X_train_pca contains your training data reduced to the specified number of principal components\n",
    "# You can use X_train_pca for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:17.238550Z",
     "iopub.status.busy": "2023-12-19T14:26:17.237552Z",
     "iopub.status.idle": "2023-12-19T14:26:17.242973Z",
     "shell.execute_reply": "2023-12-19T14:26:17.241925Z",
     "shell.execute_reply.started": "2023-12-19T14:26:17.238506Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-18T14:43:48.260724Z",
     "iopub.status.busy": "2023-12-18T14:43:48.259863Z",
     "iopub.status.idle": "2023-12-18T14:43:48.266337Z",
     "shell.execute_reply": "2023-12-18T14:43:48.265181Z",
     "shell.execute_reply.started": "2023-12-18T14:43:48.260690Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming you have a target_column, and pca_df is your DataFrame\n",
    "\n",
    "# Add the target_column to pca_df\n",
    "# pca_df['target'] = target_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:03.178494Z",
     "iopub.status.busy": "2024-02-25T13:29:03.177705Z",
     "iopub.status.idle": "2024-02-25T13:29:03.223737Z",
     "shell.execute_reply": "2024-02-25T13:29:03.222714Z",
     "shell.execute_reply.started": "2024-02-25T13:29:03.178457Z"
    }
   },
   "outputs": [],
   "source": [
    "print(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:07.287125Z",
     "iopub.status.busy": "2024-02-25T13:29:07.286240Z",
     "iopub.status.idle": "2024-02-25T13:29:07.294823Z",
     "shell.execute_reply": "2024-02-25T13:29:07.293801Z",
     "shell.execute_reply.started": "2024-02-25T13:29:07.287091Z"
    }
   },
   "outputs": [],
   "source": [
    "target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:11.538631Z",
     "iopub.status.busy": "2024-02-25T13:29:11.537653Z",
     "iopub.status.idle": "2024-02-25T13:29:11.542929Z",
     "shell.execute_reply": "2024-02-25T13:29:11.541768Z",
     "shell.execute_reply.started": "2024-02-25T13:29:11.538598Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pca_df  # Drop the target column to get the features\n",
    "y = target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:19.274224Z",
     "iopub.status.busy": "2024-02-25T13:29:19.273876Z",
     "iopub.status.idle": "2024-02-25T13:29:19.282404Z",
     "shell.execute_reply": "2024-02-25T13:29:19.281503Z",
     "shell.execute_reply.started": "2024-02-25T13:29:19.274196Z"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:22.068121Z",
     "iopub.status.busy": "2024-02-25T13:29:22.067302Z",
     "iopub.status.idle": "2024-02-25T13:29:22.157364Z",
     "shell.execute_reply": "2024-02-25T13:29:22.156396Z",
     "shell.execute_reply.started": "2024-02-25T13:29:22.068085Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:30.817942Z",
     "iopub.status.busy": "2024-02-25T13:29:30.817200Z",
     "iopub.status.idle": "2024-02-25T13:29:30.859405Z",
     "shell.execute_reply": "2024-02-25T13:29:30.858471Z",
     "shell.execute_reply.started": "2024-02-25T13:29:30.817911Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainnew, X_test, y_trainnew, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:34.707129Z",
     "iopub.status.busy": "2024-02-25T13:29:34.706437Z",
     "iopub.status.idle": "2024-02-25T13:29:34.714173Z",
     "shell.execute_reply": "2024-02-25T13:29:34.713297Z",
     "shell.execute_reply.started": "2024-02-25T13:29:34.707098Z"
    }
   },
   "outputs": [],
   "source": [
    "y_trainnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:37.906158Z",
     "iopub.status.busy": "2024-02-25T13:29:37.905534Z",
     "iopub.status.idle": "2024-02-25T13:29:37.987851Z",
     "shell.execute_reply": "2024-02-25T13:29:37.986887Z",
     "shell.execute_reply.started": "2024-02-25T13:29:37.906125Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-18T14:44:03.224254Z",
     "iopub.status.busy": "2023-12-18T14:44:03.223922Z",
     "iopub.status.idle": "2023-12-18T14:44:03.271557Z",
     "shell.execute_reply": "2023-12-18T14:44:03.270628Z",
     "shell.execute_reply.started": "2023-12-18T14:44:03.224229Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assuming X and y are your feature matrix and target column\n",
    "# # Replace 'X' and 'y' with your actual feature matrix and target column\n",
    "\n",
    "# # Set the desired test size\n",
    "# test_size = 0.1\n",
    "\n",
    "# # Perform the train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "# # Print the sizes of the train and test sets\n",
    "# print(f\"Train Size: {len(X_train)}\")\n",
    "# print(f\"Test Size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T07:30:44.274456Z",
     "iopub.status.busy": "2023-12-19T07:30:44.274082Z",
     "iopub.status.idle": "2023-12-19T07:30:44.278855Z",
     "shell.execute_reply": "2023-12-19T07:30:44.277820Z",
     "shell.execute_reply.started": "2023-12-19T07:30:44.274428Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:43.045219Z",
     "iopub.status.busy": "2024-02-25T13:29:43.044838Z",
     "iopub.status.idle": "2024-02-25T13:29:43.051630Z",
     "shell.execute_reply": "2024-02-25T13:29:43.050520Z",
     "shell.execute_reply.started": "2024-02-25T13:29:43.045188Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainnew['target']=y_trainnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:45.899523Z",
     "iopub.status.busy": "2024-02-25T13:29:45.899145Z",
     "iopub.status.idle": "2024-02-25T13:29:45.906465Z",
     "shell.execute_reply": "2024-02-25T13:29:45.905225Z",
     "shell.execute_reply.started": "2024-02-25T13:29:45.899492Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainnew.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T10:06:43.681475Z",
     "iopub.status.busy": "2024-02-25T10:06:43.680638Z",
     "iopub.status.idle": "2024-02-25T10:06:43.775710Z",
     "shell.execute_reply": "2024-02-25T10:06:43.774790Z",
     "shell.execute_reply.started": "2024-02-25T10:06:43.681439Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:51.176433Z",
     "iopub.status.busy": "2024-02-25T13:29:51.176024Z",
     "iopub.status.idle": "2024-02-25T13:29:51.235355Z",
     "shell.execute_reply": "2024-02-25T13:29:51.234459Z",
     "shell.execute_reply.started": "2024-02-25T13:29:51.176395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for NaN values in the resulting X_trainnew\n",
    "print(\"NaN values in df_with_clusters:\", X_trainnew.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:29:53.667051Z",
     "iopub.status.busy": "2024-02-25T13:29:53.666148Z",
     "iopub.status.idle": "2024-02-25T13:30:00.183239Z",
     "shell.execute_reply": "2024-02-25T13:30:00.182155Z",
     "shell.execute_reply.started": "2024-02-25T13:29:53.667015Z"
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "# K-means algorithm\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "from cuml.cluster import KMeans as cuKMeans\n",
    "from cudf import DataFrame as cudf_DataFrame\n",
    "\n",
    "# Assuming df is your original DataFrame with PC1 to PC33 and the target column\n",
    "# The target column should be named 'target'\n",
    "\n",
    "# Extract numerical features for clustering X_trainnew\n",
    "features = X_trainnew.columns[:-1]  # Exclude the target column\n",
    "data_for_clustering = X_trainnew[features]\n",
    "\n",
    "# Define the number of clusters\n",
    "num_clusters = 100\n",
    "\n",
    "# Initialize the KMeans model on GPU\n",
    "kmeans = cuKMeans(n_clusters=num_clusters, random_state=0, n_init=1)\n",
    "\n",
    "# Fit KMeans model\n",
    "cluster_assignments_gpu = kmeans.fit_predict(data_for_clustering)\n",
    "\n",
    "# Transfer cuML Series to cuDF DataFrame\n",
    "cluster_assignments_cudf = cudf.Series(cluster_assignments_gpu)\n",
    "\n",
    "# Convert cuDF Series to a NumPy array\n",
    "cluster_assignments_cpu = cluster_assignments_cudf.to_pandas().values\n",
    "\n",
    "# Add cluster assignments to the original DataFrame\n",
    "df_with_clusters = pd.concat([X_trainnew, pd.DataFrame({'cluster': cluster_assignments_cpu})], axis=1)\n",
    "\n",
    "\n",
    "# Check for NaN values in the resulting DataFrame\n",
    "print(\"NaN values in df_with_clusters:\", df_with_clusters.isna().sum().sum())\n",
    "\n",
    "\n",
    "# Initialize a new DataFrame for the reduced dataset\n",
    "reduced_data = pd.DataFrame(columns=df_with_clusters.columns)\n",
    "\n",
    "# Randomly select 10,000 rows from the entire DataFrame with replacement\n",
    "selected_rows = df_with_clusters.sample(n=100, replace=True, random_state=0)\n",
    "\n",
    "# Check for NaN values in the selected rows\n",
    "print(\"NaN values in selected_rows:\", selected_rows.isna().sum().sum())\n",
    "\n",
    "# Concatenate the selected rows to the reduced_data DataFrame\n",
    "reduced_data = pd.concat([reduced_data, selected_rows], ignore_index=True)\n",
    "\n",
    "  # Extract class labels from the \"cluster\" column\n",
    "class_labels = reduced_data[\"cluster\"].values\n",
    "\n",
    "# Drop the \"cluster\" column from the reduced dataset\n",
    "reduced_data = reduced_data.drop(columns=[\"cluster\"])\n",
    "\n",
    "# Check for NaN values in the reduced dataset\n",
    "print(\"NaN values in reduced_data:\", reduced_data.isna().sum().sum())\n",
    "reduced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:30:06.646877Z",
     "iopub.status.busy": "2024-02-25T13:30:06.646498Z",
     "iopub.status.idle": "2024-02-25T13:30:06.653474Z",
     "shell.execute_reply": "2024-02-25T13:30:06.652373Z",
     "shell.execute_reply.started": "2024-02-25T13:30:06.646848Z"
    }
   },
   "outputs": [],
   "source": [
    "reduced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:30:08.880987Z",
     "iopub.status.busy": "2024-02-25T13:30:08.880141Z",
     "iopub.status.idle": "2024-02-25T13:30:08.919838Z",
     "shell.execute_reply": "2024-02-25T13:30:08.918836Z",
     "shell.execute_reply.started": "2024-02-25T13:30:08.880944Z"
    }
   },
   "outputs": [],
   "source": [
    "reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:30:13.191226Z",
     "iopub.status.busy": "2024-02-25T13:30:13.190795Z",
     "iopub.status.idle": "2024-02-25T13:30:13.296059Z",
     "shell.execute_reply": "2024-02-25T13:30:13.295068Z",
     "shell.execute_reply.started": "2024-02-25T13:30:13.191190Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:30:16.829811Z",
     "iopub.status.busy": "2024-02-25T13:30:16.828961Z",
     "iopub.status.idle": "2024-02-25T13:30:16.868525Z",
     "shell.execute_reply": "2024-02-25T13:30:16.867172Z",
     "shell.execute_reply.started": "2024-02-25T13:30:16.829775Z"
    }
   },
   "outputs": [],
   "source": [
    "reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:30:21.799161Z",
     "iopub.status.busy": "2024-02-25T13:30:21.798300Z",
     "iopub.status.idle": "2024-02-25T13:30:21.818100Z",
     "shell.execute_reply": "2024-02-25T13:30:21.816993Z",
     "shell.execute_reply.started": "2024-02-25T13:30:21.799127Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainnew=X_trainnew.drop(columns=['target'])\n",
    "# rain.drop(columns=['target']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:30:25.923803Z",
     "iopub.status.busy": "2024-02-25T13:30:25.922858Z",
     "iopub.status.idle": "2024-02-25T13:30:25.929508Z",
     "shell.execute_reply": "2024-02-25T13:30:25.928447Z",
     "shell.execute_reply.started": "2024-02-25T13:30:25.923765Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainnew['target']=y_trainnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:30:29.107638Z",
     "iopub.status.busy": "2024-02-25T13:30:29.106755Z",
     "iopub.status.idle": "2024-02-25T13:30:29.198808Z",
     "shell.execute_reply": "2024-02-25T13:30:29.197773Z",
     "shell.execute_reply.started": "2024-02-25T13:30:29.107602Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T15:32:41.538483Z",
     "iopub.status.busy": "2023-12-19T15:32:41.537834Z",
     "iopub.status.idle": "2023-12-19T15:32:41.542454Z",
     "shell.execute_reply": "2023-12-19T15:32:41.541409Z",
     "shell.execute_reply.started": "2023-12-19T15:32:41.538450Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_trainnew=X_trainnew.drop(columns=['cluster_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:38:28.914316Z",
     "iopub.status.busy": "2023-12-19T14:38:28.913962Z",
     "iopub.status.idle": "2023-12-19T14:38:28.918385Z",
     "shell.execute_reply": "2023-12-19T14:38:28.917447Z",
     "shell.execute_reply.started": "2023-12-19T14:38:28.914286Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:38:31.383942Z",
     "iopub.status.busy": "2023-12-19T14:38:31.382977Z",
     "iopub.status.idle": "2023-12-19T14:38:31.388000Z",
     "shell.execute_reply": "2023-12-19T14:38:31.386999Z",
     "shell.execute_reply.started": "2023-12-19T14:38:31.383906Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T08:04:40.600369Z",
     "iopub.status.busy": "2023-12-15T08:04:40.599649Z",
     "iopub.status.idle": "2023-12-15T08:04:40.604968Z",
     "shell.execute_reply": "2023-12-15T08:04:40.604056Z",
     "shell.execute_reply.started": "2023-12-15T08:04:40.600337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add this inside the loop before the condition\n",
    "# print(f\"Cluster {cluster_id}: Number of points = {len(each_cluster)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:30:53.429941Z",
     "iopub.status.busy": "2024-02-25T13:30:53.429457Z",
     "iopub.status.idle": "2024-02-25T13:30:53.436058Z",
     "shell.execute_reply": "2024-02-25T13:30:53.434857Z",
     "shell.execute_reply.started": "2024-02-25T13:30:53.429908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming  target column is named 'target' and we want to use all 33 feature columns\n",
    "X_train_1 = reduced_data.drop(columns=['target'])  # Extract all feature columns except 'target'\n",
    "y_train_1 = reduced_data['target']  # Extract the 'target' column as labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T10:12:24.687694Z",
     "iopub.status.busy": "2024-02-25T10:12:24.687345Z",
     "iopub.status.idle": "2024-02-25T10:12:24.694197Z",
     "shell.execute_reply": "2024-02-25T10:12:24.693219Z",
     "shell.execute_reply.started": "2024-02-25T10:12:24.687665Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:31:51.334453Z",
     "iopub.status.busy": "2024-02-25T13:31:51.334074Z",
     "iopub.status.idle": "2024-02-25T13:31:51.377124Z",
     "shell.execute_reply": "2024-02-25T13:31:51.375884Z",
     "shell.execute_reply.started": "2024-02-25T13:31:51.334424Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T10:12:35.330138Z",
     "iopub.status.busy": "2024-02-25T10:12:35.329093Z",
     "iopub.status.idle": "2024-02-25T10:12:35.441806Z",
     "shell.execute_reply": "2024-02-25T10:12:35.440478Z",
     "shell.execute_reply.started": "2024-02-25T10:12:35.330095Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_2 = X_train_1.drop(columns=['cluster_id'])  # Extract all feature columns except 'target'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T10:24:28.791819Z",
     "iopub.status.busy": "2024-02-25T10:24:28.790886Z",
     "iopub.status.idle": "2024-02-25T10:24:28.828562Z",
     "shell.execute_reply": "2024-02-25T10:24:28.827368Z",
     "shell.execute_reply.started": "2024-02-25T10:24:28.791756Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:06:23.956263Z",
     "iopub.status.busy": "2023-12-14T19:06:23.955104Z",
     "iopub.status.idle": "2023-12-14T19:06:23.960396Z",
     "shell.execute_reply": "2023-12-14T19:06:23.959466Z",
     "shell.execute_reply.started": "2023-12-14T19:06:23.956218Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T13:32:02.973396Z",
     "iopub.status.busy": "2024-02-25T13:32:02.973012Z",
     "iopub.status.idle": "2024-02-25T13:32:03.011267Z",
     "shell.execute_reply": "2024-02-25T13:32:03.010160Z",
     "shell.execute_reply.started": "2024-02-25T13:32:02.973366Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T14:03:34.981355Z",
     "iopub.status.busy": "2024-02-25T14:03:34.980601Z",
     "iopub.status.idle": "2024-02-25T14:03:35.049919Z",
     "shell.execute_reply": "2024-02-25T14:03:35.048655Z",
     "shell.execute_reply.started": "2024-02-25T14:03:34.981311Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Handle missing values in X_train_1 by filling them with zeros\n",
    "X_train_1_filled = X_train_1.fillna(0)\n",
    "\n",
    "# Convert X_train_1 to a PyTorch tensor\n",
    "X_tensor = torch.tensor(X_train_1_filled.values, dtype=torch.float32)\n",
    "\n",
    "# Convert y_train_1 to strings to avoid mixed data types\n",
    "y_train_1_str = y_train_1.astype(str)\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_train_1_str)\n",
    "\n",
    "# Convert y_encoded to a PyTorch tensor\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Assuming number_of_features is the number of features in your input data\n",
    "number_of_features = X_train_1.shape[1]\n",
    "\n",
    "# Assuming number_of_classes is the number of unique classes in your target labels\n",
    "number_of_classes = len(np.unique(y_train_1_str))\n",
    "\n",
    "# Create an instance of the Network class\n",
    "network = Network(number_of_features, 128, number_of_classes)  # Assuming hidden_size is 128\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Define validation dataset\n",
    "X_val_1_filled = X_val_1.fillna(0)\n",
    "X_val_tensor = torch.tensor(X_val_1_filled.values, dtype=torch.float32)\n",
    "y_val_str = y_val_1.astype(str)\n",
    "y_val_encoded = label_encoder.transform(y_val_str)\n",
    "y_val_tensor = torch.tensor(y_val_encoded, dtype=torch.long)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train your network\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = network(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T10:36:24.189565Z",
     "iopub.status.busy": "2024-02-25T10:36:24.189196Z",
     "iopub.status.idle": "2024-02-25T10:36:24.635077Z",
     "shell.execute_reply": "2024-02-25T10:36:24.633783Z",
     "shell.execute_reply.started": "2024-02-25T10:36:24.189539Z"
    }
   },
   "outputs": [],
   "source": [
    "## Model Training\n",
    "# 1:  (Random Forest)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "  # Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100,max_depth = 6 )  # You can adjust n_estimators\n",
    " # You can adjust n_estimators\n",
    "  # You can adjust n_estimators\n",
    "\n",
    "# Train the Random Forest Model\n",
    "rf_classifier.fit(X_train_1, y_train_1)\n",
    "# Step 4: Model Evaluation\n",
    "# y_pred = rf_classifier.predict(pca_df_test)  # Predict on Test Data\n",
    "y_pred = rf_classifier.predict(X_train_2)  # Predict on Test Data\n",
    "\n",
    "accuracy = accuracy_score(y_train_1, y_pred,)\n",
    "accuracy_percent = accuracy * 100\n",
    "print(f\"Accuracy: {accuracy_percent:.2f}%\")  # Display accuracy in percentage format with two decimal places\n",
    "# report = classification_report(y_true, y_pred,zero_division=1)\n",
    "\n",
    "# print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T08:07:51.367868Z",
     "iopub.status.busy": "2023-12-15T08:07:51.367087Z",
     "iopub.status.idle": "2023-12-15T08:07:51.430215Z",
     "shell.execute_reply": "2023-12-15T08:07:51.429206Z",
     "shell.execute_reply.started": "2023-12-15T08:07:51.367837Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T08:07:57.522750Z",
     "iopub.status.busy": "2023-12-15T08:07:57.522385Z",
     "iopub.status.idle": "2023-12-15T08:07:57.527481Z",
     "shell.execute_reply": "2023-12-15T08:07:57.526436Z",
     "shell.execute_reply.started": "2023-12-15T08:07:57.522710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select the first 1000 rows\n",
    "selected_rows = pca_df_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T08:08:00.922345Z",
     "iopub.status.busy": "2023-12-15T08:08:00.921984Z",
     "iopub.status.idle": "2023-12-15T08:08:00.928627Z",
     "shell.execute_reply": "2023-12-15T08:08:00.927682Z",
     "shell.execute_reply.started": "2023-12-15T08:08:00.922317Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T08:08:03.733412Z",
     "iopub.status.busy": "2023-12-15T08:08:03.733052Z",
     "iopub.status.idle": "2023-12-15T08:08:03.738256Z",
     "shell.execute_reply": "2023-12-15T08:08:03.737296Z",
     "shell.execute_reply.started": "2023-12-15T08:08:03.733381Z"
    }
   },
   "outputs": [],
   "source": [
    "y_selected_rows=y_true[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T08:08:06.977777Z",
     "iopub.status.busy": "2023-12-15T08:08:06.976820Z",
     "iopub.status.idle": "2023-12-15T08:08:06.983592Z",
     "shell.execute_reply": "2023-12-15T08:08:06.982622Z",
     "shell.execute_reply.started": "2023-12-15T08:08:06.977720Z"
    }
   },
   "outputs": [],
   "source": [
    "y_selected_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T10:37:14.637771Z",
     "iopub.status.busy": "2024-02-25T10:37:14.636874Z",
     "iopub.status.idle": "2024-02-25T10:37:14.797631Z",
     "shell.execute_reply": "2024-02-25T10:37:14.796393Z",
     "shell.execute_reply.started": "2024-02-25T10:37:14.637707Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100,max_depth = 6 )  # You can adjust n_estimators\n",
    " # You can adjust n_estimators\n",
    "  # You can adjust n_estimators\n",
    "\n",
    "# Train the Random Forest Model\n",
    "rf_classifier.fit(X_train_1, y_train_1)\n",
    "# Step 4: Model Evaluation\n",
    "# y_pred = rf_classifier.predict(pca_df_test)  # Predict on Test Data\n",
    "y_pred = rf_classifier.predict(selected_rows)  # Predict on Test Data\n",
    "\n",
    "accuracy = accuracy_score(y_selected_rows, y_pred)\n",
    "accuracy_percent = accuracy * 100\n",
    "print(f\"Accuracy_2: {accuracy_percent:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T08:18:56.392964Z",
     "iopub.status.busy": "2023-12-15T08:18:56.392594Z",
     "iopub.status.idle": "2023-12-15T08:19:08.806558Z",
     "shell.execute_reply": "2023-12-15T08:19:08.805487Z",
     "shell.execute_reply.started": "2023-12-15T08:18:56.392934Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000,max_depth = 6 )  # You can adjust n_estimators\n",
    " # You can adjust n_estimators\n",
    "  # You can adjust n_estimators\n",
    "\n",
    "# Train the Random Forest Model\n",
    "rf_classifier.fit(X_train_2, y_train_1)\n",
    "# Step 4: Model Evaluation\n",
    "y_pred = rf_classifier.predict(pca_df_test)  # Predict on Test Data\n",
    "# y_pred = rf_classifier.predict(selected_rows)  # Predict on Test Data\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "accuracy_percent = accuracy * 100\n",
    "print(f\"Accuracy_2: {accuracy_percent:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.704232Z",
     "iopub.status.idle": "2023-12-14T08:45:40.704570Z",
     "shell.execute_reply": "2023-12-14T08:45:40.704422Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.704390Z"
    }
   },
   "outputs": [],
   "source": [
    "# with kmean_2\n",
    "# # train accuracy check\n",
    "# rf_classifier = RandomForestClassifier( n_estimators=20, max_depth=3,random_state=4)  # You can adjust n_estimators\n",
    "\n",
    "# # Train the Random Forest Model\n",
    "# rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Step 4: Model Evaluation on Train Data\n",
    "# y_train_pred = rf_classifier.predict(X_train)\n",
    "# accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "# accuracy_train_percent = accuracy_train * 100\n",
    "# print(f\"Train Accuracy: {accuracy_train_percent:.2f}%\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.705895Z",
     "iopub.status.idle": "2023-12-14T08:45:40.706272Z",
     "shell.execute_reply": "2023-12-14T08:45:40.706115Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.706097Z"
    }
   },
   "outputs": [],
   "source": [
    "# 30/3/42--best accuracy==>76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T08:30:15.735377Z",
     "iopub.status.busy": "2023-12-15T08:30:15.735007Z",
     "iopub.status.idle": "2023-12-15T08:30:15.999285Z",
     "shell.execute_reply": "2023-12-15T08:30:15.998317Z",
     "shell.execute_reply.started": "2023-12-15T08:30:15.735350Z"
    }
   },
   "outputs": [],
   "source": [
    " ## 2. AdaBoost Algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    " # Step 3: Initialize the base estimator (Decision Tree)\n",
    "base_estimator = DecisionTreeClassifier()  # You can adjust max_depth\n",
    " # Step 4: Initialize the AdaBoost Classifier\n",
    "adaboost_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50,random_state=32)\n",
    "# You can adjust n_estimators (number of weak learners)max_depth=3,random_state=32\n",
    " # Step 5: Train the AdaBoost Model\n",
    "adaboost_classifier.fit(X_train_2, y_train_1)\n",
    " # Step 6: Model Evaluation\n",
    "y_pred = adaboost_classifier.predict(pca_df_test)\n",
    "accuracy_test = accuracy_score(y_true, y_pred)\n",
    "# report = classification_report(y_true, y_pred,zero_division=1)\n",
    "\n",
    "accuracy_percent_test = accuracy_test * 100\n",
    "print(f\"Accuracy_test: {accuracy_percent_test:.2f}%\")  # Display accuracy in percentage format with two decimal places\n",
    "\n",
    "# print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T08:29:03.253935Z",
     "iopub.status.busy": "2023-12-15T08:29:03.253549Z",
     "iopub.status.idle": "2023-12-15T08:29:03.313112Z",
     "shell.execute_reply": "2023-12-15T08:29:03.312140Z",
     "shell.execute_reply.started": "2023-12-15T08:29:03.253906Z"
    }
   },
   "outputs": [],
   "source": [
    " ## 2. AdaBoost Algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    " # Step 3: Initialize the base estimator (Decision Tree)\n",
    "base_estimator = DecisionTreeClassifier()  # You can adjust max_depth\n",
    " # Step 4: Initialize the AdaBoost Classifier\n",
    "adaboost_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50,random_state=32)\n",
    "# You can adjust n_estimators (number of weak learners)max_depth=3,random_state=32\n",
    " # Step 5: Train the AdaBoost Model\n",
    "adaboost_classifier.fit(X_train_2, y_train_1)\n",
    " # Step 6: Model Evaluation\n",
    "y_pred = adaboost_classifier.predict(selected_rows)\n",
    "accuracy_test = accuracy_score(y_selected_rows, y_pred)\n",
    "# report = classification_report(y_true, y_pred,zero_division=1)\n",
    "\n",
    "accuracy_percent_test = accuracy_test * 100\n",
    "print(f\"Accuracy_test: {accuracy_percent_test:.2f}%\")  # Display accuracy in percentage format with two decimal places\n",
    "\n",
    "# print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T05:35:58.251216Z",
     "iopub.status.busy": "2023-12-15T05:35:58.250841Z",
     "iopub.status.idle": "2023-12-15T05:36:00.369876Z",
     "shell.execute_reply": "2023-12-15T05:36:00.368854Z",
     "shell.execute_reply.started": "2023-12-15T05:35:58.251184Z"
    }
   },
   "outputs": [],
   "source": [
    "### ## 2. AdaBoost Algorithm for train\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    " # Step 3: Initialize the base estimator (Decision Tree)\n",
    "base_estimator = DecisionTreeClassifier(max_depth=3)  # You can adjust max_depth\n",
    " # Step 4: Initialize the AdaBoost Classifier \n",
    "adaboost_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)\n",
    "# You can adjust n_estimators (number of weak learners)\n",
    " # Step 5: Train the AdaBoost Model\n",
    "adaboost_classifier.fit(X_train_2, y_train_1)\n",
    " # Step 6: Model Evaluation\n",
    "y_pred = adaboost_classifier.predict(X_train_2)\n",
    "accuracy_3 = accuracy_score(y_train_1, y_pred)\n",
    "# report = classification_report(y_true, y_pred,zero_division=1)\n",
    "\n",
    "accuracy_percent_3 = accuracy_3 * 100\n",
    "print(f\"Accuracy_3: {accuracy_percent_3:.2f}%\")  # Display accuracy in percentage format with two decimal places\n",
    "\n",
    "# print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.709060Z",
     "iopub.status.idle": "2023-12-14T08:45:40.709389Z",
     "shell.execute_reply": "2023-12-14T08:45:40.709240Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.709224Z"
    }
   },
   "outputs": [],
   "source": [
    "# base_estimator = DecisionTreeClassifier(max_depth=3)  # You can adjust max_depth\n",
    "#  # Step 4: Initialize the AdaBoost Classifier\n",
    "# adaboost_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)===>>70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T05:41:10.292739Z",
     "iopub.status.busy": "2023-12-15T05:41:10.291789Z",
     "iopub.status.idle": "2023-12-15T05:41:10.323106Z",
     "shell.execute_reply": "2023-12-15T05:41:10.322021Z",
     "shell.execute_reply.started": "2023-12-15T05:41:10.292684Z"
    }
   },
   "outputs": [],
   "source": [
    " # 3. ************  Naive Bayes Algorithm  ************\n",
    "#     for train\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a pipeline with MinMaxScaler and Gaussian Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),  # Normalize/Scale features\n",
    "    ('classifier', GaussianNB())  # Gaussian Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Train the model using the pipeline\n",
    "pipeline.fit(X_train_2, y_train_1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_train_2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train_1, y_pred)\n",
    "\n",
    "accuracy_percent = accuracy * 100\n",
    "print(f\"Accuracy_3: {accuracy_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:39:13.420819Z",
     "iopub.status.busy": "2023-12-14T19:39:13.420404Z",
     "iopub.status.idle": "2023-12-14T19:39:15.499371Z",
     "shell.execute_reply": "2023-12-14T19:39:15.498149Z",
     "shell.execute_reply.started": "2023-12-14T19:39:13.420787Z"
    }
   },
   "outputs": [],
   "source": [
    " # 3. ************  Naive Bayes Algorithm  ************\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    " # Create a pipeline with MinMax scaling and Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),  # Scale features to the [0, 1] range\n",
    "    ('classifier', GaussianNB())\n",
    "#     ('classifier', MultinomialNB())  # Use Gaussian Naive Bayes classifier\n",
    "])\n",
    " # Perform hyperparameter tuning using GridSearchCV\n",
    "# param_grid = {\n",
    "    # You can try different Naive Bayes variations and hyperparameters here\n",
    "#     'classifier__var_smoothing': [1e-9, 1e-8, 1e-7],\n",
    "# }\n",
    "# Define a grid of hyperparameter values to search over\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(-9, 0, 10)  # Adjust the range of values as needed -9 0 10\n",
    "}\n",
    " grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "#  grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid)\n",
    "\n",
    "grid_search.fit(X_train_2, y_train_1)\n",
    " # Get the best estimator from the grid search\n",
    "best_pipeline = grid_search.best_estimator_\n",
    " # Train the best pipeline on the training data\n",
    "best_pipeline.fit(X_train_2, y_train_1)\n",
    " # Make predictions on the test data\n",
    "y_pred = best_pipeline.predict(X_train_2)\n",
    " # Evaluate the model\n",
    "accuracy = accuracy_score(y_train_1, y_pred)\n",
    "\n",
    "accuracy_percent = accuracy * 100\n",
    "print(f\"Accuracy: {accuracy_percent:.2f}%\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T05:46:24.487271Z",
     "iopub.status.busy": "2023-12-15T05:46:24.486816Z",
     "iopub.status.idle": "2023-12-15T05:46:24.649688Z",
     "shell.execute_reply": "2023-12-15T05:46:24.648586Z",
     "shell.execute_reply.started": "2023-12-15T05:46:24.487242Z"
    }
   },
   "outputs": [],
   "source": [
    "## 4. SVM - Support Vector Machines code\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Support Vector Machine classifier\n",
    "# You can adjust the kernel (linear, polynomial, radial basis function, etc.) and other hyperparameters\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', random_state=53,C=3.0)\n",
    "# svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train_2, y_train_1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pre = svm_classifier.predict(X_train_2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train_1, y_pred)\n",
    "accuracy_percent = accuracy * 100\n",
    "print(f\"Accuracy: {accuracy_percent:.2f}%\") \n",
    "# Display classification report\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_true, y_pred,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T05:50:27.351318Z",
     "iopub.status.busy": "2023-12-15T05:50:27.350733Z",
     "iopub.status.idle": "2023-12-15T05:50:27.655248Z",
     "shell.execute_reply": "2023-12-15T05:50:27.654201Z",
     "shell.execute_reply.started": "2023-12-15T05:50:27.351283Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Support Vector Machine classifier\n",
    "# You can adjust the kernel (linear, polynomial, radial basis function, etc.) and other hyperparameters\n",
    "\n",
    "# svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train_2, y_train_1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_prediction = svm_classifier.predict(selected_rows)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_6 = accuracy_score(y_selected_rows, y_prediction)\n",
    "accuracy_percent_6 = accuracy_6 * 100\n",
    "print(f\"Accuracy: {accuracy_percent_6:.2f}%\") \n",
    "# Display classification report\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_true, y_pred,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.714736Z",
     "iopub.status.idle": "2023-12-14T08:45:40.715220Z",
     "shell.execute_reply": "2023-12-14T08:45:40.714968Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.714946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save reduced_data to a CSV file\n",
    "reduced_data.to_csv(\"reduced_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.716810Z",
     "iopub.status.idle": "2023-12-14T08:45:40.717266Z",
     "shell.execute_reply": "2023-12-14T08:45:40.717056Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.717035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save reduced_data to a CSV file with a full path\n",
    "reduced_data.to_csv(\"C:\\\\Users\\\\Rashmi\\\\Desktop\\\\reduced_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.718602Z",
     "iopub.status.idle": "2023-12-14T08:45:40.719068Z",
     "shell.execute_reply": "2023-12-14T08:45:40.718841Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.718820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "reloaded_data = pd.read_csv(\"reduced_data.csv\")\n",
    "\n",
    "# Now, reloaded_data contains the data from the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.720705Z",
     "iopub.status.idle": "2023-12-14T08:45:40.721188Z",
     "shell.execute_reply": "2023-12-14T08:45:40.720949Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.720926Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Rashmi\\Desktop\\reduced_data\\reduced_data\"\n",
    "\n",
    "try:\n",
    "    reduced_data.to_csv(file_path, index=False)\n",
    "    print(f\"File saved successfully at: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.722849Z",
     "iopub.status.idle": "2023-12-14T08:45:40.723370Z",
     "shell.execute_reply": "2023-12-14T08:45:40.723145Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.723111Z"
    }
   },
   "outputs": [],
   "source": [
    "print(reloaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.724949Z",
     "iopub.status.idle": "2023-12-14T08:45:40.725456Z",
     "shell.execute_reply": "2023-12-14T08:45:40.725213Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.725190Z"
    }
   },
   "outputs": [],
   "source": [
    "reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.727775Z",
     "iopub.status.idle": "2023-12-14T08:45:40.728165Z",
     "shell.execute_reply": "2023-12-14T08:45:40.727998Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.727962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random forest\n",
    "# Step 3: Model Training (Random Forest)\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.729556Z",
     "iopub.status.idle": "2023-12-14T08:45:40.729904Z",
     "shell.execute_reply": "2023-12-14T08:45:40.729754Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.729739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 1000, max_depth = 6, random_state=42)  # You can adjust n_estimators\n",
    "# Train the Random Forest Model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "# Step 4: Model Evaluation\n",
    "y_pred = rf_classifier.predict(X_test_pca)  # Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.731256Z",
     "iopub.status.idle": "2023-12-14T08:45:40.731582Z",
     "shell.execute_reply": "2023-12-14T08:45:40.731435Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.731420Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.733158Z",
     "iopub.status.idle": "2023-12-14T08:45:40.733502Z",
     "shell.execute_reply": "2023-12-14T08:45:40.733353Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.733336Z"
    }
   },
   "outputs": [],
   "source": [
    "#For Y_Train\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Assuming cluster_centers_df contains the cluster labels from K-Means\n",
    "cluster_mapping = {}  # Create a dictionary to map cluster labels to original target labels\n",
    "\n",
    "for cluster_label, target_label in zip(cluster_centers_df, target_column_numpy):\n",
    "    if cluster_label not in cluster_mapping:\n",
    "        cluster_mapping[cluster_label] = []\n",
    "    cluster_mapping[cluster_label].append(target_label)\n",
    "\n",
    "# Now you have a mapping of cluster labels to target labels\n",
    "# You can use this mapping to assign target labels to your clustered data\n",
    "clustered_y_train = np.array([np.argmax(cluster_mapping[label]) for label in cluster_centers_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.734886Z",
     "iopub.status.idle": "2023-12-14T08:45:40.735237Z",
     "shell.execute_reply": "2023-12-14T08:45:40.735082Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.735066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the sample submission CSV file\n",
    "submission_df = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\n",
    "\n",
    "# Extract the target labels for the first 100k rows (assuming your test data has 100k rows)\n",
    "y_true = submission_df['target'][:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.736886Z",
     "iopub.status.idle": "2023-12-14T08:45:40.737250Z",
     "shell.execute_reply": "2023-12-14T08:45:40.737096Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.737079Z"
    }
   },
   "outputs": [],
   "source": [
    "# # import cuml\n",
    "# # import cudf\n",
    "# # from cuml.cluster import KMeans\n",
    "# import numpy as np\n",
    "# import cudf\n",
    "# import cuml\n",
    "# from cuml.cluster import KMeans\n",
    "\n",
    "\n",
    "# # # Assuming you have your training data in a NumPy array\n",
    "# train_data_numpyy = np.array(result_array)  # Replace ... with your actual data\n",
    "\n",
    "# # Convert the NumPy array to a cuDF DataFrame\n",
    "# # train_data_cu = cudf.DataFrame(train_data_numpy)\n",
    "\n",
    "# # Assuming you have a GPU-enabled Kaggle environment and data loaded into train_data\n",
    "\n",
    "# # # Convert your train_data to a cuDF DataFrame\n",
    "# # train_data_cu = cudf.DataFrame(train_data)\n",
    "\n",
    "# # Define the number of clusters (K)\n",
    "# n_clusters = 10000\n",
    "\n",
    "# # Create and fit the K-means model\n",
    "# kmeans = cuml.KMeans(n_clusters=n_clusters, n_init=100)\n",
    "# kmeans.fit(train_data_numpyy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Predict the cluster labels for the train data\n",
    "# train_data_numpyy['cluster_labels'] = kmeans.labels_\n",
    "\n",
    "# # Calculate the cluster centers (representative points)\n",
    "# cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# # Create a DataFrame with cluster centers\n",
    "# cluster_centers_df = cudf.DataFrame(cluster_centers)\n",
    "\n",
    "# Now, cluster_centers_df contains K representative data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.738895Z",
     "iopub.status.idle": "2023-12-14T08:45:40.739386Z",
     "shell.execute_reply": "2023-12-14T08:45:40.739165Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.739141Z"
    }
   },
   "outputs": [],
   "source": [
    "                          # Agorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.740830Z",
     "iopub.status.idle": "2023-12-14T08:45:40.741281Z",
     "shell.execute_reply": "2023-12-14T08:45:40.741071Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.741048Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using SVM algorithm\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.742441Z",
     "iopub.status.idle": "2023-12-14T08:45:40.742911Z",
     "shell.execute_reply": "2023-12-14T08:45:40.742692Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.742669Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = cluster_centers_df\n",
    "y_train = clustered_y_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.744623Z",
     "iopub.status.idle": "2023-12-14T08:45:40.745101Z",
     "shell.execute_reply": "2023-12-14T08:45:40.744870Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.744847Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.746067Z",
     "iopub.status.idle": "2023-12-14T08:45:40.746536Z",
     "shell.execute_reply": "2023-12-14T08:45:40.746304Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.746282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline with MinMax scaling and Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),  # Scale features to the [0, 1] range\n",
    "    ('classifier', GaussianNB())  # Use Gaussian Naive Bayes classifier\n",
    "])\n",
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "# param_grid = {\n",
    "    # You can try different Naive Bayes variations and hyperparameters here\n",
    "#     'classifier__var_smoothing': [1e-9, 1e-8, 1e-7],\n",
    "# }\n",
    "# Define a grid of hyperparameter values to search over\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(-9, 0, 10)  # Adjust the range of values as needed -9 0 10\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.747970Z",
     "iopub.status.idle": "2023-12-14T08:45:40.748464Z",
     "shell.execute_reply": "2023-12-14T08:45:40.748233Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.748210Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sample submission CSV file\n",
    "submission_df = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\n",
    "\n",
    "# Extract the target labels for the first 100k rows (assuming your test data has 100k rows)\n",
    "y_true = submission_df['target'][:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.750182Z",
     "iopub.status.idle": "2023-12-14T08:45:40.750618Z",
     "shell.execute_reply": "2023-12-14T08:45:40.750409Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.750388Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming you have a trained SVM model named 'clf'\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.752205Z",
     "iopub.status.idle": "2023-12-14T08:45:40.752564Z",
     "shell.execute_reply": "2023-12-14T08:45:40.752414Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.752398Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.754139Z",
     "iopub.status.idle": "2023-12-14T08:45:40.754587Z",
     "shell.execute_reply": "2023-12-14T08:45:40.754375Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.754353Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.755603Z",
     "iopub.status.idle": "2023-12-14T08:45:40.756070Z",
     "shell.execute_reply": "2023-12-14T08:45:40.755843Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.755820Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Assuming X_train_pca is the PCA-transformed training data\n",
    "# # n_clusters is the number of clusters you want to create\n",
    "# n_clusters = 3  # You can adjust this number based on your problem\n",
    "\n",
    "# # Create a K-Means clustering model\n",
    "# kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "\n",
    "# # Fit the model to your PCA-transformed training data\n",
    "# kmeans.fit(X_train_pca)\n",
    "\n",
    "# # Get the cluster labels for each data point\n",
    "# cluster_labels = kmeans.labels_\n",
    "\n",
    "# You can now use cluster_labels for classification or analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.757494Z",
     "iopub.status.idle": "2023-12-14T08:45:40.757935Z",
     "shell.execute_reply": "2023-12-14T08:45:40.757730Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.757709Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the PCA-transformed data into training and test sets\n",
    "# X_train_pca, X_test_pca, = train_test_split(X_projected, test_size=0.3333333, random_state=42)\n",
    "\n",
    "# # Convert NumPy arrays to Pandas DataFrames\n",
    "# X_train_pca_df = pd.DataFrame(data=X_train_pca, columns=[f'PC{i}' for i in range(1, 34)])\n",
    "# X_test_pca_df = pd.DataFrame(data=X_test_pca, columns=[f'PC{i}' for i in range(1, 34)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.759660Z",
     "iopub.status.idle": "2023-12-14T08:45:40.760121Z",
     "shell.execute_reply": "2023-12-14T08:45:40.759890Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.759868Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.761132Z",
     "iopub.status.idle": "2023-12-14T08:45:40.761561Z",
     "shell.execute_reply": "2023-12-14T08:45:40.761355Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.761334Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.762941Z",
     "iopub.status.idle": "2023-12-14T08:45:40.763396Z",
     "shell.execute_reply": "2023-12-14T08:45:40.763188Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.763166Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.765159Z",
     "iopub.status.idle": "2023-12-14T08:45:40.765620Z",
     "shell.execute_reply": "2023-12-14T08:45:40.765387Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.765366Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade threadpoolctl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.767382Z",
     "iopub.status.idle": "2023-12-14T08:45:40.767825Z",
     "shell.execute_reply": "2023-12-14T08:45:40.767612Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.767590Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Columns in pca_df:\", pca_df.columns)\n",
    "print(\"Columns in train:\", train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.769193Z",
     "iopub.status.idle": "2023-12-14T08:45:40.769529Z",
     "shell.execute_reply": "2023-12-14T08:45:40.769375Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.769359Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Index of pca_df:\", pca_df.index)\n",
    "print(\"Index of train:\", train.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.770557Z",
     "iopub.status.idle": "2023-12-14T08:45:40.770888Z",
     "shell.execute_reply": "2023-12-14T08:45:40.770741Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.770725Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Columns in train:\", train.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.772263Z",
     "iopub.status.idle": "2023-12-14T08:45:40.772624Z",
     "shell.execute_reply": "2023-12-14T08:45:40.772461Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.772444Z"
    }
   },
   "outputs": [],
   "source": [
    "print(reduced_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.773705Z",
     "iopub.status.idle": "2023-12-14T08:45:40.774044Z",
     "shell.execute_reply": "2023-12-14T08:45:40.773870Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.773855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reducing rows using K-means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extracting numeric columns from pca_df\n",
    "numeric_columns = pca_df.select_dtypes(include=[np.number])\n",
    "num_clusters = 10000\n",
    "\n",
    "# Initialize KMeans with 'num_clusters' clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=1)  #random_state is a parameter that controls the random number generator used by the algorithm.\n",
    "                                                                   #n_init controls the number of times the algorithm will be run with different centroid initializations, and in your code, it's set to 1, meaning only one run.\n",
    "cluster_assignments = kmeans.fit_predict(numeric_columns)\n",
    "pca_df['cluster'] = cluster_assignments\n",
    "# Assuming 'target_df' contains the target column and shares the same index as 'pca_df'\n",
    "merged_df = pd.merge(pca_df, target_column, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "averaged_records = []\n",
    "\n",
    "# Loop through each unique cluster\n",
    "for cluster_id in merged_df['cluster'].unique():\n",
    "    cluster_records = merged_df[merged_df['cluster'] == cluster_id]\n",
    "    \n",
    "    # Counting occurrences of each class in the cluster\n",
    "    class_counts = cluster_records['target'].value_counts()\n",
    "    \n",
    "    #class with the most occurrences (predominant class)\n",
    "    predominant_class = class_counts.idxmax()\n",
    "    \n",
    "    # Select records of the predominant class and calculate their mean\n",
    "    predominant_records = cluster_records[cluster_records['target'] == predominant_class]\n",
    "    averaged_record = predominant_records.drop(columns=['cluster', 'target']).mean()\n",
    "    \n",
    "    # Assigning the predominant class to the averaged record\n",
    "    averaged_record['target'] = predominant_class\n",
    "    \n",
    "    # Appending the averaged record to the list\n",
    "    averaged_records.append(averaged_record)\n",
    "\n",
    "# Create the 'reduced_df' DataFrame from the list of averaged records\n",
    "reduced_df = pd.DataFrame(averaged_records)\n",
    "\n",
    "# 'reduced_df'contains the reduced and averaged records with the predominant class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.775583Z",
     "iopub.status.idle": "2023-12-14T08:45:40.775901Z",
     "shell.execute_reply": "2023-12-14T08:45:40.775761Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.775746Z"
    }
   },
   "outputs": [],
   "source": [
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.776870Z",
     "iopub.status.idle": "2023-12-14T08:45:40.777214Z",
     "shell.execute_reply": "2023-12-14T08:45:40.777065Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.777049Z"
    }
   },
   "outputs": [],
   "source": [
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.778113Z",
     "iopub.status.idle": "2023-12-14T08:45:40.778436Z",
     "shell.execute_reply": "2023-12-14T08:45:40.778297Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.778282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your target column is named 'target' and you want to use all 33 feature columns\n",
    "X_train = reduced_df.drop(columns=['target'])  # Extract all feature columns except 'target'\n",
    "y_train = reduced_df['target']  # Extract the 'target' column as labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.779560Z",
     "iopub.status.idle": "2023-12-14T08:45:40.779898Z",
     "shell.execute_reply": "2023-12-14T08:45:40.779751Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.779735Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assuming you have your training data X_train and y_train\n",
    "# clf = SVC(kernel='poly')  # Example with a linear kernel\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.781265Z",
     "iopub.status.idle": "2023-12-14T08:45:40.781593Z",
     "shell.execute_reply": "2023-12-14T08:45:40.781446Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.781430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.782495Z",
     "iopub.status.idle": "2023-12-14T08:45:40.782804Z",
     "shell.execute_reply": "2023-12-14T08:45:40.782658Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.782644Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sample submission CSV file\n",
    "submission_df = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\n",
    "\n",
    "# Extract the target labels for the first 100k rows (assuming your test data has 100k rows)\n",
    "y_true = submission_df['target'][:100000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.784500Z",
     "iopub.status.idle": "2023-12-14T08:45:40.784844Z",
     "shell.execute_reply": "2023-12-14T08:45:40.784693Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.784677Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming you have a trained SVM model named 'clf'\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.786634Z",
     "iopub.status.idle": "2023-12-14T08:45:40.787099Z",
     "shell.execute_reply": "2023-12-14T08:45:40.786876Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.786855Z"
    }
   },
   "outputs": [],
   "source": [
    "# y=reduced_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.788633Z",
     "iopub.status.idle": "2023-12-14T08:45:40.789112Z",
     "shell.execute_reply": "2023-12-14T08:45:40.788876Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.788853Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.790481Z",
     "iopub.status.idle": "2023-12-14T08:45:40.790953Z",
     "shell.execute_reply": "2023-12-14T08:45:40.790722Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.790699Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000,max_depth=6, random_state=42)\n",
    "# rf_classifier = RandomForestClassifier(\n",
    "#     n_estimators=1000,\n",
    "#     max_depth=6,\n",
    "#     min_samples_split=5,\n",
    "#     class_weight=\"balanced\",\n",
    "#     random_state=42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.792722Z",
     "iopub.status.idle": "2023-12-14T08:45:40.793217Z",
     "shell.execute_reply": "2023-12-14T08:45:40.792995Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.792959Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.794636Z",
     "iopub.status.idle": "2023-12-14T08:45:40.795051Z",
     "shell.execute_reply": "2023-12-14T08:45:40.794841Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.794824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = rf_classifier.predict(X_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.796528Z",
     "iopub.status.idle": "2023-12-14T08:45:40.796895Z",
     "shell.execute_reply": "2023-12-14T08:45:40.796728Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.796712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.798682Z",
     "iopub.status.idle": "2023-12-14T08:45:40.799073Z",
     "shell.execute_reply": "2023-12-14T08:45:40.798878Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.798861Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can also print a detailed classification report\n",
    "report = classification_report(y_true, y_pred )\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.800169Z",
     "iopub.status.idle": "2023-12-14T08:45:40.800505Z",
     "shell.execute_reply": "2023-12-14T08:45:40.800355Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.800338Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.801612Z",
     "iopub.status.idle": "2023-12-14T08:45:40.801945Z",
     "shell.execute_reply": "2023-12-14T08:45:40.801795Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.801779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.803552Z",
     "iopub.status.idle": "2023-12-14T08:45:40.803885Z",
     "shell.execute_reply": "2023-12-14T08:45:40.803737Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.803721Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.805095Z",
     "iopub.status.idle": "2023-12-14T08:45:40.805409Z",
     "shell.execute_reply": "2023-12-14T08:45:40.805266Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.805251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline with MinMax scaling and Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),  # Scale features to the [0, 1] range\n",
    "    ('classifier', GaussianNB())  # Use Gaussian Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.806592Z",
     "iopub.status.idle": "2023-12-14T08:45:40.806911Z",
     "shell.execute_reply": "2023-12-14T08:45:40.806770Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.806755Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline with MinMax scaling and Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),  # Scale features to the [0, 1] range\n",
    "    ('classifier', GaussianNB())  # Use Gaussian Naive Bayes classifier\n",
    "])\n",
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "# param_grid = {\n",
    "    # You can try different Naive Bayes variations and hyperparameters here\n",
    "#     'classifier__var_smoothing': [1e-9, 1e-8, 1e-7],\n",
    "# }\n",
    "# Define a grid of hyperparameter values to search over\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(-9, 0, 10)  # Adjust the range of values as needed -9 0 10\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.808079Z",
     "iopub.status.idle": "2023-12-14T08:45:40.808413Z",
     "shell.execute_reply": "2023-12-14T08:45:40.808266Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.808251Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.809661Z",
     "iopub.status.idle": "2023-12-14T08:45:40.810020Z",
     "shell.execute_reply": "2023-12-14T08:45:40.809844Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.809828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the best estimator from the grid search\n",
    "best_pipeline = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.811395Z",
     "iopub.status.idle": "2023-12-14T08:45:40.811732Z",
     "shell.execute_reply": "2023-12-14T08:45:40.811577Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.811561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the best pipeline on the training data\n",
    "best_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.812700Z",
     "iopub.status.idle": "2023-12-14T08:45:40.813057Z",
     "shell.execute_reply": "2023-12-14T08:45:40.812881Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.812865Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = best_pipeline.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.814286Z",
     "iopub.status.idle": "2023-12-14T08:45:40.814618Z",
     "shell.execute_reply": "2023-12-14T08:45:40.814472Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.814456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print( accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.815672Z",
     "iopub.status.idle": "2023-12-14T08:45:40.815998Z",
     "shell.execute_reply": "2023-12-14T08:45:40.815837Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.815822Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a Gaussian Naive Bayes classifier\n",
    "# nb_classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.817559Z",
     "iopub.status.idle": "2023-12-14T08:45:40.817868Z",
     "shell.execute_reply": "2023-12-14T08:45:40.817729Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.817715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "# nb_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.819521Z",
     "iopub.status.idle": "2023-12-14T08:45:40.819863Z",
     "shell.execute_reply": "2023-12-14T08:45:40.819715Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.819698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the pipeline (scaling and classification)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.821451Z",
     "iopub.status.idle": "2023-12-14T08:45:40.821772Z",
     "shell.execute_reply": "2023-12-14T08:45:40.821623Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.821608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.822833Z",
     "iopub.status.idle": "2023-12-14T08:45:40.823174Z",
     "shell.execute_reply": "2023-12-14T08:45:40.823025Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.823003Z"
    }
   },
   "outputs": [],
   "source": [
    "# C4.5 Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.824290Z",
     "iopub.status.idle": "2023-12-14T08:45:40.824598Z",
     "shell.execute_reply": "2023-12-14T08:45:40.824456Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.824442Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.model_selection import  GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.827331Z",
     "iopub.status.idle": "2023-12-14T08:45:40.827675Z",
     "shell.execute_reply": "2023-12-14T08:45:40.827519Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.827503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a C4.5 Decision Tree classifier\n",
    "# c45_classifier = DecisionTreeClassifier(criterion='entropy',splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.828695Z",
     "iopub.status.idle": "2023-12-14T08:45:40.829031Z",
     "shell.execute_reply": "2023-12-14T08:45:40.828861Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.828847Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.832229Z",
     "iopub.status.idle": "2023-12-14T08:45:40.832540Z",
     "shell.execute_reply": "2023-12-14T08:45:40.832401Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.832386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.834022Z",
     "iopub.status.idle": "2023-12-14T08:45:40.834488Z",
     "shell.execute_reply": "2023-12-14T08:45:40.834277Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.834256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifier on the training data\n",
    "dt_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.840863Z",
     "iopub.status.idle": "2023-12-14T08:45:40.841222Z",
     "shell.execute_reply": "2023-12-14T08:45:40.841061Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.841044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = dt_classifier.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.842509Z",
     "iopub.status.idle": "2023-12-14T08:45:40.842844Z",
     "shell.execute_reply": "2023-12-14T08:45:40.842697Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.842665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-14T08:45:40.851060Z",
     "iopub.status.idle": "2023-12-14T08:45:40.851414Z",
     "shell.execute_reply": "2023-12-14T08:45:40.851262Z",
     "shell.execute_reply.started": "2023-12-14T08:45:40.851246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the model\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 3168850,
     "sourceId": 33102,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30527,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
