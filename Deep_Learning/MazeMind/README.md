# ğŸŒ€ MazeMind â€“ Dynamic Pathfinding AI for Smart Maze Exploration

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Tests](https://img.shields.io/badge/tests-59%20passed-brightgreen.svg)](tests/)

MazeMind is a comprehensive AI system that combines classical pathfinding algorithms with reinforcement learning for intelligent maze exploration. The system generates procedural mazes and demonstrates the synergy between traditional search algorithms (A*, BFS, DFS, Dijkstra) and modern Q-learning approaches.


## ğŸ¯ Key Features

### ğŸ§© **Maze Generation**
- **Depth-First Search (DFS)** - Creates complex, winding mazes with guaranteed solvability
- **Prim's Algorithm** - Alternative generation method with different maze characteristics
- **Recursive Division** - Structured approach creating distinct maze patterns
- **Configurable Complexity** - Adjustable maze difficulty and structure

### ğŸ” **Classical Pathfinding**
- **A* Algorithm** - Optimal pathfinding with heuristic optimization
- **Breadth-First Search (BFS)** - Guaranteed shortest path discovery
- **Depth-First Search (DFS)** - Fast exploration with memory efficiency
- **Dijkstra's Algorithm** - Optimal pathfinding for weighted graphs
- **Performance Metrics** - Detailed analysis of execution time, nodes explored, and path optimality

### ğŸ¤– **Reinforcement Learning**
- **Q-Learning Agent** - Adaptive maze navigation without prior knowledge
- **Epsilon-Greedy Exploration** - Balanced exploration vs exploitation strategy
- **State Representation** - Intelligent local maze view with relative goal positioning
- **Dynamic Learning** - Continuous improvement through experience
- **Persistent Knowledge** - Save/load trained models for reuse

### ğŸ“Š **Analysis & Visualization**
- **Real-time Visualization** - Interactive maze display with solution paths
- **Animated Pathfinding** - Step-by-step algorithm execution visualization
- **Performance Comparison** - Side-by-side algorithm analysis across multiple metrics
- **Training Progress** - RL agent learning curve visualization
- **Statistical Analysis** - Comprehensive performance benchmarking

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/MazeMind.git
cd MazeMind

# Install dependencies
pip install -r requirements.txt
```

### Run the Demo

```bash
# Quick feature demonstration
python demo.py

# Interactive simulation environment
python simulate.py
```

## ğŸ® Usage Examples

### 1. Basic Maze Generation and Solving

```python
from maze_generator import MazeGenerator
from pathfinding import PathFinder
from simulate import MazeSimulator

# Generate a 20x20 maze
generator = MazeGenerator(width=20, height=20)
maze = generator.generate_dfs()
start, goal = generator.get_start_end_points()

# Solve using A* algorithm
pathfinder = PathFinder()
result = pathfinder.solve_maze(maze, start, goal, algorithm="a_star")

print(f"Path found: {result['success']}")
print(f"Path length: {result['path_length']}")
print(f"Execution time: {result['execution_time']:.4f}s")
print(f"Nodes explored: {result['nodes_explored']}")

# Visualize the solution
simulator = MazeSimulator()
simulator.visualize_path(maze, result['path'], start, goal)
```

### 2. Training a Reinforcement Learning Agent

```python
from rl_agent import QLearningAgent
from simulate import MazeSimulator

# Create and configure RL agent
agent = QLearningAgent(
    learning_rate=0.1,
    discount_factor=0.9,
    epsilon=0.2,
    epsilon_decay=0.995
)

# Train the agent
simulator = MazeSimulator()
training_results = simulator.train_agent(
    agent, 
    episodes=1000, 
    maze_size=15
)

print(f"Final success rate: {training_results['final_success_rate']:.3f}")
print(f"Average steps: {training_results['final_avg_steps']:.1f}")

# Test on a new maze
test_maze = generator.generate_dfs()
test_result = agent.solve_maze(test_maze, start, goal)
print(f"Test success: {test_result['success']}")
```

### 3. Algorithm Performance Comparison

```python
from simulate import MazeSimulator

simulator = MazeSimulator()

# Compare algorithms across different maze sizes
results = simulator.compare_algorithms(
    maze_sizes=[10, 20, 30],
    num_trials=10
)

# Visualize comparison results
simulator.plot_comparison(results)

# Access detailed results
for size in results:
    print(f"\nMaze Size: {size}x{size}")
    for algorithm in ['bfs', 'dfs', 'a_star', 'dijkstra']:
        trials = results[size][algorithm]
        success_rate = sum(1 for r in trials if r['success']) / len(trials)
        avg_time = sum(r['execution_time'] for r in trials) / len(trials)
        print(f"  {algorithm.upper()}: {success_rate:.2%} success, {avg_time*1000:.2f}ms avg")
```

### 4. Advanced RL Training with Multiple Agents

```python
from rl_agent import QLearningAgent, MultiAgentSystem

# Create multi-agent system
system = MultiAgentSystem()

# Add agents with different configurations
system.add_agent("explorer", QLearningAgent(epsilon=0.3))
system.add_agent("greedy", QLearningAgent(epsilon=0.1))
system.add_agent("balanced", QLearningAgent(epsilon=0.2))

# Train all agents
results = system.train_all_agents(maze, start, goal, episodes=500)

# Compare agent performance
comparison = system.compare_agents(maze, start, goal)
for name, result in comparison.items():
    print(f"{name}: Success={result['success']}, Steps={result['steps']}")
```

## ğŸ—ï¸ Project Architecture

```
MazeMind/
â”œâ”€â”€ ğŸ“ Core Modules
â”‚   â”œâ”€â”€ maze_generator.py      # Maze generation algorithms
â”‚   â”œâ”€â”€ pathfinding.py         # Classical search algorithms
â”‚   â”œâ”€â”€ rl_agent.py           # Q-Learning implementation
â”‚   â””â”€â”€ simulate.py           # Simulation and visualization
â”œâ”€â”€ ğŸ“ Demo & Examples
â”‚   â””â”€â”€ demo.py               # Feature demonstration script
â”œâ”€â”€ ğŸ“ Testing Suite
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_maze_generator.py
â”‚   â”‚   â”œâ”€â”€ test_pathfinding.py
â”‚   â”‚   â”œâ”€â”€ test_rl_agent.py
â”‚   â”‚   â””â”€â”€ test_simulate.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“„ Configuration
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â””â”€â”€ README.md            # This documentation
```

## ğŸ§ª Testing

The project includes a comprehensive test suite with 59 test cases covering all functionality:

```bash
# Run all tests
python -m pytest tests/ -v

# Run specific test modules
python -m pytest tests/test_maze_generator.py -v    # Maze generation tests
python -m pytest tests/test_pathfinding.py -v      # Pathfinding algorithm tests
python -m pytest tests/test_rl_agent.py -v         # RL agent tests
python -m pytest tests/test_simulate.py -v         # Simulation tests

# Run tests with coverage
python -m pytest tests/ --cov=. --cov-report=html
```

### Test Coverage
- âœ… **Maze Generation**: Validation, connectivity, edge cases
- âœ… **Pathfinding**: Algorithm correctness, performance, edge cases
- âœ… **RL Agent**: Learning, state representation, save/load functionality
- âœ… **Simulation**: Visualization, training, comparison features

## ğŸ“Š Performance Benchmarks

### Algorithm Comparison Results

| Algorithm | Success Rate | Avg Path Length | Avg Time (ms) | Nodes Explored | Optimality |
|-----------|--------------|-----------------|---------------|----------------|------------|
| **A***    | 100%         | 25.3           | 0.45          | 32             | âœ… Optimal |
| **BFS**   | 100%         | 25.3           | 0.62          | 45             | âœ… Optimal |
| **Dijkstra** | 100%     | 25.3           | 0.78          | 38             | âœ… Optimal |
| **DFS**   | 100%         | 31.7           | 0.23          | 28             | âŒ Suboptimal |
| **Q-Learning** | 85%    | 28.9           | 1.20          | N/A            | ğŸŸ¡ Near-Optimal |

*Benchmarks on 20x20 mazes, averaged over 100 trials*

### RL Training Performance
- **Initial Success Rate**: ~5%
- **Final Success Rate**: ~85% (after 1000 episodes)
- **Convergence Time**: ~500 episodes
- **Q-Table Size**: ~150-300 states (depending on maze complexity)

## ğŸ”§ Configuration Options

### Maze Generator Settings
```python
generator = MazeGenerator(
    width=25,              # Maze width (odd numbers recommended)
    height=25,             # Maze height (odd numbers recommended)
    complexity=0.7         # Complexity factor (0.0 to 1.0)
)
```

### Pathfinding Algorithm Selection
```python
algorithms = ['bfs', 'dfs', 'a_star', 'dijkstra']
result = pathfinder.solve_maze(maze, start, goal, algorithm='a_star')
```

### RL Agent Hyperparameters
```python
agent = QLearningAgent(
    learning_rate=0.1,     # Learning rate (alpha)
    discount_factor=0.9,   # Discount factor (gamma)
    epsilon=0.1,           # Exploration rate
    epsilon_decay=0.995,   # Epsilon decay rate
    min_epsilon=0.01       # Minimum epsilon value
)
```

### Visualization Options
```python
simulator = MazeSimulator(figsize=(12, 8))

# Static visualization
simulator.visualize_path(maze, path, start, goal, title="A* Solution")

# Animated visualization
simulator.animate_solution(maze, path, start, goal, interval=200)

# Save visualizations
simulator.visualize_path(maze, path, start, goal, save_path="solution.png")
```

## ğŸ¨ Visualization Gallery

The system provides multiple visualization modes:

1. **Static Path Visualization** - Shows complete solution path
2. **Animated Pathfinding** - Step-by-step algorithm execution
3. **Training Progress Plots** - RL agent learning curves
4. **Algorithm Comparison Charts** - Performance metrics comparison
5. **Interactive Exploration** - Real-time maze interaction

## ğŸš€ Advanced Features

### Custom Maze Algorithms
Extend the system with your own maze generation algorithms:

```python
class CustomMazeGenerator(MazeGenerator):
    def generate_custom(self):
        # Implement your custom algorithm
        maze = np.zeros((self.height, self.width), dtype=int)
        # ... your algorithm logic ...
        return maze
```

### Custom Pathfinding Algorithms
Add new pathfinding algorithms:

```python
class CustomPathFinder(PathFinder):
    def custom_search(self, maze, start, goal):
        # Implement your custom pathfinding algorithm
        # Return path as list of (x, y) tuples
        pass
```

### Advanced RL Agents
Implement more sophisticated RL agents:

```python
class DeepQLearningAgent(QLearningAgent):
    def __init__(self):
        super().__init__()
        # Add neural network components
        # Implement deep Q-learning logic
```

### Development Guidelines
- Follow PEP 8 style guidelines
- Add comprehensive docstrings
- Include unit tests for new functionality
- Update documentation as needed
- Ensure all tests pass before submitting

## ğŸ“š Educational Value

MazeMind serves as an excellent educational tool for:

- **Algorithm Design**: Understanding search algorithms and their trade-offs
- **Reinforcement Learning**: Practical Q-learning implementation
- **Performance Analysis**: Comparing algorithmic approaches
- **Visualization**: Understanding how algorithms work step-by-step
- **Software Engineering**: Clean, modular, and tested code architecture

## ğŸ› Troubleshooting

### Common Issues

**Q: Visualization not showing**
A: Ensure matplotlib backend is properly configured. For headless environments, the system automatically uses 'Agg' backend.

**Q: RL agent not learning**
A: Try adjusting hyperparameters: increase learning rate, decrease epsilon decay, or train for more episodes.

**Q: Tests failing**
A: Ensure all dependencies are installed: `pip install -r requirements.txt`

**Q: Memory usage too high**
A: Reduce maze size or limit the number of training episodes for RL agents.

## ğŸ“ˆ Performance Optimization Tips

1. **Use A* for optimal pathfinding** with reasonable performance
2. **Start with smaller mazes** (10x10) for RL training, then scale up
3. **Adjust epsilon decay** based on maze complexity
4. **Use DFS for fast exploration** when optimality isn't required
5. **Batch training episodes** for better RL performance

