# ðŸ§  Autonomous Snake Navigation System - Advanced Reinforcement Learning Framework

A Python implementation of the classic Snake game with reinforcement learning agents (Q-Learning and Deep Q-Network) to train AI to play optimally.

## ðŸŽ¯ Project Overview

This project implements:
- **Snake Game Environment**: A Pygame-based game environment with configurable grid sizes
- **Q-Learning Agent**: Tabular Q-learning for smaller state spaces
- **Deep Q-Network (DQN)**: Neural network-based agent for larger state spaces
- **Training Pipeline**: Complete training loop with metrics and visualization
- **Testing Suite**: Comprehensive tests for all components

## ðŸš€ Features

- **Flexible Environment**: Configurable grid sizes, reward functions, and game parameters
- **Multiple RL Agents**: Both Q-Learning and DQN implementations
- **Real-time Visualization**: Watch the agent learn and play
- **Comprehensive Logging**: Track training progress and performance metrics
- **Extensive Testing**: Unit tests for all components

## ðŸ“‹ Requirements

- Python 3.8+
- Pygame
- PyTorch
- NumPy
- Matplotlib
- Seaborn

## ðŸ› ï¸ Installation

1. **Clone the repository**:

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

## ðŸŽ® Usage

### Quick Start - Watch AI Play

```bash
python main.py --mode train --agent dqn --episodes 1000
```

### Train Q-Learning Agent

```bash
python main.py --mode train --agent qlearning --episodes 500 --grid-size 10
```

### Train DQN Agent

```bash
python main.py --mode train --agent dqn --episodes 2000 --grid-size 15
```

### Watch Trained Agent

```bash
python main.py --mode play --agent dqn --model-path models/dqn_model.pth
```

### Run Tests

```bash
python -m pytest tests/ -v
```

## ðŸ“Š Training Results

The agents typically achieve:
- **Q-Learning**: 15-25 average score on 10x10 grid
- **DQN**: 30-50 average score on 15x15 grid

Training progress is logged and visualized with:
- Average score per episode
- Reward progression
- Learning curves

## ðŸ—ï¸ Project Structure

```
SnakeRL/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ snake_env.py          # Snake game environment
â”‚   â”‚   â””â”€â”€ state_representation.py # State encoding utilities
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_agent.py         # Abstract base agent
â”‚   â”‚   â”œâ”€â”€ qlearning_agent.py    # Q-Learning implementation
â”‚   â”‚   â””â”€â”€ dqn_agent.py          # Deep Q-Network implementation
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ visualization.py      # Plotting and visualization
â”‚   â”‚   â””â”€â”€ logger.py             # Logging utilities
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ dqn_model.py          # Neural network architecture
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_environment.py       # Environment tests
â”‚   â”œâ”€â”€ test_agents.py            # Agent tests
â”‚   â””â”€â”€ test_utils.py             # Utility tests
â”œâ”€â”€ main.py                       # Main training script
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ README.md                     # This file
â””â”€â”€ .gitignore                    # Git ignore file
```

## ðŸ§ª Testing

The project includes comprehensive tests:

```bash
# Run all tests
python -m pytest tests/ -v

# Run specific test file
python -m pytest tests/test_environment.py -v

# Run with coverage
python -m pytest tests/ --cov=src --cov-report=html
```

## ðŸ“ˆ Performance Metrics

- **Average Score**: Mean score over last 100 episodes
- **Best Score**: Highest score achieved during training
- **Success Rate**: Percentage of episodes with score > 0
- **Training Time**: Time taken to complete training

## ðŸ”§ Configuration

Key parameters can be adjusted in the training script:
- Grid size (default: 10x10)
- Number of episodes (default: 1000)
- Learning rate (default: 0.1 for Q-Learning, 0.001 for DQN)
- Epsilon decay (default: 0.995)
- Reward values (food: +10, collision: -10, step: -0.1)