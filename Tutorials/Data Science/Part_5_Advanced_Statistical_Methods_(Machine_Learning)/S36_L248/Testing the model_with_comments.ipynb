{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#Apply a fix to the statsmodels library\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('2.02. Binary predictors.csv')\n",
    "data = raw_data.copy()\n",
    "data['Admitted'] = data['Admitted'].map({'Yes': 1, 'No': 0})\n",
    "data['Gender'] = data['Gender'].map({'Female': 1, 'Male': 0})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the dependent and the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Admitted']\n",
    "x1 = data[['SAT','Gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = sm.add_constant(x1)\n",
    "reg_log = sm.Logit(y,x)\n",
    "results_log = reg_log.fit()\n",
    "# Get the regression summary\n",
    "results_log.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.exp(1.94)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "#np.set_printoptions(formatter=None)\n",
    "results_log.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(data['Admitted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_log.pred_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_log.pred_table(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(results_log.pred_table())\n",
    "cm_df.columns = ['Predicted 0','Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = np.array(cm_df)\n",
    "accuracy_train = (cm[0,0]+cm[1,1])/cm.sum()\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model and assessing its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test = pd.read_csv('2.03. Test dataset.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map the test data as you did with the train data\n",
    "test['Admitted'] = test['Admitted'].map({'Yes': 1, 'No': 0})\n",
    "test['Gender'] = test['Gender'].map({'Female': 1, 'Male': 0})\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check how the inputs should look like\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the actual values (true valies ; targets)\n",
    "test_actual = test['Admitted']\n",
    "# Prepare the test data to be predicted\n",
    "test_data = test.drop(['Admitted'],axis=1)\n",
    "test_data = sm.add_constant(test_data)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(data,actual_values,model):\n",
    "        \n",
    "        # Confusion matrix \n",
    "        \n",
    "        # Parameters\n",
    "        # ----------\n",
    "        # data: data frame or array\n",
    "            # data is a data frame formatted in the same way as your input data (without the actual values)\n",
    "            # e.g. const, var1, var2, etc. Order is very important!\n",
    "        # actual_values: data frame or array\n",
    "            # These are the actual values from the test_data\n",
    "            # In the case of a logistic regression, it should be a single column with 0s and 1s\n",
    "            \n",
    "        # model: a LogitResults object\n",
    "            # this is the variable where you have the fitted model \n",
    "            # e.g. results_log in this course\n",
    "        # ----------\n",
    "        \n",
    "        #Predict the values using the Logit model\n",
    "        pred_values = model.predict(data)\n",
    "        # Specify the bins \n",
    "        bins=np.array([0,0.5,1])\n",
    "        # Create a histogram, where if values are between 0 and 0.5 tell will be considered 0\n",
    "        # if they are between 0.5 and 1, they will be considered 1\n",
    "        cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n",
    "        # Calculate the accuracy\n",
    "        accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n",
    "        # Return the confusion matrix and the accuracy\n",
    "        return cm, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a confusion matrix with the test data\n",
    "cm = confusion_matrix(test_data,test_actual,results_log)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for easier understanding (not needed later on)\n",
    "cm_df = pd.DataFrame(cm[0])\n",
    "cm_df.columns = ['Predicted 0','Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missclassification rate\n",
    "# Note that Accuracy + Missclassification rate = 1 = 100%\n",
    "print ('Missclassification rate: '+str((1+1)/19))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
