{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae7ac78",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b466a645",
   "metadata": {},
   "source": [
    "Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08366288",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "DATASET_PATH = 'sign_language_dataset'  # Root directory for dataset\n",
    "LABELS = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\") \n",
    "CAPTURE_DELAY = 0.5  # Time between captures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd953f",
   "metadata": {},
   "source": [
    "Set Up Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b424d",
   "metadata": {},
   "source": [
    "Dataset Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in LABELS:\n",
    "    os.makedirs(os.path.join(DATASET_PATH, label), exist_ok=True)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press a letter key (A-Z) to start capturing images for that class.\")\n",
    "print(\"Press ESC to exit.\")\n",
    "\n",
    "last_capture_time = time.time()\n",
    "current_label = None\n",
    "capturing = False\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    \n",
    "    # Define Region of Interest (ROI)\n",
    "    x1, y1 = frame_width - 300, 100\n",
    "    x2, y2 = frame_width - 100, 300\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    rgb_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Draw ROI on frame\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Detect hand landmarks\n",
    "    results = hands.process(rgb_roi)\n",
    "    white_bg = np.ones_like(roi, dtype=np.uint8) * 255\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                white_bg,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS\n",
    "            )\n",
    "\n",
    "        # Capture and save image if under 100 limit\n",
    "        if capturing and (time.time() - last_capture_time > CAPTURE_DELAY) and count < 100:\n",
    "            gray = cv2.cvtColor(white_bg, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE))\n",
    "            save_path = os.path.join(DATASET_PATH, current_label, f\"{current_label}_{count}.png\")\n",
    "            cv2.imwrite(save_path, resized)\n",
    "            print(f\"[INFO] Saved: {save_path}\")\n",
    "            count += 1\n",
    "            last_capture_time = time.time()\n",
    "\n",
    "        # Stop capturing once 100 images are reached\n",
    "        if count >= 100:\n",
    "            print(f\"[INFO] Reached 100 images for '{current_label}'. Stopping capture.\")\n",
    "            capturing = False\n",
    "\n",
    "    # Display current label\n",
    "    if current_label:\n",
    "        cv2.putText(frame, f\"Label: {current_label}\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Dataset Collection\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Start/continue capturing when A-Z is pressed\n",
    "    if 65 <= key <= 90:  # ASCII for A-Z\n",
    "        current_label = chr(key)\n",
    "        capturing = True\n",
    "        count = len(os.listdir(os.path.join(DATASET_PATH, current_label)))\n",
    "        print(f\"[INFO] Started capturing for '{current_label}'... Existing: {count}\")\n",
    "    elif key == 27:  # ESC key to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31f89e",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a3274",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfcc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b968bcc",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = 'sign_language_dataset'  # Path to your dataset\n",
    "IMG_SIZE = 64 \n",
    "EPOCHS = 10\n",
    "\n",
    "# Load images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label in os.listdir(DATASET_DIR):\n",
    "    label_dir = os.path.join(DATASET_DIR, label)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue\n",
    "    for img_file in os.listdir(label_dir):\n",
    "        img_path = os.path.join(label_dir, img_file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751f85a",
   "metadata": {},
   "source": [
    "Preprocessing and Train,Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and reshape\n",
    "images = images / 255.0\n",
    "images = images.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "# Encode labels\n",
    "lb = LabelBinarizer()\n",
    "labels_encoded = lb.fit_transform(labels)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a07651",
   "metadata": {},
   "source": [
    "Build, Train  and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003f4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muska\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,354</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,605,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)             │         \u001b[38;5;34m3,354\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,627,930</span> (6.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,627,930\u001b[0m (6.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,627,930</span> (6.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,627,930\u001b[0m (6.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.0661 - loss: 3.2905 - val_accuracy: 0.4596 - val_loss: 2.5887\n",
      "Epoch 2/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.4433 - loss: 2.0905 - val_accuracy: 0.9385 - val_loss: 0.4391\n",
      "Epoch 3/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8210 - loss: 0.6741 - val_accuracy: 0.9769 - val_loss: 0.1345\n",
      "Epoch 4/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.8996 - loss: 0.3554 - val_accuracy: 0.9846 - val_loss: 0.1004\n",
      "Epoch 5/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9345 - loss: 0.2478 - val_accuracy: 0.9885 - val_loss: 0.0564\n",
      "Epoch 6/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9461 - loss: 0.1840 - val_accuracy: 0.9904 - val_loss: 0.0581\n",
      "Epoch 7/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9664 - loss: 0.1443 - val_accuracy: 0.9904 - val_loss: 0.0508\n",
      "Epoch 8/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9675 - loss: 0.1230 - val_accuracy: 0.9942 - val_loss: 0.0490\n",
      "Epoch 9/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9793 - loss: 0.0846 - val_accuracy: 0.9923 - val_loss: 0.0383\n",
      "Epoch 10/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9816 - loss: 0.0631 - val_accuracy: 0.9942 - val_loss: 0.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(lb.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save model and label binarizer\n",
    "model.save('sign_model.h5')\n",
    "import pickle\n",
    "with open('label_binarizer.pkl', 'wb') as f:\n",
    "    pickle.dump(lb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd2916",
   "metadata": {},
   "source": [
    "Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7becda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 's' to START collecting letters.\n",
      "Press 'e' to END and show the word.\n",
      "Press 'q' to QUIT.\n",
      "[INFO] Started collecting letters...\n",
      "[INFO] Added letter: H\n",
      "[INFO] Added letter: I\n",
      "[INFO] Added letter: M\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "IMG_SIZE = 64\n",
    "\n",
    "# Load model and label binarizer\n",
    "model = load_model('sign_model.h5')\n",
    "with open('label_binarizer.pkl', 'rb') as f:\n",
    "    lb = pickle.load(f)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Word collection state\n",
    "collecting = False\n",
    "collected_word = \"\"\n",
    "stored_words = []\n",
    "last_prediction = \"\"\n",
    "last_time = time.time()\n",
    "\n",
    "print(\"Press 's' to START collecting letters.\")\n",
    "print(\"Press 'e' to END and show the word.\")\n",
    "print(\"Press 'q' to QUIT.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "\n",
    "    # Define two ROIs:\n",
    "    # Left box (hand must be present here)\n",
    "    left_x1, left_y1 = 50, 100\n",
    "    left_x2, left_y2 = 250, 300\n",
    "    roi_left = frame[left_y1:left_y2, left_x1:left_x2]\n",
    "    rgb_left = cv2.cvtColor(roi_left, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Right box (detect sign here)\n",
    "    right_x1, right_y1 = frame_width - 300, 100\n",
    "    right_x2, right_y2 = frame_width - 100, 300\n",
    "    roi_right = frame[right_y1:right_y2, right_x1:right_x2]\n",
    "    rgb_right = cv2.cvtColor(roi_right, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect hands in left box\n",
    "    results_left = hands.process(rgb_left)\n",
    "    hand_in_left = bool(results_left.multi_hand_landmarks)\n",
    "\n",
    "    # Detect hands in right box\n",
    "    results_right = hands.process(rgb_right)\n",
    "    hand_in_right = bool(results_right.multi_hand_landmarks)\n",
    "\n",
    "    label = \"No Hand\"\n",
    "    palm_visible = False\n",
    "\n",
    "    # Predict only if hand present in right box (but only collect if hands in both)\n",
    "    if hand_in_right:\n",
    "        palm_visible = True\n",
    "        # Draw landmarks on a white background for prediction\n",
    "        white_bg = np.ones((roi_right.shape[0], roi_right.shape[1], 3), dtype=np.uint8) * 255\n",
    "        for hand_landmarks in results_right.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                white_bg,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2)\n",
    "            )\n",
    "\n",
    "        gray = cv2.cvtColor(white_bg, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE))\n",
    "        normalized = resized / 255.0\n",
    "        reshaped = normalized.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "        pred = model.predict(reshaped, verbose=0)\n",
    "        label = lb.classes_[np.argmax(pred)]\n",
    "        last_prediction = label\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Collect letter ONLY if collecting mode AND hands present in both boxes\n",
    "    if collecting and hand_in_left and hand_in_right:\n",
    "        # Draw blue rectangles to indicate active capture on both boxes\n",
    "        cv2.rectangle(frame, (left_x1, left_y1), (left_x2, left_y2), (255, 0, 0), 2)\n",
    "        cv2.rectangle(frame, (right_x1, right_y1), (right_x2, right_y2), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f'Hand Visible: {label}', (left_x1, left_y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "        # Append letter every 2 seconds\n",
    "        if current_time - last_time > 2:\n",
    "            collected_word += last_prediction\n",
    "            last_time = current_time\n",
    "            print(f\"[INFO] Added letter: {last_prediction}\")\n",
    "\n",
    "    # Draw ROIs on frame (green if hand detected, red otherwise)\n",
    "    color_left = (0, 255, 0) if hand_in_left else (0, 0, 255)\n",
    "    color_right = (0, 255, 0) if hand_in_right else (0, 0, 255)\n",
    "    cv2.rectangle(frame, (left_x1, left_y1), (left_x2, left_y2), color_left, 2)\n",
    "    cv2.rectangle(frame, (right_x1, right_y1), (right_x2, right_y2), color_right, 2)\n",
    "\n",
    "    cv2.putText(frame, f'Prediction: {label}', (right_x1, right_y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show collected words\n",
    "    cv2.putText(frame, f'Collected: {collected_word}', (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    cv2.putText(frame, f'Words: {\" | \".join(stored_words)}', (10, 100),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow('Sign Language Recognition', frame)\n",
    "\n",
    "    # Handle keys\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('s'):\n",
    "        collected_word = \"\"\n",
    "        collecting = True\n",
    "        print(\"[INFO] Started collecting letters...\")\n",
    "    elif key == ord('e'):\n",
    "        collecting = False\n",
    "        if collected_word:\n",
    "            stored_words.append(collected_word)\n",
    "        print(\"[INFO] Collection stopped.\")\n",
    "        print(f\"Formed Words: {' | '.join(stored_words)}\")\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
